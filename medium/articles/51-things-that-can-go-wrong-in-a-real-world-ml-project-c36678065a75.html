<article class="meteredContent"><div class="l"><div class="gg gh gi gj gk gl gm ce gn ch l"></div><div class="l"><header class="pw-post-byline-header go gp gq gr gs gt gu gv gw gx l"><div class="o gy u"><div class="o"><div class="fj l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://modern-cdo.medium.com/?source=post_page-----c36678065a75--------------------------------" rel="noopener follow"><div class="l do"><img alt="Sandeep Uttamchandani" class="l ch fl gz ha fp" height="48" loading="lazy" src="https://miro.medium.com/fit/c/96/96/1*71wo4Rv3wEcf39RfKNo73Q.jpeg" width="48"/><div class="fk fl l gz ha fo aq"></div></div></a></div><div class="l"><div class="pw-author bm b dm dn ga"><div class="hb o hc"><div><div aria-hidden="false" class="ci"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://modern-cdo.medium.com/?source=post_page-----c36678065a75--------------------------------" rel="noopener follow">Sandeep Uttamchandani</a></div></div><div class="hd he hf hg hh d"><span><a class="bm b hi bo hj hk hl hm hn ho hp bd bz hq hr hs cd cf cg ch ci cj" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2570cf937eb2&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2F51-things-that-can-go-wrong-in-a-real-world-ml-project-c36678065a75&amp;user=Sandeep+Uttamchandani&amp;userId=2570cf937eb2&amp;source=post_page-2570cf937eb2----c36678065a75---------------------follow_byline-----------" rel="noopener follow">Follow</a></span></div></div></div><div class="o ao ht"><p class="pw-published-date bm b bn bo cn"><span>Dec 31, 2020</span></p><div aria-hidden="true" class="hu ci"><span aria-hidden="true" class="l"><span class="bm b bn bo cn">·</span></span></div><div class="pw-reading-time bm b bn bo cn">21 min read</div><div aria-hidden="true" class="hu ci"><span aria-hidden="true" class="l"><span class="bm b bn bo cn">·</span></span></div><div class="hx l"><div aria-hidden="false" class="l"><button class="l dz hv bb"><div class="j i d"><div><div aria-hidden="false" class="ci"><svg fill="none" height="20" viewbox="0 0 20 20" width="20"><path d="M12.4 12.77l-1.81 4.99a.63.63 0 0 1-1.18 0l-1.8-4.99a.63.63 0 0 0-.38-.37l-4.99-1.81a.62.62 0 0 1 0-1.18l4.99-1.8a.63.63 0 0 0 .37-.38l1.81-4.99a.63.63 0 0 1 1.18 0l1.8 4.99a.63.63 0 0 0 .38.37l4.99 1.81a.63.63 0 0 1 0 1.18l-4.99 1.8a.63.63 0 0 0-.37.38z" fill="#FFC017"></path></svg></div></div></div><div class="h k hy hz ia"><svg class="hw" fill="none" height="20" viewbox="0 0 20 20" width="20"><path d="M12.4 12.77l-1.81 4.99a.63.63 0 0 1-1.18 0l-1.8-4.99a.63.63 0 0 0-.38-.37l-4.99-1.81a.62.62 0 0 1 0-1.18l4.99-1.8a.63.63 0 0 0 .37-.38l1.81-4.99a.63.63 0 0 1 1.18 0l1.8 4.99a.63.63 0 0 0 .38.37l4.99 1.81a.63.63 0 0 1 0 1.18l-4.99 1.8a.63.63 0 0 0-.37.38z" fill="#FFC017"></path></svg><p class="bm b bn bo cn">Member-only</p></div></button></div></div></div></div></div><div class="o ao"><div class="h k ib ic id"><div class="ie l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on twitter" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci if dw ig"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M20 5.34c-.67.41-1.4.7-2.18.87a3.45 3.45 0 0 0-5.02-.1 3.49 3.49 0 0 0-1.02 2.47c0 .28.03.54.07.8a9.91 9.91 0 0 1-7.17-3.66 3.9 3.9 0 0 0-.5 1.74 3.6 3.6 0 0 0 1.56 2.92 3.36 3.36 0 0 1-1.55-.44V10c0 1.67 1.2 3.08 2.8 3.42-.3.06-.6.1-.94.12l-.62-.06a3.5 3.5 0 0 0 3.24 2.43 7.34 7.34 0 0 1-4.36 1.49l-.81-.05a9.96 9.96 0 0 0 5.36 1.56c6.4 0 9.91-5.32 9.9-9.9v-.5c.69-.49 1.28-1.1 1.74-1.81-.63.3-1.3.48-2 .56A3.33 3.33 0 0 0 20 5.33" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="ie l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on facebook" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci if dw ig"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M19.75 12.04c0-4.3-3.47-7.79-7.75-7.79a7.77 7.77 0 0 0-5.9 12.84 7.77 7.77 0 0 0 4.69 2.63v-5.49h-1.9v-2.2h1.9v-1.62c0-1.88 1.14-2.9 2.8-2.9.8 0 1.49.06 1.69.08v1.97h-1.15c-.91 0-1.1.43-1.1 1.07v1.4h2.17l-.28 2.2h-1.88v5.52a7.77 7.77 0 0 0 6.7-7.71" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="ie l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on linkedin" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci if dw ig"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M19.75 5.39v13.22a1.14 1.14 0 0 1-1.14 1.14H5.39a1.14 1.14 0 0 1-1.14-1.14V5.39a1.14 1.14 0 0 1 1.14-1.14h13.22a1.14 1.14 0 0 1 1.14 1.14zM8.81 10.18H6.53v7.3H8.8v-7.3zM9 7.67a1.31 1.31 0 0 0-1.3-1.32h-.04a1.32 1.32 0 0 0 0 2.64A1.31 1.31 0 0 0 9 7.71v-.04zm8.46 5.37c0-2.2-1.4-3.05-2.78-3.05a2.6 2.6 0 0 0-2.3 1.18h-.07v-1h-2.14v7.3h2.28V13.6a1.51 1.51 0 0 1 1.36-1.63h.09c.72 0 1.26.45 1.26 1.6v3.91h2.28l.02-4.43z" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="l fr"><div><div aria-hidden="false" class="ci"><button class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci if dw ig"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path clip-rule="evenodd" d="M3.57 14.67c0-.57.13-1.11.38-1.6l.02-.02v-.02l.02-.02c0-.02 0-.02.02-.02.12-.26.3-.52.57-.8L7.78 9v-.02l.01-.02c.44-.41.91-.7 1.44-.85a4.87 4.87 0 0 0-1.19 2.36A5.04 5.04 0 0 0 8 11.6L6.04 13.6c-.19.19-.32.4-.38.65a2 2 0 0 0 0 .9c.08.2.2.4.38.57l1.29 1.31c.27.28.62.42 1.03.42.42 0 .78-.14 1.06-.42l1.23-1.25.79-.78 1.15-1.16c.08-.09.19-.22.28-.4.1-.2.15-.42.15-.67 0-.16-.02-.3-.06-.45l-.02-.02v-.02l-.07-.14s0-.03-.04-.06l-.06-.13-.02-.02c0-.02 0-.03-.02-.05a.6.6 0 0 0-.14-.16l-.48-.5c0-.04.02-.1.04-.15l.06-.12 1.17-1.14.09-.09.56.57c.02.04.08.1.16.18l.05.04.03.06.04.05.03.04.04.06.1.14.02.02c0 .02.01.03.03.04l.1.2v.02c.1.16.2.38.3.68a1 1 0 0 1 .04.25 3.2 3.2 0 0 1 .02 1.33 3.49 3.49 0 0 1-.95 1.87l-.66.67-.97.97-1.56 1.57a3.4 3.4 0 0 1-2.47 1.02c-.97 0-1.8-.34-2.49-1.03l-1.3-1.3a3.55 3.55 0 0 1-1-2.51v-.01h-.02v.02zm5.39-3.43c0-.19.02-.4.07-.63.13-.74.44-1.37.95-1.87l.66-.67.97-.98 1.56-1.56c.68-.69 1.5-1.03 2.47-1.03.97 0 1.8.34 2.48 1.02l1.3 1.32a3.48 3.48 0 0 1 1 2.48c0 .58-.11 1.11-.37 1.6l-.02.02v.02l-.02.04c-.14.27-.35.54-.6.8L16.23 15l-.01.02-.01.02c-.44.42-.92.7-1.43.83a4.55 4.55 0 0 0 1.23-3.52L18 10.38c.18-.21.3-.42.35-.65a2.03 2.03 0 0 0-.01-.9 1.96 1.96 0 0 0-.36-.58l-1.3-1.3a1.49 1.49 0 0 0-1.06-.42c-.42 0-.77.14-1.06.4l-1.2 1.27-.8.8-1.16 1.15c-.08.08-.18.21-.29.4a1.66 1.66 0 0 0-.08 1.12l.02.03v.02l.06.14s.01.03.05.06l.06.13.02.02.01.02.01.02c.05.08.1.13.14.16l.47.5c0 .04-.02.09-.04.15l-.06.12-1.15 1.15-.1.08-.56-.56a2.3 2.3 0 0 0-.18-.19c-.02-.01-.02-.03-.02-.04l-.02-.02a.37.37 0 0 1-.1-.12c-.03-.03-.05-.04-.05-.06l-.1-.15-.02-.02-.02-.04-.08-.17v-.02a5.1 5.1 0 0 1-.28-.69 1.03 1.03 0 0 1-.04-.26c-.06-.23-.1-.46-.1-.7v.01z" fill="#A8A8A8" fill-rule="evenodd"></path></svg></span></button></div></div></div><div class="ih o ao"><span><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc36678065a75&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2F51-things-that-can-go-wrong-in-a-real-world-ml-project-c36678065a75&amp;source=--------------------------bookmark_header-----------" rel="noopener follow"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="au de aw ax ay az ba if bc hv ij ik il"><svg aria-label="Add to list bookmark button" class="ii" fill="none" height="25" viewbox="0 0 25 25" width="25"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg></button></a></span></div></div><div class="ck im"><div><div aria-hidden="false" class="ci"></div></div></div></div></div><div class="in io ip j i d"><div class="fj l"><span><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc36678065a75&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2F51-things-that-can-go-wrong-in-a-real-world-ml-project-c36678065a75&amp;source=--------------------------bookmark_header-----------" rel="noopener follow"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="au de aw iq ay az ba ir bc hv cd o ao is it il"><svg aria-label="Add to list bookmark button" class="ii" fill="none" height="25" viewbox="0 0 25 25" width="25"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg><p class="bm b bn bo cn">Save</p></button></a></span></div><div class="iu l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on twitter" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci if dw ig"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M20 5.34c-.67.41-1.4.7-2.18.87a3.45 3.45 0 0 0-5.02-.1 3.49 3.49 0 0 0-1.02 2.47c0 .28.03.54.07.8a9.91 9.91 0 0 1-7.17-3.66 3.9 3.9 0 0 0-.5 1.74 3.6 3.6 0 0 0 1.56 2.92 3.36 3.36 0 0 1-1.55-.44V10c0 1.67 1.2 3.08 2.8 3.42-.3.06-.6.1-.94.12l-.62-.06a3.5 3.5 0 0 0 3.24 2.43 7.34 7.34 0 0 1-4.36 1.49l-.81-.05a9.96 9.96 0 0 0 5.36 1.56c6.4 0 9.91-5.32 9.9-9.9v-.5c.69-.49 1.28-1.1 1.74-1.81-.63.3-1.3.48-2 .56A3.33 3.33 0 0 0 20 5.33" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="iu l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on facebook" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci if dw ig"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M19.75 12.04c0-4.3-3.47-7.79-7.75-7.79a7.77 7.77 0 0 0-5.9 12.84 7.77 7.77 0 0 0 4.69 2.63v-5.49h-1.9v-2.2h1.9v-1.62c0-1.88 1.14-2.9 2.8-2.9.8 0 1.49.06 1.69.08v1.97h-1.15c-.91 0-1.1.43-1.1 1.07v1.4h2.17l-.28 2.2h-1.88v5.52a7.77 7.77 0 0 0 6.7-7.71" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="iu l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on linkedin" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci if dw ig"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M19.75 5.39v13.22a1.14 1.14 0 0 1-1.14 1.14H5.39a1.14 1.14 0 0 1-1.14-1.14V5.39a1.14 1.14 0 0 1 1.14-1.14h13.22a1.14 1.14 0 0 1 1.14 1.14zM8.81 10.18H6.53v7.3H8.8v-7.3zM9 7.67a1.31 1.31 0 0 0-1.3-1.32h-.04a1.32 1.32 0 0 0 0 2.64A1.31 1.31 0 0 0 9 7.71v-.04zm8.46 5.37c0-2.2-1.4-3.05-2.78-3.05a2.6 2.6 0 0 0-2.3 1.18h-.07v-1h-2.14v7.3h2.28V13.6a1.51 1.51 0 0 1 1.36-1.63h.09c.72 0 1.26.45 1.26 1.6v3.91h2.28l.02-4.43z" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="l fr"><div><div aria-hidden="false" class="ci"><button class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci if dw ig"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path clip-rule="evenodd" d="M3.57 14.67c0-.57.13-1.11.38-1.6l.02-.02v-.02l.02-.02c0-.02 0-.02.02-.02.12-.26.3-.52.57-.8L7.78 9v-.02l.01-.02c.44-.41.91-.7 1.44-.85a4.87 4.87 0 0 0-1.19 2.36A5.04 5.04 0 0 0 8 11.6L6.04 13.6c-.19.19-.32.4-.38.65a2 2 0 0 0 0 .9c.08.2.2.4.38.57l1.29 1.31c.27.28.62.42 1.03.42.42 0 .78-.14 1.06-.42l1.23-1.25.79-.78 1.15-1.16c.08-.09.19-.22.28-.4.1-.2.15-.42.15-.67 0-.16-.02-.3-.06-.45l-.02-.02v-.02l-.07-.14s0-.03-.04-.06l-.06-.13-.02-.02c0-.02 0-.03-.02-.05a.6.6 0 0 0-.14-.16l-.48-.5c0-.04.02-.1.04-.15l.06-.12 1.17-1.14.09-.09.56.57c.02.04.08.1.16.18l.05.04.03.06.04.05.03.04.04.06.1.14.02.02c0 .02.01.03.03.04l.1.2v.02c.1.16.2.38.3.68a1 1 0 0 1 .04.25 3.2 3.2 0 0 1 .02 1.33 3.49 3.49 0 0 1-.95 1.87l-.66.67-.97.97-1.56 1.57a3.4 3.4 0 0 1-2.47 1.02c-.97 0-1.8-.34-2.49-1.03l-1.3-1.3a3.55 3.55 0 0 1-1-2.51v-.01h-.02v.02zm5.39-3.43c0-.19.02-.4.07-.63.13-.74.44-1.37.95-1.87l.66-.67.97-.98 1.56-1.56c.68-.69 1.5-1.03 2.47-1.03.97 0 1.8.34 2.48 1.02l1.3 1.32a3.48 3.48 0 0 1 1 2.48c0 .58-.11 1.11-.37 1.6l-.02.02v.02l-.02.04c-.14.27-.35.54-.6.8L16.23 15l-.01.02-.01.02c-.44.42-.92.7-1.43.83a4.55 4.55 0 0 0 1.23-3.52L18 10.38c.18-.21.3-.42.35-.65a2.03 2.03 0 0 0-.01-.9 1.96 1.96 0 0 0-.36-.58l-1.3-1.3a1.49 1.49 0 0 0-1.06-.42c-.42 0-.77.14-1.06.4l-1.2 1.27-.8.8-1.16 1.15c-.08.08-.18.21-.29.4a1.66 1.66 0 0 0-.08 1.12l.02.03v.02l.06.14s.01.03.05.06l.06.13.02.02.01.02.01.02c.05.08.1.13.14.16l.47.5c0 .04-.02.09-.04.15l-.06.12-1.15 1.15-.1.08-.56-.56a2.3 2.3 0 0 0-.18-.19c-.02-.01-.02-.03-.02-.04l-.02-.02a.37.37 0 0 1-.1-.12c-.03-.03-.05-.04-.05-.06l-.1-.15-.02-.02-.02-.04-.08-.17v-.02a5.1 5.1 0 0 1-.28-.69 1.03 1.03 0 0 1-.04-.26c-.06-.23-.1-.46-.1-.7v.01z" fill="#A8A8A8" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></header><span class="l"></span><section><div><div class="fo as ja jb jc jd"></div><div class="je jf jg jh ji"><div class=""><h1 class="pw-post-title jj jk jl bm jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ga" id="0ac3">98 things that can go wrong in an ML project</h1></div><div class=""><h2 class="pw-subtitle-paragraph ki jk jl bm b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz cn" id="a72c">Are any of these landmines hiding in your real-world ML initiative?</h2></div><blockquote class="la"><p class="lb lc jl bm ld le lf lg lh li lj lk cn" id="a361"><a class="au ll" href="https://dzone.com/articles/top-10-reasons-why-87-of-the-machine-learning-proj" rel="noopener ugc nofollow" target="_blank">87%</a> of ML projects fail today!</p></blockquote><p class="pw-post-body-paragraph lm ln jl lo b lp lq km lr ls lt kp lu lv lw lx ly lz ma mb mc md me mf mg lk je ga" id="7217">These numbers should be taken with a grain of salt. Irrespective of the actual number, it does reflect reality — I have <a class="au ll" href="https://www.amazon.com/Self-Service-Data-Roadmap-Democratize-Insight-ebook/dp/B08HSSBC7F" rel="noopener ugc nofollow" target="_blank">seen</a> a significant percentage of ML-based projects never get into production!</p><figure class="mh mi mj mk gx ml"><div class="m fs l do"><div class="mm mn l"></div></div><figcaption class="mo bl gn gl gm mp mq bm b bn bo cn">GIF via <a class="au ll" href="https://giphy.com/gifs/wreckedtbs-funny-tbs-yNFg0qdiJTX1sCTjNc" rel="noopener ugc nofollow" target="_blank">giphy</a></figcaption></figure><blockquote class="mr ms mt"><p class="lm ln mu lo b lp mv km lr ls mw kp lu mx my lx ly mz na mb mc nb nc mf mg lk je ga" id="fdc1">The goal of this blog is to share my experiences on things that can go wrong in an ML project (they added up to 98!). The motivation with this post is for you to potentially avoid these landmines in your role as a <a class="au ll" href="https://hackernoon.com/7-gotchas-data-engineers-need-to-watch-out-for-in-an-ml-project-ev6t33mx" rel="noopener ugc nofollow" target="_blank">data engineer</a>, data scientist, ML engineer, <a class="au ll" href="https://medium.com/wrong-ml/the-chief-data-officers-scorecard-for-digital-transformation-a030c86d667c?source=friends_link&amp;sk=454a85e0e58d424281256e1951022d59" rel="noopener">data-business leader</a> driving an ML initiative.</p></blockquote><figure class="mh mi mj mk gx ml gl gm paragraph-image"><div class="ne nf do ng ce nh" role="button" tabindex="0"><div class="gl gm nd"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/1*SCOtTiGUKiP0F7Io39SIig.png 640w, https://miro.medium.com/max/720/1*SCOtTiGUKiP0F7Io39SIig.png 720w, https://miro.medium.com/max/750/1*SCOtTiGUKiP0F7Io39SIig.png 750w, https://miro.medium.com/max/786/1*SCOtTiGUKiP0F7Io39SIig.png 786w, https://miro.medium.com/max/828/1*SCOtTiGUKiP0F7Io39SIig.png 828w, https://miro.medium.com/max/1100/1*SCOtTiGUKiP0F7Io39SIig.png 1100w, https://miro.medium.com/max/1400/1*SCOtTiGUKiP0F7Io39SIig.png 1400w"/><img alt="" class="ce ni nj c" height="229" loading="lazy" role="presentation" width="700"/></picture></div></div><figcaption class="mo bl gn gl gm mp mq bm b bn bo cn">Experiences divided into 6 phases of any ML project. Depending on your role, feel free to read the respective sections in this blog (Image by author)</figcaption></figure><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="fc40">This is a long post divided the post into 6 categories. Feel free to read categories that relate best to your role as a data engineer, data scientist, ML engineer, <a class="au ll" href="https://www.linkedin.com/pulse/identifying-unicorn-managers-within-cdo-team-uttamchandani-ph-d-/" rel="noopener ugc nofollow" target="_blank">data-business leader</a>:</p><ul class=""><li class="nk nl jl lo b lp mv ls mw lv nm lz nn md no lk np nq nr ns ga" id="7d25"><a class="au ll" href="#6988" rel="noopener ugc nofollow"><strong class="lo jm">ML Problem definition</strong></a>: The formative stage of defining the scope, value definition, timelines, <a class="au ll" href="https://medium.com/wrong-ml/why-data-rights-governance-is-non-trivial-to-implement-in-the-real-world-a075cb06883a?source=friends_link&amp;sk=fef2a65a48c62458f7a8c364d49a17f8" rel="noopener">governance</a>, <a class="au ll" href="https://medium.com/wrong-ml/8-dq-traits-to-spot-talent-for-your-data-team-114ab7c368e9?source=friends_link&amp;sk=91697bea1fffbcd511a30e01bbfaeed4" rel="noopener">resources</a> associated with the deliverable.</li><li class="nk nl jl lo b lp nt ls nu lv nv lz nw md nx lk np nq nr ns ga" id="b153"><a class="au ll" href="#c076" rel="noopener ugc nofollow"><strong class="lo jm">Dataset Selection</strong></a>: This stage can take a <a class="au ll" href="https://medium.com/wrong-ml/challenges-with-clickstream-datasets-in-the-real-world-4b0798572215?source=friends_link&amp;sk=bc98b511f12607873c91bf19632346dc" rel="noopener">few hours or a few months</a> depending on the overall data platform <a class="au ll" href="https://medium.com/wrong-ml/the-secret-ingredient-in-successful-ml-projects-data-culture-347829b51f03?source=friends_link&amp;sk=52663e6152877ab612eac303ac71f9b5" rel="noopener">maturity and hygiene</a>. Data is the lifeblood of ML, so getting the <a class="au ll" href="https://medium.com/wrong-ml/challenges-in-finding-relevant-data-attributes-for-building-ml-models-97ae420a079f?source=friends_link&amp;sk=352c947d6559574e3697468a9012e20a" rel="noopener">right and reliable datasets</a> is supercritical.</li><li class="nk nl jl lo b lp nt ls nu lv nv lz nw md nx lk np nq nr ns ga" id="b666"><a class="au ll" href="#3873" rel="noopener ugc nofollow"><strong class="lo jm">Data Preparation</strong></a>: Real-world data is messy. Understanding data properties and <a class="au ll" href="https://medium.com/wrong-ml/why-data-wrangling-is-difficult-to-estimate-f6a54ec3f73c?source=friends_link&amp;sk=ad348030415fcc8d884bb4c35e1b1d0c" rel="noopener">preparing properly</a> can save endless hours down the line in debugging.</li><li class="nk nl jl lo b lp nt ls nu lv nv lz nw md nx lk np nq nr ns ga" id="df0d"><a class="au ll" href="#841a" rel="noopener ugc nofollow"><strong class="lo jm">ML Model Design</strong></a>: This phase involved <a class="au ll" href="https://medium.com/wrong-ml/why-creating-ml-model-features-is-challenging-in-the-real-world-79c8e6cd91d9?source=friends_link&amp;sk=2d4eb7a9961e021c8e2564044209ab87" rel="noopener">feature selection</a>, decomposing the problem, and formulating the right model algorithms.</li><li class="nk nl jl lo b lp nt ls nu lv nv lz nw md nx lk np nq nr ns ga" id="671b"><a class="au ll" href="#45cb" rel="noopener ugc nofollow"><strong class="lo jm">Model Training</strong></a>: Building the model, evaluating with the hold-out examples, and online experimentation.</li><li class="nk nl jl lo b lp nt ls nu lv nv lz nw md nx lk np nq nr ns ga" id="ecce"><a class="au ll" href="#271d" rel="noopener ugc nofollow"><strong class="lo jm">Operationalize in Production</strong></a>: This is the post-deployment phase involving <a class="au ll" href="https://medium.com/wrong-ml/observability-data-pipelines-99eda62b1704?source=friends_link&amp;sk=994fc87e78cc2fcdb28fbdae1f53ebcb" rel="noopener">observability</a> of the model and ML <a class="au ll" href="https://medium.com/wrong-ml/re-think-your-data-pipelines-in-the-decoupled-era-5b032bc8b779?source=friends_link&amp;sk=577cf9c5ceb0b3da0d34b7317f7f53ec" rel="noopener">pipelines</a>, refresh of the model with new data, and <a class="au ll" href="https://medium.com/wrong-ml/taming-data-quality-with-circuit-breakers-dbe550d3ca78?source=friends_link&amp;sk=5aff64c334b54728eb363db2fd26d4b0" rel="noopener">tracking</a> success metrics in the context of the original problem.</li></ul><figure class="mh mi mj mk gx ml gl gm paragraph-image"><div class="ne nf do ng ce nh" role="button" tabindex="0"><div class="gl gm ny"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/1*5v4QO6GySYUg0CeVWkjy3g.png 640w, https://miro.medium.com/max/720/1*5v4QO6GySYUg0CeVWkjy3g.png 720w, https://miro.medium.com/max/750/1*5v4QO6GySYUg0CeVWkjy3g.png 750w, https://miro.medium.com/max/786/1*5v4QO6GySYUg0CeVWkjy3g.png 786w, https://miro.medium.com/max/828/1*5v4QO6GySYUg0CeVWkjy3g.png 828w, https://miro.medium.com/max/1100/1*5v4QO6GySYUg0CeVWkjy3g.png 1100w, https://miro.medium.com/max/1400/1*5v4QO6GySYUg0CeVWkjy3g.png 1400w"/><img alt="" class="ce ni nj c" height="432" loading="lazy" role="presentation" width="700"/></picture></div></div><figcaption class="mo bl gn gl gm mp mq bm b bn bo cn">Breakdown of number of experiences covered in each of the categories (Image by author)</figcaption></figure><h1 class="nz oa jl bm ob oc od oe of og oh oi oj kr ok ks ol ku om kv on kx oo ky op oq ga" id="6988"><strong class="ba">ML Problem definition</strong></h1><ol class=""><li class="nk nl jl lo b lp or ls os lv ot lz ou md ov lk ow nq nr ns ga" id="e249"><strong class="lo jm"><em class="mu">“Vague success metrics of the ML model” </em></strong>Implementing an ML model to increase customer happiness. How do you define <em class="mu">“happiness?”</em> Instead, focus on the most paired down metric that is measurable and sensitive to the desired outcome. An intermediate or proxy metric to happiness can be time spent to accomplish a repeating workflow (such as creating an invoice in an accounting software) or the number of times the referral link is shared.</li></ol><figure class="mh mi mj mk gx ml"><div class="m fs l do"><div class="ox mn l"></div></div><figcaption class="mo bl gn gl gm mp mq bm b bn bo cn">GIF via <a class="au ll" href="https://giphy.com/gifs/rickandmorty-season-1-adult-swim-rick-and-morty-fu2rIw18eIDz1YCrCT" rel="noopener ugc nofollow" target="_blank">giphy</a></figcaption></figure><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="174b">2.<strong class="lo jm"><em class="mu">“Even if we had the perfect model — no clue of how it will be used within existing workflows”</em></strong> This is very common. More often we focus on going from data to insights, but miss out on the last mile from insight to the outcome. Simply predicting customer churn is of little value till is applied to the customer success process to proactively reach out and manage these customers.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="467a">3.<strong class="lo jm"><em class="mu">“Building a 100% accurate model — no clarity on the acceptable trade-offs such as precision versus recall”</em></strong> ML models are often considered to be “magic.” If you have a model predicting patients with a potential of heart attack, what are you optimizing: precision or recall. In this example, a high recall that minimizes False Negatives is more important than minimizing False Positive.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="a7a1">4.<strong class="lo jm"><em class="mu">“Using a hammer to kill an ant — not checking the performance of simpler alternatives”</em></strong> With the buzz around ML, simpler alternatives or heuristic models are often overlooked. I have seen examples where the existing heuristics are meeting the success metric by 99%, yet there will be a request to build an ML model. Also, in thinking ML models, also consider Human in Loop alternatives.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="c63c">5.<strong class="lo jm"><em class="mu">“Not all ML problems are worth solving — the impact may not be worth the effort”</em></strong> To deliver a well-tuned robust ML model deployed in production can range from 6–24 months depending on the complexity. Being clear about the strategic value of the project is critical.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="acfc">6. <strong class="lo jm"><em class="mu">“Drowning the business team in technical mumbo jumbo”</em></strong> Discussing the <a class="au ll" href="/understanding-auc-roc-curve-68b2303cc9c5" rel="noopener" target="_blank">AUC</a> or normalized entropy is of little value to business users. Instead, spending time explaining the significance and what these mean in the context of the business outcome or success criteria.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="84b9">7.<strong class="lo jm"> “Underestimating project costs — ignoring data costs” </strong>ML is data-dependent — acquiring and labeling data requires sizeable teams. Data sets are expensive to maintain. The cost of data is often ignored in project costs and can also lead to incorrect attrition decisions.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="6280">8. <strong class="lo jm">“Treating AI Ethics as a nice-to-have” </strong>It is important to treat ethics as both a software design consideration and a policy concern. In every aspect of the ML pipeline, evaluate the ethical considerations aligned with the values of the team — from data collection to decision making, to validation and monitoring of performance and effectiveness.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="dc8f">9. <strong class="lo jm">“Not leveraging past metrics during project formulation” </strong>During the initial project formulation stage, I have often seen misalignment across the business, engineering, and data teams on resources allocation, milestones, and timeline. Oftentimes, data scientists and project managers from similar past projects are missing in this decision making it guesswork driven rather than leveraging data metrics.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="36d2">10.<strong class="lo jm"> “Not managing stakeholder expectations” </strong>Oftentimes, ML projects start with unrealistic high expectations based on very little understanding of data availability, data quality, business workflow, team resources, end-to-end product deployment. Business stakeholders seldom are given a view of the risks and cost of incorrect predictions from the standpoint of user experience.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="55c8">11. <strong class="lo jm">“Not defining criteria to kill the project” </strong>Often, fundamental gaps are discovered in the data and business problem. Even though there is enough evidence, there are no clear criteria defined to pull the plug on the project. Instead, the teams should be encouraged to put projects “on ice” with clear documentation on when to review the project (for example, when certain data attributes are collected for a period of time). Partial projects can provide valuable lessons that can be applied in other current or future projects</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="5447">12. <strong class="lo jm">“Not preparing for the appropriate ML project complexity” </strong>Not all ML projects have the same complexity in terms of training, deployment, maintenance. One way to judge the complexity is whether the training and inference are online versus offline. Offline training and offline inference models are the easiest ML models. Online training differs in the time window for model refresh ranging from a few weeks to an order of minutes. Models that are online training and online inference require very robust ML pipelines with drift monitoring and alerting.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="3c02">13. <strong class="lo jm">“Not decomposing the problem” </strong>Instead of thinking of the business problem as one single ML model, most often the solution involves utilizing a sequence of models and methods.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="dc7a">14. <strong class="lo jm">“Optimizing on the model perpetually” </strong>Improving the model can be an endless endeavor. Resources to improve the model come at an opportunity cost as they are not working on other projects. A strategy that helps is to limit the number of resources and evaluate the status of the project at the end of a time-bound window. In the evaluation, you can decide to allocate another increment of time-bound resources, or declare the outcome is good enough for now. Often times, allocating additional resource increments only if you see traction.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="f8e3">15.<strong class="lo jm"> “Assuming Petabytes of data available == Successful ML project” </strong>During the planning process, the fact that there is a large amount of data is taken by business stakeholders as a proxy that an accurate ML model can be built. The size of the data is not a guarantee for a successful ML project — data quality, missing data facts, incomplete representation of the truth we are aiming to model, and several other aspects decide the fate of the project.</p><h1 class="nz oa jl bm ob oc od oe of og oh oi oj kr ok ks ol ku om kv on kx oo ky op oq ga" id="c076">Dataset Selection</h1><figure class="mh mi mj mk gx ml"><div class="m fs l do"><div class="ox mn l"></div></div><figcaption class="mo bl gn gl gm mp mq bm b bn bo cn">GIF via <a class="au ll" href="https://giphy.com/gifs/l4RKhOL0xiBdbgglFi" rel="noopener ugc nofollow" target="_blank">giphy</a></figcaption></figure><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="91e2">16. <strong class="lo jm"><em class="mu">“I thought this dataset attribute </em></strong><a class="au ll" href="https://medium.com/wrong-ml/schema-on-read-curse-of-data-lakes-our-5-antidotes-1386199d262f?source=friends_link&amp;sk=cff1d1104f9f82f495f1bf453327d76c" rel="noopener"><strong class="lo jm"><em class="mu">means something else</em></strong></a><strong class="lo jm"><em class="mu">” </em></strong>Data attributes are typically never documented. Prior to the big data era, data was curated before being added to the central data warehouse. This is known as <em class="mu">schema-on-write. </em>Today, the approach with data lakes is to first aggregate the data and then infer the meaning of data at the time of consumption. This is known as <em class="mu">schema-on-read.</em></p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="5aaf">17. <strong class="lo jm"><em class="mu">“5 definitions exist for the same business metric —</em></strong><a class="au ll" href="https://unraveldata.com/resources/standardizing-business-metrics-democratizing-experimentation-at-intuit/" rel="noopener ugc nofollow" target="_blank"><strong class="lo jm"><em class="mu"> which one to use</em></strong></a><strong class="lo jm"><em class="mu">”</em></strong> Derived data or metrics can have multiple sources of truth and business definitions. For instance, even basic metrics such as “new customers” have multiple definitions corresponding to what qualifies a customer as a new customer.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="6c8f">18.<strong class="lo jm"><em class="mu">“Where is the dataset I need for my model?”</em></strong> Given the siloed nature of data, different datasets are managed and cataloged by multiple teams. A lot of tribal knowledge within the data team on datasets and corresponding contact person.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="79c9">19.<strong class="lo jm"><em class="mu">“The data warehouse is stale”</em></strong> Raw data is copied to the Data Lake and ETl’ed into Data Warehouses. Given ETL pipeline errors, the business metrics in the warehouse can be stale and not refreshed in a timely fashion.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="ec22">20.<strong class="lo jm"><em class="mu">“Need to instrument the application for more clickstream events — it will take months”</em></strong> Managing instrumentation for adding beacons — more details in this <a class="au ll" href="https://medium.com/wrong-ml/challenges-with-clickstream-datasets-in-the-real-world-4b0798572215" rel="noopener">blog</a>.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="1981">21.<strong class="lo jm"><em class="mu">“Assuming all the datasets have the same quality”</em></strong> This is a classic mistake. Not all datasets are reliable. Some of them are updated and managed by source teams very closely while other datasets are abandoned or not regularly updated or have flaky ETL pipelines.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="9691">22.<strong class="lo jm"><em class="mu">“Customer changed preference to not use their data for ML. Why are those records still included”</em></strong> Data rights are now becoming critical. It is <a class="au ll" href="https://medium.com/wrong-ml/why-data-rights-governance-is-non-trivial-to-implement-in-the-real-world-a075cb06883a" rel="noopener">important to track and enforce</a> during ML model training and re-training.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="00f4">23.<strong class="lo jm"><em class="mu">“Uncoordinated schema changes at the data source”</em></strong> Very common, there are schema changes at the source that are uncoordinated with downstream processing. The changes can range from schema changes (breaking existing pipelines) to difficult to detect sematic changes to the data attributes. Also, when business metrics change, there is a lack of versioning of the definitions.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="82fb">24. <strong class="lo jm"><em class="mu">“Not unit testing the input data”</em> </strong>In traditional software development projects, it is a best practice to write unit tests to validate code dependencies. In ML projects, a similar best practice should be applied to continuously test, verify, and monitor your input data.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="c658">25.<strong class="lo jm"> <em class="mu">“Watch out for biased datasets”</em> </strong>Datasets do not capture the ultimate truth from the statistical standpoint. They only capture the attributes that the application owners required at that time for their use-case. It is important to analyze datasets for bias and dropped data. Understanding the context of the dataset is supercritical.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="a026">26.<strong class="lo jm"> <em class="mu">“Letting datasets become orphans without stewards”</em></strong> Datasets are useless if they cannot be understood. Trying to reverse engineer the meaning of columns is often a ‘losing battle.” The key is to aggressively document the dataset attribute details as well as ensuring that there is a data steward responsible for a dataset to update and evolve the documentation details.</p><h1 class="nz oa jl bm ob oc od oe of og oh oi oj kr ok ks ol ku om kv on kx oo ky op oq ga" id="3873"><strong class="ba">Data Preparation</strong></h1><figure class="mh mi mj mk gx ml"><div class="m fs l do"><div class="ox mn l"></div></div><figcaption class="mo bl gn gl gm mp mq bm b bn bo cn">GIF via <a class="au ll" href="https://giphy.com/gifs/schittscreek-comedy-pop-tv-xUOwG4ZJBYTfPrpnbO" rel="noopener ugc nofollow" target="_blank">giphy</a></figcaption></figure><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="a90f">27. <strong class="lo jm"><em class="mu">“Don't forget data expires?” </em></strong>Data has an expiry date. Records of customer behavior from 10 years back may not representative. Additionally, ensuring data is <a class="au ll" href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables" rel="noopener ugc nofollow" target="_blank">IID</a> (Independent and Identically Distributed) for model training, as well as taking into account the seasonality of data.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="3594">28. <strong class="lo jm"><em class="mu">“Systematic data issues making overall dataset bias”</em></strong> If errors in the dataset are random, they are less harmful to model training. But if there is a bug such that a specific row or column is systematically missing, it can lead to a bias in the dataset. For instance, device details of customer clicks are missing for Andriod users due to a bug, the dataset will be biased for iPhone user activity.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="1947">29. <strong class="lo jm"><em class="mu">“Unnoticed sudden distribution changes in the data”</em></strong> Datasets are constantly evolving. Analysis of the data distribution is not a one-time activity required only at the time of model creation. Instead, there is a need to continuously monitor datasets for drifts, especially for online training.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="771e">30. <strong class="lo jm"><em class="mu">“Using all the data for training — each model iteration can take days”</em></strong> While more data helps to build an accurate model, sometimes data is huge with billions of records. Training on a larger dataset takes both time and resources. Each training iteration takes longer slowing down the overall project completion. There is a need to use data sampling effectively.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="5d56">31. “<strong class="lo jm"><em class="mu">We are using the best polyglot datastores — but how do I now write queries effectively across this data?</em></strong>” There is no silver bullet datastore. Typically polyglot persistence is used with specialized datastores for Key-Value, Document, Graph, Time-series data, etc. While heterogeneity delivers the best performance, data teams have a learning curve to effectively build pipelines to analyze and join data across datastores.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="2958">32. <strong class="lo jm"><em class="mu">“Ignoring seasonality in sales or customer behavior data”</em> </strong>Before using a dataset, verify the properties of iid, stationary (not changing over time), and ensure the same distribution during training and testing. Seasonality is often missed which is a classic violation of stationarity.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="7438">33. <strong class="lo jm"><em class="mu">“Not randomizing before splitting training and test data”</em> </strong>Very often, without randomization, we may end up with all fall data in training and summer data in the test. This can lead to loss-epoch graphs that require unnecessary debugging. A low-hanging fruit that is often missed.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="43e9">34. <strong class="lo jm"><em class="mu">“Test examples have duplicates in the training set”</em> </strong>Oftentimes, we have been excited by really high accuracy numbers. Double checking often reveals that many of the examples in the test set are duplicates of examples in the training set. In such scenarios, the measurements of model generalization are non-deterministic.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="d5f1">35. <strong class="lo jm"><em class="mu">“Not Qualifying the test set”</em> </strong>Ensuring test sets yield statistically meaningful results and representative of the data set as a whole.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="3c29">36. <strong class="lo jm"><em class="mu">“Using Normalization instead of Standardization for scaling feature values”</em> </strong>To bring features to the same scale, use normalization (MinMaxScaler) when the data is uniformly distributed and standardization (StandardScaler) when the feature is approximately Gaussian.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="84fb">37. <strong class="lo jm"><em class="mu">“Improper handling of outliers”</em> </strong>Based on the problem, outliers can either be a noise to ignore or an important to take into account. Outliers typically arising from collection errors can be ignored. Machine learning algorithms differ in their sensitivity to outliers — AdaBoost is more sensitive to outliers compared to XgBoost which is more sensitive than a decision tree that would simply count an outlier as a false classification. Proper handling of outliers requires understanding if they can be ignored and picking the appropriate algorithm based on sensitivity.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="1949">38. <strong class="lo jm"><em class="mu">“Arbitrary sample selection within a large dataset”</em></strong> Given very large datasets, sampling is typically arbitrary. Paying special attention to leveraging techniques such as <a class="au ll" href="https://en.wikipedia.org/wiki/Importance_sampling" rel="noopener ugc nofollow" target="_blank">importance sampling</a>.</p><h1 class="nz oa jl bm ob oc od oe of og oh oi oj kr ok ks ol ku om kv on kx oo ky op oq ga" id="841a">ML Model Design</h1><figure class="mh mi mj mk gx ml"><div class="m fs l do"><div class="ox mn l"></div></div><figcaption class="mo bl gn gl gm mp mq bm b bn bo cn">GIF via <a class="au ll" href="https://giphy.com/gifs/89a-3d-design-art-bpmNf92LmkoMw" rel="noopener ugc nofollow" target="_blank">giphy</a></figcaption></figure><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="8aa0">39.<strong class="lo jm"> <em class="mu">“Leverage feature crossing before jumping to non-linear models”</em> </strong>Linear learners scale well for massive data and easier to maintain in production. For problems that are not inherently linear, I have seen Feature crossing as an effective approach for several problems i.e., adding cross features when data is not linearly separable in the input space is<strong class="lo jm"> </strong>especially effective with massive input data sets.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="59e2">40. <strong class="lo jm"><em class="mu">“Model accuracy too good to be true — check for feature leakage”</em></strong> Improper feature values can lead to feature leakage — the <a class="au ll" href="https://medium.com/wrong-ml/why-creating-ml-model-features-is-challenging-in-the-real-world-79c8e6cd91d9" rel="noopener">blog</a> with more details.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="b1e0">41. “<strong class="lo jm"><em class="mu">Relying on flaky pipelines for generating time-dependent features”</em></strong> Time-dependent features such as “product views so far” needs a low latency robust pipeline to calculate values. Often times, bugs in these pipelines are very difficult to debug in the context of the overall model behavior.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="9720">42. <strong class="lo jm"><em class="mu">“Lack of balance between bias (underfitting) and variance (overfitting)”</em></strong> Watching out for a simplified model that is underfitting the data (bias) to the other extreme of a complex high dimensional model with sensitivity to the slightest variation of the data (variance). Striking the right balance is required for an effective model in production.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="ab46">43. <strong class="lo jm"><em class="mu">“Compromising interpretability prematurely for performance”</em></strong> ML projects pre-maturely fast forward into applying deep-learning. In early iterations of an ML project, an interpretable model is more important than a black box performant one. An interpretable model helps generate the next set of hypotheses about the features and data properties. The key is to start with simple models and not optimizing prematurely. Simpler models are easier to debug and iterate.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="2d66">44.<strong class="lo jm"> <em class="mu">“Adding new features without justification on how they increase the model quality”</em> </strong>Adding features that encode new information will improve model performance, but at the cost of increased model complexity (as well as complexity on testing, deployment, maintenance of corresponding ML feature pipelines). New features added to the model should be justified using correlation matrices or training the models and analyzing results with and without the feature. In software engineering terminology, adding new features after the model is performing reasonably should be treated as a “code freeze phase” with every feature change reviewed carefully.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="2f83">45.<strong class="lo jm"><em class="mu">“Skipping Feature scaling”</em> </strong>Without feature scaling, the model pays too much attention to the features having a wider range. This is important for model correctness.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="91be">47. <strong class="lo jm"><em class="mu">“Always using deep learning instead of traditional feature engineering”</em></strong> For problems with weak baselines and good intuition, better to focus on simpler models applying the intuitions of feature engineering rather than black-box deep learning. Deep learning is good for problems with a significant amount of data, and compute bandwidth.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="7e72">48. <a class="au ll" href="https://dzone.com/articles/feature-hashing-for-scalable-machine-learning" rel="noopener ugc nofollow" target="_blank"><strong class="lo jm"><em class="mu">“Not applying hashing for sparse features”</em></strong></a></p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="afcb">49. <strong class="lo jm"><em class="mu">“Not attempting to reduce the dimensionality of models”</em></strong> High dimensional models are difficult to manage in training and production deployment. It is important to reduce the number of dimensions associated with the model using techniques such as <a class="au ll" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">PCA</a>, <a class="au ll" href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis" rel="noopener ugc nofollow" target="_blank">LDA</a>.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="0b32">50. <em class="mu">“</em><strong class="lo jm"><em class="mu">Not applying a business lens to Classification Threshold tuning” </em></strong>“Tuning” a threshold for logistic regression is different from tuning hyperparameters such as learning rate. Choosing a threshold involves assessing the cost of making an incorrect prediction. For instance, mistakenly labeling a transaction as fraudulent will lead to delay in processing and involve wasted effort in human analysis. However, labeling a fraudulent transaction as non-fraudulent will lead to financial loss to the business. Better to optimize the threshold for recall instead of precision.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="8279">51. <strong class="lo jm"><em class="mu">“Treating regularization as a nice-to-have in logistic regression”</em> </strong>Without regularization, the asymptotic nature of logistic regression would keep driving loss towards 0 in high dimensions. Applying techniques such as L2/L1 regularization or early stopping is a must-have.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="482c">52.<strong class="lo jm"> <em class="mu">“Shy in using embeddings”</em> </strong>Using <a class="au ll" href="/neural-network-embeddings-explained-4d028e6f0526" rel="noopener" target="_blank">embedding</a> to translate large sparse vectors into a lower-dimensional space (while preserving semantic relationships) is an important optimization that is sometimes overlooked.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="130c">53. <strong class="lo jm"><em class="mu">“Using guesswork for deciding model freshness requirements”</em></strong> Instead of applying heuristics or rules-of-thumb, it is important to evaluate the performance degradation of the model as a function of the refresh interval. I have seen projects where once a month refresh would suffice but the team is doing daily or weekly refresh which is expensive. On the other extreme, a delayed refresh can lead to a negative impact. Picking the right refresh based on data analysis is the key</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="13f7">54. <strong class="lo jm"><em class="mu">“Not using features that apply to a very small fraction of your data”</em></strong> This one is a classic mistake. Often times, I have dismissed features that did not have a good coverage only to find out that while they had a low coverage overall, the feature was present in 95% of the positive examples. The takeaway, do not dismiss low coverage features early.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="0ed1">55.<strong class="lo jm"><em class="mu">“Lots of features in the model but too little data”</em> </strong>It is important to be realistic w.r.t. the number of features you are adding to the model relative to the cardinality of data examples available for learning. It is possible to build on even small datasets if you are building a simple model. The key is to avoid scenarios of building complex models with limited data.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="327e">56. <strong class="lo jm"><em class="mu">“Unclear model scope” </em></strong>Being clear with the scope of the model w.r.t coverage of feature values. For instance, if the prediction model is trained for first-time visitors rather than repeat users in a specific region.</p><h1 class="nz oa jl bm ob oc od oe of og oh oi oj kr ok ks ol ku om kv on kx oo ky op oq ga" id="45cb"><strong class="ba">Model Training &amp; Tuning</strong></h1><p class="pw-post-body-paragraph lm ln jl lo b lp or km lr ls os kp lu lv oy lx ly lz oz mb mc md pa mf mg lk je ga" id="f90f">57. <strong class="lo jm"><em class="mu">“Ad-hoc tuning is faster compared to a scientific approach”</em></strong> Oftentimes, tuning of hyperparameters &amp; model architecture tends to be ad-hoc. While it appears to be faster, following a scientific approach always pays off based on my experience.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="b8b1">58. <strong class="lo jm"><em class="mu">“A model that won’t converge”</em></strong> During model training, there are scenarios when the loss-epoch graph keeps bouncing around and does not seem to converge irrespective of the number of epochs. There is no silver bullet as there are multiple root-causes to investigate — bad training examples, missing truths, changing data distributions, too high a learning rate. The most common one I have seen is bad training examples related to a combination of anomalous data and incorrect labels.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="f39e">59. <strong class="lo jm"><em class="mu">“Loss value reduces and then increases significantly with epochs”</em></strong> Sometimes there are scenarios where the model seems to be converging but suddenly the loss value increases significantly. There are multiple reasons for this kind of exploding loss — the most common one I have seen is outliers in the data that are not evenly distributed/shuffled in the data. Shuffling, in general, is an important step including patterns where the loss is showing a repeating step function behavior.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="c01f">60. <strong class="lo jm"><em class="mu">“Improper tracking of details related to model versions and experiments”</em></strong> This is accidental complexity — not being able to track the details leads to wasted work and moving in circles. Results from model experiments are tracked with cryptic names such as <em class="mu">tmp_1, tmp_2,</em> etc. Given the lack of details, new members joining the team often revisit the exploration that has already been done in the past.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="9585">61. <strong class="lo jm"><em class="mu">“Ignoring the specificity and sparsity trade-off”</em></strong> Instead of building a generic model, imagine building a model for a specific geographic region or specific user persona. Specificity will make the data more sparse but can lead to better accuracy for those specific problems. It is important to explore these trade-offs during tuning.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="c471">62. <strong class="lo jm"><em class="mu">“Loss value is reducing but recall/precision not improving”</em> </strong>Typically TF Keras and other ML libraries have a default classification threshold of 0.5. With this threshold, sometimes the recall value gets pegged at 0 as the classification probability will never get higher than the positive classification threshold, especially for problems with a large class imbalance. It is important to Investigate ROC AUC.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="5653">63. <strong class="lo jm"><em class="mu">“Prematurely jumping to online experimentation” </em></strong>Having clear thresholds defined on when the offline validation is meeting the success criteria to start experimenting with online validation.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="cef0">64. <strong class="lo jm"><em class="mu">“In multi-class classification, not prioritizing specific per-class metrics accuracy”</em> </strong>For multi-class prediction problems, instead of tracking just the overall classification accuracy, it is often useful to prioritize accuracy of specific classes and iteratively work on improving the model class-by-class. For instance, in classifying different forms of fraudulent transactions, focus on increasing the recall of specific classes (such as foreign transactions) based on business needs.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="1f8f">65. <strong class="lo jm"><em class="mu">“Not paying attention to infrastructure capacity”</em></strong> Sometimes, there is only so much optimization that can be done in models and feature engineering. Given large datasets, the infrastructure capacity can become the limiting factor.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="bdc7">66. <strong class="lo jm"><em class="mu">“Evaluating models using different datasets” </em></strong>This one falls in the obvious category but is sometimes overlooked. Models can be trained using different datasets but an apples-to-apples comparison requires using the same test dataset.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="313f">67. <strong class="lo jm"><em class="mu">“Reporting model accuracy for the overall data”</em></strong> Instead, it is more useful to segment the dataset in cohorts (based on business definition) and evaluate the model performance for different clusters.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="20d4">68. <strong class="lo jm"><em class="mu">“Training results not reproducible”</em></strong> To reproduce specific training results, it is important to snapshot the code (algo), data, config, and parameter values. Over recent times, the problem of reproducibility has been addressed in MLFlow, TFX, and multiple other solutions.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="4c40">69. <strong class="lo jm"><em class="mu">“Long time before first online experiment”</em></strong> No matter how complex the problem, the goal should be to have the first A/B experimentation of the model within the first 3 months even if the overall project takes 18–24 months. It helps uncover several aspects of the problem or dataset assumptions and speeds up the development of the final model.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="c973">70. <strong class="lo jm"><em class="mu">“Model behaves differently in online experimentation compared to offline validation”</em></strong> This is typically representative of over-fitting or inconsistencies in online-offline feature generation.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="8fc4">71. <strong class="lo jm"><em class="mu">“Ignoring </em></strong><a class="au ll" href="/dangerous-feedback-loops-in-ml-e9394f2e8f43" rel="noopener" target="_blank"><strong class="lo jm"><em class="mu">feedback loops</em></strong></a><strong class="lo jm"><em class="mu">”</em></strong> A feedback loop typically arises when a model is trained with a given set of features that are correlated with the outcome from another model.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="34b2">72. <strong class="lo jm"><em class="mu">“Making multiple changes within an experiment”</em></strong> Besides ML model changes, tracking the product or engineering changes is equally important. The goal should be to not bundle multiple changes being it difficult to correlate cause and effect.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="0777">73.<strong class="lo jm"><em class="mu"> “Ad-hoc framework to analyze the results of the experiment”</em></strong> Having a consistent way to analyze the results of the experiment is supercritical. Often times, a lot of time is spent on understanding the metrics collected from the experiment.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="763b">74. <strong class="lo jm"><em class="mu">“No backup plan if the experiment goes south”</em></strong> Critical to proactively measure if the experiment is causing a negative impact on the test group and reverting the changes.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="d491">75. <strong class="lo jm"><em class="mu">“Not calibrating the model ”</em></strong> In addition to accuracy, <a class="au ll" href="https://medium.com/analytics-vidhya/calibration-in-machine-learning-e7972ac93555" rel="noopener">model calibration</a> should be used. It's an easy sanity check measure of predicted probabilities with the observed distribution of classes.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="9af2">76. <strong class="lo jm"><em class="mu">“Always manually tuning hyperparameters”</em> </strong>Hyperparameters play an important role in model performance. The ideal combination of hyperparameter values is data-dependent requiring experimentation and tuning. Traditionally, this is done manually with trial-and-error. Oftentimes, automated hyperparameter tuning service available in cloud services such as <a class="au ll" href="https://cloud.google.com/ml-engine/docs/tensorflow/hyperparameter-tuning-overview" rel="noopener ugc nofollow" target="_blank">Google,</a> <a class="au ll" href="https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-ex.html" rel="noopener ugc nofollow" target="_blank">AWS</a>, <a class="au ll" href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters" rel="noopener ugc nofollow" target="_blank">Azure</a> is quite effective and helps improve the overall team productivity.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="4fe3">77. <strong class="lo jm"><em class="mu">“Ignoring prediction bias”</em> </strong>Prediction bias is the difference in the average of predications and average of labels in the dataset. Prediction bias serves as an early indicator of model issues — a big nonzero prediction bias is indicative of a bug somewhere in the model. Interesting <a class="au ll" href="https://research.fb.com/wp-content/uploads/2016/11/practical-lessons-from-predicting-clicks-on-ads-at-facebook.pdf" rel="noopener ugc nofollow" target="_blank">facebook paper</a> in the context of ads CTR. Typically, the bias is useful to measure across prediction buckets.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="2b0f">78. <strong class="lo jm">“<em class="mu">Calling it a success just on model accuracy numbers</em>” </strong>Accuracy of 95% means 95 of 100 predictions were correct. Accuracy is a flawed metric with a class imbalance in the dataset. Instead investigate deeply into metrics such as precision/recall and how it correlates to overall user metrics such as spam detection, tumor classification, etc.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="fc19">79. <strong class="lo jm">“<em class="mu">Not understanding the impact of regularization Lambda</em>” </strong>Lambda is a key parameter in striking the balance between simplicity and training-data fit. High lambda → simple model → possibly <em class="mu">underfitting</em>. Low Lambda → complex model → potential <em class="mu">overfitting</em> your data (won’t be able to generalize to new data). The ideal value of lambda is one that generalizes well to previously unseen data — data-dependent and requires analysis.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="d657">80. <strong class="lo jm"><em class="mu">“Tuning hyperparameters at the same time”</em> </strong>Sounds like a common sense which is sometimes uncommon. Changes to regularization parameters can be confounded with the effects from changes in the learning rate, etc. During tuning, sequence the tuning on different parameters.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="59ec">81. <strong class="lo jm"><em class="mu">“Using the same test set over and over” </em></strong>More the same data is used for parameter and hyperparameter settings, the lesser confidence that the results will actually generalize. It is important to collect more data and keep adding to the test and validation sets.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="42f8">82.<strong class="lo jm"><em class="mu"> “Setting batch size hyperparameter small”</em> </strong>For datasets with a very large number of examples that cannot fit in memory, reducing the batch size helps. Setting a <em class="mu">very</em> small batch can cause instability. The typical strategy is to start with a large batch size value, and then decrease the size until there is degradation.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="6ac0">83. <em class="mu">“</em><strong class="lo jm"><em class="mu">Not paying attention to initiation value in neural networks</em></strong><em class="mu">”</em> Given non-convex optimization in NN, <a class="au ll" href="https://www.deeplearning.ai/ai-notes/initialization/" rel="noopener ugc nofollow" target="_blank">initialization matters</a>.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="e8cb">84. <strong class="lo jm"><em class="mu">“Not tracking the results of failed experiments”</em> </strong>During the model building experients, a wide range of data, models, and configurations (parameter and hyperparameters) are explored. Typically, failure experiments are not well documented and seen as a waste of time. It is important to capture the context such that new team members can leverage the learnings and mot reinvent the wheel.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="841f">85. <strong class="lo jm"><em class="mu">“Assuming wrong labels always need to be fixed”</em> </strong>When wrong labels are detected, it is tempting to jump and get them fixed. it is important to first analyze misclassified examples for root-cause. Often times, errors due to incorrect labels may be a very small percentage. There might be a bigger opportunity to better train for specific data slices that might be the predominant root-cause.</p><h1 class="nz oa jl bm ob oc od oe of og oh oi oj kr ok ks ol ku om kv on kx oo ky op oq ga" id="271d"><strong class="ba">Operationalize</strong></h1><figure class="mh mi mj mk gx ml"><div class="m fs l do"><div class="pb mn l"></div></div><figcaption class="mo bl gn gl gm mp mq bm b bn bo cn">GIF via <a class="au ll" href="https://giphy.com/gifs/usnationalarchives-space-nasa-apollo-11-kvl2YhR110qsBrHid2" rel="noopener ugc nofollow" target="_blank">giphy</a></figcaption></figure><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="6fbb">86. <strong class="lo jm"><em class="mu">“ETL Pipeline SLA was 8 am. It’s now 4 pm and still processing — why is my metrics processing slow today ”</em></strong></p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="4b02">87. <strong class="lo jm"><em class="mu">“Metrics processing pipelines completed successfully but results are wrong?” </em></strong>This is typically due to missing job dependencies. Oftentimes, the processing of metrics would start even before the raw data ingestion has completed. This leads to incorrect results.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="12f1">88. <strong class="lo jm"><em class="mu">“Measuring model performance as a whole instead of data slices”</em> </strong>Evaluating model performance across the entire dataset is a border-line vanity metric. Instead, report on model performance across various data slices. Checking the model across the data slices helps remove bias and uncovers valuable insights about the dataset, model, and the truth we are aiming to model.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="79ce">89. <strong class="lo jm"><em class="mu">“Tracking pipeline logic only to control training-serving skew”</em> </strong>The typical reason for skew in model performance during training and inference is due to a discrepancy in handling data in the training and serving pipelines. But there are other aspects such as changes in data properties between training and serving as well as potential feedback loops.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="4604">90. <strong class="lo jm"><em class="mu">“Using two different programming languages between training and serving”</em> </strong>Avoid scenarios where training and serving pipelines are written in different languages.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="d6bf">91. <strong class="lo jm"><em class="mu">“Response time to generate an inference is too high”</em></strong> The model endpoint may be saturated due to limited resources. With the automation of model deployment solutions, this is less relevant today.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="8a95">92. <strong class="lo jm"><em class="mu">“Data quality issues at source, or ingestion into the lake, or ETL processing” </em></strong>Quality issues need to be <a class="au ll" href="https://quickbooks-engineering.intuit.com/managing-data-issues-as-incidents-226f5f1c9e72?source=friends_link&amp;sk=7eca658f035039a9db05473ca21c25b6" rel="noopener ugc nofollow" target="_blank">proactively tracked</a> to ensure the correctness of inferences as well as for online model training.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="6f0d">93. <strong class="lo jm"><em class="mu">“Cloud costs jumped up 3X this month” </em></strong>Given the linear pay-per-use pricing model in the cloud, ML costs can quickly get out of control. I recall the team spending 200K over the weekend running on high-end GPUs.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="7393">94. <strong class="lo jm"><em class="mu">“Skipping model optimization phase in Neural Networks”</em> </strong>In the initial phases, the focus is on generating the most accurate model. Once the model quality is reasonable, it is important to have a dedicated focus to try decreasing depth and width (overfitting) before deployment. One useful technique is halving the width at each NN layer. This will naturally impact model quality — the optimization phase is dedicated time in the project to balance quality with model depth and width.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="d314">95. <strong class="lo jm"><em class="mu">“Not testing the correctness of the ML pipeline”</em> </strong>There is a lot of focus on testing of ML model correctness. What about testing the ML pipeline? Starting with quality validation of input data, validating features, validating model deployment, etc. 95%+ of issues I have seen in production are related to ML pipelines!</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="41dd">96. <strong class="lo jm"><em class="mu">“No checks and bounds for data and concept drift”</em></strong> For models deployed in production, it is critical to have the right processes in place. For instance, DDL monitoring at sources for concept drift.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="bf77">97. <strong class="lo jm"><em class="mu">“Adding unnecessary calibration layers”</em> </strong>Sometimes to balance model bias (such as prediction bias for different data slices), a quicker shortcut is to add a calibration layer on the top of the model to manage these scenarios. In software engineering terminology, this is a classic example of “tech debt.” Such systems are difficult to update, debug and manage in production.</p><p class="pw-post-body-paragraph lm ln jl lo b lp mv km lr ls mw kp lu lv my lx ly lz na mb mc md nc mf mg lk je ga" id="0ccb">98. <strong class="lo jm"><em class="mu">“Slow poisoning of the model”</em> </strong>It is easier to detect 0–1 kind of errors with data and ML pipelines. The problems that are the most difficult to debug are ones where a table is being updated intermittently or systematic errors happening sporadically on certain table attributes. In such scenarios, the models will degrade gradually and adjust to the changes. The key is investing in <a class="au ll" href="https://medium.com/wrong-ml/taming-data-quality-with-circuit-breakers-dbe550d3ca78" rel="noopener">robust monitoring</a> for every aspect of the pipeline and data attributes.</p><blockquote class="mr ms mt"><p class="lm ln mu lo b lp mv km lr ls mw kp lu mx my lx ly mz na mb mc nb nc mf mg lk je ga" id="eb62"><strong class="lo jm">If you are interested in more battle scars learned from real-world data and AI/ML/DataOps, checkout </strong><a class="au ll" href="https://unraveldata.com/" rel="noopener ugc nofollow" target="_blank"><strong class="lo jm">Unravel Data</strong></a><strong class="lo jm">, as well as check out my </strong><a class="au ll" href="https://www.amazon.com/Self-Service-Data-Roadmap-Democratize-Insight-dp-1492075256/dp/1492075256" rel="noopener ugc nofollow" target="_blank"><strong class="lo jm">book</strong></a><strong class="lo jm"> and </strong><a class="au ll" href="https://open.spotify.com/show/5tt1lS8NlGaziNGRSxZd4y" rel="noopener ugc nofollow" target="_blank"><strong class="lo jm">podcast</strong></a><strong class="lo jm">.</strong></p></blockquote><figure class="mh mi mj mk gx ml"><div class="m fs l do"><div class="pc mn l"></div></div><figcaption class="mo bl gn gl gm mp mq bm b bn bo cn">My interview on Data Standard Podcast sharing ML project experiences listed in this blog</figcaption></figure></div></div></section></div></div></article>