<article class="meteredContent"><div class="l"><div class="gg gh gi gj gk gl gm ce gn ch l"></div><div class="l"><header class="pw-post-byline-header go gp gq gr gs gt gu gv gw gx l"><div class="o gy u"><div class="o"><div class="fj l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@praveennellihela?source=post_page-----9058f85de5f7--------------------------------" rel="noopener follow"><div class="l do"><img alt="Praveen Nellihela" class="l ch fl gz ha fp" height="48" loading="lazy" src="https://miro.medium.com/fit/c/96/96/1*gmHZuvYu8lZgmQYl7S_P6w.jpeg" width="48"/><div class="fk fl l gz ha fo aq"></div></div></a></div><div class="l"><div class="pw-author bm b dm dn ga"><div class="hb o hc"><div><div aria-hidden="false" class="ci"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@praveennellihela?source=post_page-----9058f85de5f7--------------------------------" rel="noopener follow">Praveen Nellihela</a></div></div><div class="hd he hf hg hh d"><span><a class="bm b hi bo hj hk hl hm hn ho hp bd bz hq hr hs cd cf cg ch ci cj" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F359bd37be9f3&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-succesful-machine-learning-models-through-proper-datasets-9058f85de5f7&amp;user=Praveen+Nellihela&amp;userId=359bd37be9f3&amp;source=post_page-359bd37be9f3----9058f85de5f7---------------------follow_byline-----------" rel="noopener follow">Follow</a></span></div></div></div><div class="o ao ht"><p class="pw-published-date bm b bn bo cn"><span>Sep 6</span></p><div aria-hidden="true" class="hu ci"><span aria-hidden="true" class="l"><span class="bm b bn bo cn">·</span></span></div><div class="pw-reading-time bm b bn bo cn">13 min read</div><div aria-hidden="true" class="hu ci"><span aria-hidden="true" class="l"><span class="bm b bn bo cn">·</span></span></div><div class="hx l"><div aria-hidden="false" class="l"><button class="l dz hv bb"><div class="j i d"><div><div aria-hidden="false" class="ci"><svg fill="none" height="20" viewbox="0 0 20 20" width="20"><path d="M12.4 12.77l-1.81 4.99a.63.63 0 0 1-1.18 0l-1.8-4.99a.63.63 0 0 0-.38-.37l-4.99-1.81a.62.62 0 0 1 0-1.18l4.99-1.8a.63.63 0 0 0 .37-.38l1.81-4.99a.63.63 0 0 1 1.18 0l1.8 4.99a.63.63 0 0 0 .38.37l4.99 1.81a.63.63 0 0 1 0 1.18l-4.99 1.8a.63.63 0 0 0-.37.38z" fill="#FFC017"></path></svg></div></div></div><div class="h k hy hz ia"><svg class="hw" fill="none" height="20" viewbox="0 0 20 20" width="20"><path d="M12.4 12.77l-1.81 4.99a.63.63 0 0 1-1.18 0l-1.8-4.99a.63.63 0 0 0-.38-.37l-4.99-1.81a.62.62 0 0 1 0-1.18l4.99-1.8a.63.63 0 0 0 .37-.38l1.81-4.99a.63.63 0 0 1 1.18 0l1.8 4.99a.63.63 0 0 0 .38.37l4.99 1.81a.63.63 0 0 1 0 1.18l-4.99 1.8a.63.63 0 0 0-.37.38z" fill="#FFC017"></path></svg><p class="bm b bn bo cn">Member-only</p></div></button></div></div></div></div></div><div class="o ao"><div class="h k ib ic id"><div class="ie l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on twitter" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci if dw ig"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M20 5.34c-.67.41-1.4.7-2.18.87a3.45 3.45 0 0 0-5.02-.1 3.49 3.49 0 0 0-1.02 2.47c0 .28.03.54.07.8a9.91 9.91 0 0 1-7.17-3.66 3.9 3.9 0 0 0-.5 1.74 3.6 3.6 0 0 0 1.56 2.92 3.36 3.36 0 0 1-1.55-.44V10c0 1.67 1.2 3.08 2.8 3.42-.3.06-.6.1-.94.12l-.62-.06a3.5 3.5 0 0 0 3.24 2.43 7.34 7.34 0 0 1-4.36 1.49l-.81-.05a9.96 9.96 0 0 0 5.36 1.56c6.4 0 9.91-5.32 9.9-9.9v-.5c.69-.49 1.28-1.1 1.74-1.81-.63.3-1.3.48-2 .56A3.33 3.33 0 0 0 20 5.33" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="ie l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on facebook" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci if dw ig"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M19.75 12.04c0-4.3-3.47-7.79-7.75-7.79a7.77 7.77 0 0 0-5.9 12.84 7.77 7.77 0 0 0 4.69 2.63v-5.49h-1.9v-2.2h1.9v-1.62c0-1.88 1.14-2.9 2.8-2.9.8 0 1.49.06 1.69.08v1.97h-1.15c-.91 0-1.1.43-1.1 1.07v1.4h2.17l-.28 2.2h-1.88v5.52a7.77 7.77 0 0 0 6.7-7.71" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="ie l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on linkedin" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci if dw ig"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M19.75 5.39v13.22a1.14 1.14 0 0 1-1.14 1.14H5.39a1.14 1.14 0 0 1-1.14-1.14V5.39a1.14 1.14 0 0 1 1.14-1.14h13.22a1.14 1.14 0 0 1 1.14 1.14zM8.81 10.18H6.53v7.3H8.8v-7.3zM9 7.67a1.31 1.31 0 0 0-1.3-1.32h-.04a1.32 1.32 0 0 0 0 2.64A1.31 1.31 0 0 0 9 7.71v-.04zm8.46 5.37c0-2.2-1.4-3.05-2.78-3.05a2.6 2.6 0 0 0-2.3 1.18h-.07v-1h-2.14v7.3h2.28V13.6a1.51 1.51 0 0 1 1.36-1.63h.09c.72 0 1.26.45 1.26 1.6v3.91h2.28l.02-4.43z" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="l fr"><div><div aria-hidden="false" class="ci"><button class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci if dw ig"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path clip-rule="evenodd" d="M3.57 14.67c0-.57.13-1.11.38-1.6l.02-.02v-.02l.02-.02c0-.02 0-.02.02-.02.12-.26.3-.52.57-.8L7.78 9v-.02l.01-.02c.44-.41.91-.7 1.44-.85a4.87 4.87 0 0 0-1.19 2.36A5.04 5.04 0 0 0 8 11.6L6.04 13.6c-.19.19-.32.4-.38.65a2 2 0 0 0 0 .9c.08.2.2.4.38.57l1.29 1.31c.27.28.62.42 1.03.42.42 0 .78-.14 1.06-.42l1.23-1.25.79-.78 1.15-1.16c.08-.09.19-.22.28-.4.1-.2.15-.42.15-.67 0-.16-.02-.3-.06-.45l-.02-.02v-.02l-.07-.14s0-.03-.04-.06l-.06-.13-.02-.02c0-.02 0-.03-.02-.05a.6.6 0 0 0-.14-.16l-.48-.5c0-.04.02-.1.04-.15l.06-.12 1.17-1.14.09-.09.56.57c.02.04.08.1.16.18l.05.04.03.06.04.05.03.04.04.06.1.14.02.02c0 .02.01.03.03.04l.1.2v.02c.1.16.2.38.3.68a1 1 0 0 1 .04.25 3.2 3.2 0 0 1 .02 1.33 3.49 3.49 0 0 1-.95 1.87l-.66.67-.97.97-1.56 1.57a3.4 3.4 0 0 1-2.47 1.02c-.97 0-1.8-.34-2.49-1.03l-1.3-1.3a3.55 3.55 0 0 1-1-2.51v-.01h-.02v.02zm5.39-3.43c0-.19.02-.4.07-.63.13-.74.44-1.37.95-1.87l.66-.67.97-.98 1.56-1.56c.68-.69 1.5-1.03 2.47-1.03.97 0 1.8.34 2.48 1.02l1.3 1.32a3.48 3.48 0 0 1 1 2.48c0 .58-.11 1.11-.37 1.6l-.02.02v.02l-.02.04c-.14.27-.35.54-.6.8L16.23 15l-.01.02-.01.02c-.44.42-.92.7-1.43.83a4.55 4.55 0 0 0 1.23-3.52L18 10.38c.18-.21.3-.42.35-.65a2.03 2.03 0 0 0-.01-.9 1.96 1.96 0 0 0-.36-.58l-1.3-1.3a1.49 1.49 0 0 0-1.06-.42c-.42 0-.77.14-1.06.4l-1.2 1.27-.8.8-1.16 1.15c-.08.08-.18.21-.29.4a1.66 1.66 0 0 0-.08 1.12l.02.03v.02l.06.14s.01.03.05.06l.06.13.02.02.01.02.01.02c.05.08.1.13.14.16l.47.5c0 .04-.02.09-.04.15l-.06.12-1.15 1.15-.1.08-.56-.56a2.3 2.3 0 0 0-.18-.19c-.02-.01-.02-.03-.02-.04l-.02-.02a.37.37 0 0 1-.1-.12c-.03-.03-.05-.04-.05-.06l-.1-.15-.02-.02-.02-.04-.08-.17v-.02a5.1 5.1 0 0 1-.28-.69 1.03 1.03 0 0 1-.04-.26c-.06-.23-.1-.46-.1-.7v.01z" fill="#A8A8A8" fill-rule="evenodd"></path></svg></span></button></div></div></div><div class="ih o ao"><span><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9058f85de5f7&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-succesful-machine-learning-models-through-proper-datasets-9058f85de5f7&amp;source=--------------------------bookmark_header-----------" rel="noopener follow"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="au de aw ax ay az ba if bc hv ij ik il"><svg aria-label="Add to list bookmark button" class="ii" fill="none" height="25" viewbox="0 0 25 25" width="25"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg></button></a></span></div></div><div class="ck im"><div><div aria-hidden="false" class="ci"></div></div></div></div></div><div class="in io ip j i d"><div class="fj l"><span><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9058f85de5f7&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuilding-succesful-machine-learning-models-through-proper-datasets-9058f85de5f7&amp;source=--------------------------bookmark_header-----------" rel="noopener follow"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="au de aw iq ay az ba ir bc hv cd o ao is it il"><svg aria-label="Add to list bookmark button" class="ii" fill="none" height="25" viewbox="0 0 25 25" width="25"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg><p class="bm b bn bo cn">Save</p></button></a></span></div><div class="iu l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on twitter" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci if dw ig"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M20 5.34c-.67.41-1.4.7-2.18.87a3.45 3.45 0 0 0-5.02-.1 3.49 3.49 0 0 0-1.02 2.47c0 .28.03.54.07.8a9.91 9.91 0 0 1-7.17-3.66 3.9 3.9 0 0 0-.5 1.74 3.6 3.6 0 0 0 1.56 2.92 3.36 3.36 0 0 1-1.55-.44V10c0 1.67 1.2 3.08 2.8 3.42-.3.06-.6.1-.94.12l-.62-.06a3.5 3.5 0 0 0 3.24 2.43 7.34 7.34 0 0 1-4.36 1.49l-.81-.05a9.96 9.96 0 0 0 5.36 1.56c6.4 0 9.91-5.32 9.9-9.9v-.5c.69-.49 1.28-1.1 1.74-1.81-.63.3-1.3.48-2 .56A3.33 3.33 0 0 0 20 5.33" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="iu l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on facebook" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci if dw ig"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M19.75 12.04c0-4.3-3.47-7.79-7.75-7.79a7.77 7.77 0 0 0-5.9 12.84 7.77 7.77 0 0 0 4.69 2.63v-5.49h-1.9v-2.2h1.9v-1.62c0-1.88 1.14-2.9 2.8-2.9.8 0 1.49.06 1.69.08v1.97h-1.15c-.91 0-1.1.43-1.1 1.07v1.4h2.17l-.28 2.2h-1.88v5.52a7.77 7.77 0 0 0 6.7-7.71" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="iu l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on linkedin" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci if dw ig"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M19.75 5.39v13.22a1.14 1.14 0 0 1-1.14 1.14H5.39a1.14 1.14 0 0 1-1.14-1.14V5.39a1.14 1.14 0 0 1 1.14-1.14h13.22a1.14 1.14 0 0 1 1.14 1.14zM8.81 10.18H6.53v7.3H8.8v-7.3zM9 7.67a1.31 1.31 0 0 0-1.3-1.32h-.04a1.32 1.32 0 0 0 0 2.64A1.31 1.31 0 0 0 9 7.71v-.04zm8.46 5.37c0-2.2-1.4-3.05-2.78-3.05a2.6 2.6 0 0 0-2.3 1.18h-.07v-1h-2.14v7.3h2.28V13.6a1.51 1.51 0 0 1 1.36-1.63h.09c.72 0 1.26.45 1.26 1.6v3.91h2.28l.02-4.43z" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="l fr"><div><div aria-hidden="false" class="ci"><button class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci if dw ig"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path clip-rule="evenodd" d="M3.57 14.67c0-.57.13-1.11.38-1.6l.02-.02v-.02l.02-.02c0-.02 0-.02.02-.02.12-.26.3-.52.57-.8L7.78 9v-.02l.01-.02c.44-.41.91-.7 1.44-.85a4.87 4.87 0 0 0-1.19 2.36A5.04 5.04 0 0 0 8 11.6L6.04 13.6c-.19.19-.32.4-.38.65a2 2 0 0 0 0 .9c.08.2.2.4.38.57l1.29 1.31c.27.28.62.42 1.03.42.42 0 .78-.14 1.06-.42l1.23-1.25.79-.78 1.15-1.16c.08-.09.19-.22.28-.4.1-.2.15-.42.15-.67 0-.16-.02-.3-.06-.45l-.02-.02v-.02l-.07-.14s0-.03-.04-.06l-.06-.13-.02-.02c0-.02 0-.03-.02-.05a.6.6 0 0 0-.14-.16l-.48-.5c0-.04.02-.1.04-.15l.06-.12 1.17-1.14.09-.09.56.57c.02.04.08.1.16.18l.05.04.03.06.04.05.03.04.04.06.1.14.02.02c0 .02.01.03.03.04l.1.2v.02c.1.16.2.38.3.68a1 1 0 0 1 .04.25 3.2 3.2 0 0 1 .02 1.33 3.49 3.49 0 0 1-.95 1.87l-.66.67-.97.97-1.56 1.57a3.4 3.4 0 0 1-2.47 1.02c-.97 0-1.8-.34-2.49-1.03l-1.3-1.3a3.55 3.55 0 0 1-1-2.51v-.01h-.02v.02zm5.39-3.43c0-.19.02-.4.07-.63.13-.74.44-1.37.95-1.87l.66-.67.97-.98 1.56-1.56c.68-.69 1.5-1.03 2.47-1.03.97 0 1.8.34 2.48 1.02l1.3 1.32a3.48 3.48 0 0 1 1 2.48c0 .58-.11 1.11-.37 1.6l-.02.02v.02l-.02.04c-.14.27-.35.54-.6.8L16.23 15l-.01.02-.01.02c-.44.42-.92.7-1.43.83a4.55 4.55 0 0 0 1.23-3.52L18 10.38c.18-.21.3-.42.35-.65a2.03 2.03 0 0 0-.01-.9 1.96 1.96 0 0 0-.36-.58l-1.3-1.3a1.49 1.49 0 0 0-1.06-.42c-.42 0-.77.14-1.06.4l-1.2 1.27-.8.8-1.16 1.15c-.08.08-.18.21-.29.4a1.66 1.66 0 0 0-.08 1.12l.02.03v.02l.06.14s.01.03.05.06l.06.13.02.02.01.02.01.02c.05.08.1.13.14.16l.47.5c0 .04-.02.09-.04.15l-.06.12-1.15 1.15-.1.08-.56-.56a2.3 2.3 0 0 0-.18-.19c-.02-.01-.02-.03-.02-.04l-.02-.02a.37.37 0 0 1-.1-.12c-.03-.03-.05-.04-.05-.06l-.1-.15-.02-.02-.02-.04-.08-.17v-.02a5.1 5.1 0 0 1-.28-.69 1.03 1.03 0 0 1-.04-.26c-.06-.23-.1-.46-.1-.7v.01z" fill="#A8A8A8" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></header><span class="l"></span><section><div><div class="fo as ja jb jc jd"></div><div class="je jf jg jh ji"><div class=""><h1 class="pw-post-title jj jk jl bm jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ga" id="41fd">How to Build a Proper Dataset</h1></div><div class=""><h2 class="pw-subtitle-paragraph ki jk jl bm b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz cn" id="f936">Because bad data leads to bad models</h2></div><figure class="lb lc ld le gx lf gl gm paragraph-image"><div class="lg lh do li ce lj" role="button" tabindex="0"><div class="gl gm la"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/1*v1jntTVy1GN-ErSyyj5ZQA.jpeg 640w, https://miro.medium.com/max/720/1*v1jntTVy1GN-ErSyyj5ZQA.jpeg 720w, https://miro.medium.com/max/750/1*v1jntTVy1GN-ErSyyj5ZQA.jpeg 750w, https://miro.medium.com/max/786/1*v1jntTVy1GN-ErSyyj5ZQA.jpeg 786w, https://miro.medium.com/max/828/1*v1jntTVy1GN-ErSyyj5ZQA.jpeg 828w, https://miro.medium.com/max/1100/1*v1jntTVy1GN-ErSyyj5ZQA.jpeg 1100w, https://miro.medium.com/max/1400/1*v1jntTVy1GN-ErSyyj5ZQA.jpeg 1400w"/><img alt="" class="ce lk ll c" height="467" loading="eager" role="presentation" width="700"/></picture></div></div><figcaption class="lm bl gn gl gm ln lo bm b bn bo cn">Photo by <a class="au lp" href="https://unsplash.com/@tabeaschimpf?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Tabea Schimpf</a> on <a class="au lp" href="https://unsplash.com/s/photos/map?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 class="lq lr jl bm ls lt lu lv lw lx ly lz ma kr mb ks mc ku md kv me kx mf ky mg mh ga" id="a801">Why does a proper dataset matter?</h1><p class="pw-post-body-paragraph mi mj jl mk b ml mm km mn mo mp kp mq mr ms mt mu mv mw mx my mz na nb nc nd je ga" id="2838">Fundamentally, a machine learning model is something that configures itself to predict an outcome, based on the training data. If you feed the model training data that does not represent the data the model will face when actually being used, you cannot expect anything but flawed predictions. This article will give you insights on creating a good dataset that is able to represent data the model will face when it is being used in the real world.</p><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="1315">But before we start building great datasets, first, we must understand the <strong class="mk jm">components</strong> of a dataset (classes, labels, features, and feature vectors). Then, we will move on to <strong class="mk jm">data preparation</strong>. This is where we <strong class="mk jm">handle missing features</strong> and <strong class="mk jm">scale features</strong>. Next, we discuss why and how we should <strong class="mk jm">partition the dataset. </strong>Finally, we will look at why we should have a<strong class="mk jm"> final look</strong> at our data and how to find problems that could still exist, such as <strong class="mk jm">outliers</strong> and <strong class="mk jm">mislabeled</strong> <strong class="mk jm">data</strong>.</p><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="7dbd">Let’s get started.</p><h1 class="lq lr jl bm ls lt lu lv lw lx ly lz ma kr mb ks mc ku md kv me kx mf ky mg mh ga" id="f985">Components of a dataset</h1><h2 class="nj lr jl bm ls nk nl nm lw nn no np ma mr nq nr mc mv ns nt me mz nu nv mg nw ga" id="127b">Classes and Labels</h2><p class="pw-post-body-paragraph mi mj jl mk b ml mm km mn mo mp kp mq mr ms mt mu mv mw mx my mz na nb nc nd je ga" id="87ad">In classification tasks, the input features are mapped to discrete output variables. For example, by considering the input data, the model predicts whether the output is a “dog”, “cat”, “horse” etc. These output variables are defined as <strong class="mk jm">classes</strong>. An identifier called a <strong class="mk jm">label </strong>is given to each input in the training data to represent these classes.</p><figure class="lb lc ld le gx lf gl gm paragraph-image"><div class="lg lh do li ce lj" role="button" tabindex="0"><div class="gl gm nx"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/1*_G6qaEllhLEY_Mh10w1_Sw.png 640w, https://miro.medium.com/max/720/1*_G6qaEllhLEY_Mh10w1_Sw.png 720w, https://miro.medium.com/max/750/1*_G6qaEllhLEY_Mh10w1_Sw.png 750w, https://miro.medium.com/max/786/1*_G6qaEllhLEY_Mh10w1_Sw.png 786w, https://miro.medium.com/max/828/1*_G6qaEllhLEY_Mh10w1_Sw.png 828w, https://miro.medium.com/max/1100/1*_G6qaEllhLEY_Mh10w1_Sw.png 1100w, https://miro.medium.com/max/1400/1*_G6qaEllhLEY_Mh10w1_Sw.png 1400w"/><img alt="" class="ce lk ll c" height="146" loading="lazy" role="presentation" width="700"/></picture></div></div><figcaption class="lm bl gn gl gm ln lo bm b bn bo cn">How a classification model classifies input data into classes.</figcaption></figure><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="c598">To a model, the inputs are just numbers. A model does not care or know the distinction between an image of a dog or a voice sample. This also applies to the labels. As a result, classes can be represented in any way we want. In practice, we often use integer values starting with 0 to map the labels to their respective class. An example is shown below.</p><pre class="lb lc ld le gx ny bs nz oa dz ob"><span class="ga nj lr jl ob b dm oc od l oe of" id="7bb6">┌───────┬────────────┐<br/>│ Label │   Class    │<br/>├───────┼────────────┤<br/>│     0 │ person     │<br/>│     1 │ bicycle    │<br/>│     2 │ car        │<br/>│     3 │ motorcycle │<br/>│     4 │ airplane   │<br/>│     5 │ bus        │<br/>│     6 │ train      │<br/>│     7 │ truck      │<br/>│     8 │ boat       │<br/>│     9 │ traffic    │<br/>└───────┴────────────┘<br/>The table is an exerpt from the <a class="au lp" href="https://cocodataset.org/#home" rel="noopener ugc nofollow" target="_blank">COCO dataset</a>, showing its classes and labels.</span></pre><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="3af6">In the above example, every input that represents a bicycle is labeled 1 and every input representing a boat is labeled 8. You may now wonder, what exactly are we labeling? What is actually the input of a person or a boat? This is where the all-important <strong class="mk jm">features</strong> come in.</p><h2 class="nj lr jl bm ls nk nl nm lw nn no np ma mr nq nr mc mv ns nt me mz nu nv mg nw ga" id="160f">Features and Feature Vectors</h2><p class="pw-post-body-paragraph mi mj jl mk b ml mm km mn mo mp kp mq mr ms mt mu mv mw mx my mz na nb nc nd je ga" id="60ec">Features are the inputs used by the model to produce a label as output. These are numbers, as mentioned above, and represent different things depending on the task. For example, in the <a class="au lp" href="https://www.kaggle.com/datasets/uciml/iris" rel="noopener ugc nofollow" target="_blank">iris dataset</a> which contains data about three species of Iris flowers, the <strong class="mk jm">features</strong> are the dimensions of the sepals and petals.</p><pre class="lb lc ld le gx ny bs nz oa dz ob"><span class="ga nj lr jl ob b dm oc od l oe of" id="60da">The <strong class="ob jm">four available features </strong>in the iris dataset for the iris flower species known as Setosa are shown below. <br/>(only the first 5 rows of the dataset are shown) </span><span class="ga nj lr jl ob b dm og od l oe of" id="5889">   <strong class="ob jm">Sepal.Length  Sepal.Width  Petal.Length  Petal.Width   </strong>Species <br/>1          5.1          3.5           1.4          0.2    setosa<br/>2          4.9          3.0           1.4          0.2    setosa<br/>3          4.7          3.2           1.3          0.2    setosa<br/>4          4.6          3.1           1.5          0.2    setosa<br/>5          5.0          3.6           1.4          0.2    setosa</span></pre><blockquote class="oh oi oj"><p class="mi mj ok mk b ml ne km mn mo nf kp mq ol ng mt mu om nh mx my on ni nb nc nd je ga" id="3d26">Note: The <strong class="mk jm"><em class="jl">classes</em></strong> in the above dataset are the different species of Iris flowers, <strong class="mk jm">Setosa</strong>, <strong class="mk jm">Virginica</strong>, and <strong class="mk jm">Versicolor</strong> with <strong class="mk jm"><em class="jl">labels</em></strong> 0, 1 and 2 given to these classes respectively.</p></blockquote><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="073f">Thus, features are simply the numbers that we will use as inputs to the machine learning model. When the machine learning model is trained, the model learns relationships between the input features and the output labels. We make an assumption here that there actually is a relationship between the features and the labels. In cases where there are no relationships to learn, the model may fail to train.</p><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="c7d4">Once training is concluded, the learned relationships are used to predict output labels of input <strong class="mk jm">feature vectors</strong> (sets of features given as input) with <strong class="mk jm">unknown</strong> class labels. If the model continues to make bad predictions, a reason might be that the features used to train the model were not adequate enough to identify a good relationship. This is why choosing the correct features matters at the start of any machine learning project. More information on choosing good features and why bad features should be ignored can be found in my post below.</p><div class="oo op gt gv oq or"><a href="/feature-selection-choosing-the-right-features-for-your-machine-learning-algorithm-379bda9f3e05" rel="noopener follow" target="_blank"><div class="os o fr"><div class="ot o da dx en ou"><h2 class="bm jm dm bo fs ov fu fv ow fx fz jk ga">Feature Selection: Choosing the Right Features for Your Machine Learning Algorithm</h2><div class="ox l"><h3 class="bm b dm bo fs ov fu fv ow fx fz cn">Sometimes, less is more</h3></div><div class="oy l"><p class="bm b hi bo fs ov fu fv ow fx fz cn">towardsdatascience.com</p></div></div><div class="oz l"><div class="pa l pb pc pd oz pe lk or"></div></div></div></a></div><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="4e40">Features can be of different types, such as floating point numbers, integers, ordinal values (natural, ordered categories where the difference between values are not always the same), and categorical values (where numbers are used as codes, e.g male=0 and female=1).</p><blockquote class="oh oi oj"><p class="mi mj ok mk b ml ne km mn mo nf kp mq ol ng mt mu om nh mx my on ni nb nc nd je ga" id="ddee">Recap: features are numbers that represent something we know, that can help build a relationship to the output label. Take a look at some rows of the Iris dataset below, now showing all components, features class and labels. Each row containing all four features is one feature vector.</p></blockquote><pre class="lb lc ld le gx ny bs nz oa dz ob"><span class="ga nj lr jl ob b dm oc od l oe of" id="9527">                      Features                          <br/>          ________________|_____________________       Class   Label<br/>         |                                      |        |       |<br/><strong class="ob jm">Sepal.Length  Sepal.Width  Petal.Length  Petal.Width  Species</strong> <br/>    5.1          3.5           1.4          0.2       setosa     0<br/>    7.1          3.0           5.9          2.1       virginica  1<br/>    4.7          3.2           1.3          0.2       setosa     0<br/>    6.5          3.0           5.8          2.2       virginica  1<br/>    6.9          3.1           4.9          1.5       versicolor 2</span></pre><h1 class="lq lr jl bm ls lt lu lv lw lx ly lz ma kr mb ks mc ku md kv me kx mf ky mg mh ga" id="2aea">Preparing the data</h1><p class="pw-post-body-paragraph mi mj jl mk b ml mm km mn mo mp kp mq mr ms mt mu mv mw mx my mz na nb nc nd je ga" id="092a">Now that we have a good grip on what a dataset contains, there are two important things to consider before we start building our great dataset:</p><ul class=""><li class="pf pg jl mk b ml ne mo nf mr ph mv pi mz pj nd pk pl pm pn ga" id="0188">How to handle missing feature values</li><li class="pf pg jl mk b ml po mo pp mr pq mv pr mz ps nd pk pl pm pn ga" id="3a69">Feature Scaling</li></ul><h2 class="nj lr jl bm ls nk nl nm lw nn no np ma mr nq nr mc mv ns nt me mz nu nv mg nw ga" id="abf2">Dealing with missing features</h2><p class="pw-post-body-paragraph mi mj jl mk b ml mm km mn mo mp kp mq mr ms mt mu mv mw mx my mz na nb nc nd je ga" id="b03b">You may come across cases where there are missing features in your data, for example, you may have forgotten to make a measurement, or some data for a sample has been corrupted. Most machine learning models do not have the ability to accept missing data, so we must fill in these values with some data.</p><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="7afd">There are two approaches you can take in such scenarios. You could either add a value that falls far outside the feature’s range with the basis that the model will pay less significance to it or, use the mean value of the features over the data set in place of the missing value.</p><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="fa5f">In the example below, some features are missing, indicated by blank spaces.</p><pre class="lb lc ld le gx ny bs nz oa dz ob"><span class="ga nj lr jl ob b dm oc od l oe of" id="a811"><strong class="ob jm">Sepal.Length  Sepal.Width  Petal.Length  Petal.Width  Label</strong> <br/>    5.1          3.5           1.4                      0<br/>    7.1                        5.9          2.1         1<br/>    4.7          3.2                        0.2         0<br/>                 3.0           5.8          2.2         1<br/>    6.9          3.1           4.9          1.5         2</span></pre><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="4732">The mean for all the features is calculated and shown in the table below</p><pre class="lb lc ld le gx ny bs nz oa dz ob"><span class="ga nj lr jl ob b dm oc od l oe of" id="d657"><strong class="ob jm">Sepal.Length  Sepal.Width  Petal.Length  Petal.Width </strong><br/>    5.95         3.2           4.5          1.5          </span></pre><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="5dea">By replacing the missing values with the mean values, we can obtain a dataset that can be used to train a model. This is not better than having real data but should be good enough to use when data is missing. An alternative if the dataset is large enough is to use the mode (most occurring value) by identifying it through a generated histogram.</p><h2 class="nj lr jl bm ls nk nl nm lw nn no np ma mr nq nr mc mv ns nt me mz nu nv mg nw ga" id="87df">Feature scaling</h2><p class="pw-post-body-paragraph mi mj jl mk b ml mm km mn mo mp kp mq mr ms mt mu mv mw mx my mz na nb nc nd je ga" id="c534">Oftentimes, feature vectors that are made of different features can have multiple different ranges. For example, while a set of features will be between 0 and 1, another feature can have values between 0 to 100,000. Another will be between -100 to 100. As a result, some features will dominate others because of their larger range, which causes the model to suffer in accuracy. To overcome this problem, <strong class="mk jm">feature scaling</strong> is used.</p><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="f409">To understand this concept as well as to reinforce what we learned in the above sections, we will create a synthetic data set and look at it.</p><pre class="lb lc ld le gx ny bs nz oa dz ob"><span class="ga nj lr jl ob b dm oc od l oe of" id="bf2b">┌────────┬─────┬──────┬──────┬───────────┬────────┬───────┐<br/>│ <strong class="ob jm">Sample</strong> │ <strong class="ob jm">f1  </strong>│<strong class="ob jm">  f2  </strong>│ <strong class="ob jm"> f3 </strong> │<strong class="ob jm">    f4     </strong>│ <strong class="ob jm">  f5   </strong>│<strong class="ob jm"> Label</strong> │<br/>├────────┼─────┼──────┼──────┼───────────┼────────┼───────┤<br/>│      0 │  30 │ 3494 │ 6042 │  0.000892 │ 0.4422 │     0 │<br/>│      1 │  17 │ 6220 │ 7081 │ 0.0003064 │ 0.5731 │     1 │<br/>│      2 │  <strong class="ob jm"><em class="ok">16 │ 3490 │ 1605 │ 0.0002371 │   0.23</em></strong> │     0 │<br/>│      3 │   5 │ 9498 │ 7650 │ 0.0008715 │ 0.8401 │     1 │<br/>│      4 │  48 │ 8521 │ 6680 │ 0.0003957 │ 0.3221 │     1 │<br/>│      5 │  64 │ 2887 │ 6073 │ 0.0005087 │ 0.6635 │     1 │<br/>│      6 │  94 │ 6953 │ 7970 │ 0.0005284 │ 0.9112 │     0 │<br/>│      7 │  39 │ 6837 │ 9967 │ 0.0004239 │ 0.4788 │     1 │<br/>│      8 │  85 │ 9377 │ 4953 │ 0.0003521 │ 0.5061 │     0 │<br/>│      9 │  46 │ 4597 │ 2337 │ 0.0004158 │ 0.8139 │     0 │<br/>└────────┴─────┴──────┴──────┴───────────┴────────┴───────┘<br/>The first column is the sample number. <br/>Each row of a sample is an input to the model, given as a <strong class="ob jm">feature vector.<br/></strong>A feature vector is represented by 5 <strong class="ob jm">features</strong> for each sample<br/>- feature set is {f1, f2, f3, f4, f5}<br/>- Typically the full feature vector is refered to with the uppercase letter (F).<br/>- Feature vector for sample 3 can be refered to as F3. <br/>One feature vector is highlighted in bold for sample 2 in the table.<br/>The last column is the <strong class="ob jm">label</strong>. There are two <strong class="ob jm">classes</strong>, represented by the labels 0 and 1.</span><span class="ga nj lr jl ob b dm og od l oe of" id="1311">Notice how samples start with 0. This is because we work with Python and Python is 0 indexed.</span></pre><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="fd8f">Now to the scaling part. You can see in our synthetic data table that different features have different ranges. Let's look at all the features and consider their minimum, maximum, average, and range values.</p><pre class="lb lc ld le gx ny bs nz oa dz ob"><span class="ga nj lr jl ob b dm oc od l oe of" id="acfe">┌──────────┬───────────┬───────────┬──────────┬────────────┐<br/>│ Feature  │   Range   │  Minimum  │ Maximum  │  Average   │<br/>├──────────┼───────────┼───────────┼──────────┼────────────┤<br/>│ f1       │        89 │         5 │       94 │       44.4 │<br/>│ f2       │      6611 │      2887 │     9498 │     6187.4 │<br/>│ f3       │      8362 │      1605 │     9967 │     6035.8 │<br/>│ f4       │ 0.0006549 │ 0.0002371 │ 0.000892 │ 0.00049316 │<br/>│ f5       │    0.6812 │      0.23 │   0.9112 │     0.5781 │<br/>└──────────┴───────────┴───────────┴──────────┴────────────┘</span><span class="ga nj lr jl ob b dm og od l oe of" id="572f">Notice how the features have widely varying ranges. This means that we should carry out feature scaling.</span></pre><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="400c">Let’s look at two ways of scaling. First, we consider the easiest method, known as <strong class="mk jm">mean centering</strong>. This is done by subtracting the average value of the feature over the whole dataset. Average over the whole set simply means the sum of each value divided by the total number of values.</p><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="34b5">For f1, the average value is 44.4. Therefore, to center f1, we replace each sample value belonging to the f1 feature with the value-44.4. For sample 0, it's 30–44.4, for sample 2, it's 17–44.4, and so on. Doing this for all values, we get the table below.</p><pre class="lb lc ld le gx ny bs nz oa dz ob"><span class="ga nj lr jl ob b dm oc od l oe of" id="5a90">┌────────┬───────┬─────────┬─────────┬─────────────┬─────────┐<br/>│ Sample │  f1   │   f2    │   f3    │     f4      │   f5    │<br/>├────────┼───────┼─────────┼─────────┼─────────────┼─────────┤<br/>│      0 │ -14.4 │ -2693.4 │     6.2 │  0.00039884 │ -0.1359 │<br/>│      1 │ -27.4 │    32.6 │  1045.2 │ -0.00018676 │  -0.005 │<br/>│      2 │ -28.4 │ -2697.4 │ -4430.8 │ -0.00025606 │ -0.3481 │<br/>│      3 │ -39.4 │  3310.6 │  1614.2 │  0.00037834 │   0.262 │<br/>│      4 │   3.6 │  2333.6 │   644.2 │ -0.00009746 │  -0.256 │<br/>│      5 │  19.6 │ -3300.4 │    37.2 │  0.00001554 │  0.0854 │<br/>│      6 │  49.6 │   765.6 │  1934.2 │  0.00003524 │  0.3331 │<br/>│      7 │  -5.4 │   649.6 │  3931.2 │ -0.00006926 │ -0.0993 │<br/>│      8 │  40.6 │  3189.6 │ -1082.8 │ -0.00014106 │  -0.072 │<br/>│      9 │   1.6 │ -1590.4 │ -3698.8 │ -0.00007736 │  0.2358 │<br/>└────────┴───────┴─────────┴─────────┴─────────────┴─────────┘<br/>The synthetic data set we created, after mean centering. <br/>Note that now, the average value for each feature is 0. In other words the center for each feature is 0 and values can be above or below this center.</span></pre><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="913a">Mean centering can be sufficient for feature scaling. But we can go a bit further. Notice how the ranges are not the same even if we conduct mean centering. By performing what's called <strong class="mk jm">normalization</strong>, or <strong class="mk jm">standardization, </strong>we can change the spread of data around the mean (standard deviation) to be in the same range. In other words, we change the standard deviation to 1. Because we also perform mean centering, when normalizing data, the features will have a mean of 0 in addition to the standard deviation being 1.</p><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="7c31">This can be done simply, if we have a feature value x, by replacing x with value z where:</p><figure class="lb lc ld le gx lf gl gm paragraph-image"><div class="gl gm pt"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 159px" srcset="https://miro.medium.com/max/640/0*2OH-YZ20f16S0U1M.png 640w, https://miro.medium.com/max/720/0*2OH-YZ20f16S0U1M.png 720w, https://miro.medium.com/max/750/0*2OH-YZ20f16S0U1M.png 750w, https://miro.medium.com/max/786/0*2OH-YZ20f16S0U1M.png 786w, https://miro.medium.com/max/828/0*2OH-YZ20f16S0U1M.png 828w, https://miro.medium.com/max/1100/0*2OH-YZ20f16S0U1M.png 1100w, https://miro.medium.com/max/318/0*2OH-YZ20f16S0U1M.png 318w"/><img alt="" class="ce lk ll c" height="78" loading="lazy" role="presentation" width="159"/></picture></div><figcaption class="lm bl gn gl gm ln lo bm b bn bo cn">The equation<a class="au lp" href="https://en.wikipedia.org/wiki/Standard_score" rel="noopener ugc nofollow" target="_blank"> to calculate standard score.</a></figcaption></figure><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="b52f">where: <em class="ok">μ</em> is the <a class="au lp" href="https://en.wikipedia.org/wiki/Mean" rel="noopener ugc nofollow" target="_blank">mean</a> and <em class="ok">σ</em> is the <a class="au lp" href="https://en.wikipedia.org/wiki/Standard_deviation" rel="noopener ugc nofollow" target="_blank">standard deviation</a> of each feature across<br/>the dataset.</p><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="b334">Fortunately, since we are working with Python and Numpy, this can be done by a single line of code.</p><pre class="lb lc ld le gx ny bs nz oa dz ob"><span class="ga nj lr jl ob b dm oc od l oe of" id="24e2"># Assuming the data is stored in the numpy array F<br/><strong class="ob jm">F = (F - F.mean(axis=0)) / F.std(axis=0)</strong></span></pre><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="ffb9">This will provide the following table:</p><pre class="lb lc ld le gx ny bs nz oa dz ob"><span class="ga nj lr jl ob b dm oc od l oe of" id="dde4">┌────────┬───────┬───────┬───────┬───────┬───────┐<br/>│ Sample │  f1   │  f2   │  f3   │  f4   │  f5   │<br/>├────────┼───────┼───────┼───────┼───────┼───────┤<br/>│      0 │ -0.51 │ -1.14 │     0 │  1.89 │ -0.63 │<br/>│      1 │ -0.98 │  0.01 │  0.43 │ -0.89 │ -0.02 │<br/>│      2 │ -1.01 │ -1.14 │ -1.84 │ -1.21 │ -1.62 │<br/>│      3 │ -1.41 │   1.4 │  0.67 │  1.79 │  1.22 │<br/>│      4 │  0.13 │  0.99 │  0.27 │ -0.46 │ -1.19 │<br/>│      5 │   0.7 │  -1.4 │  0.02 │  0.07 │   0.4 │<br/>│      6 │  1.77 │  0.32 │   0.8 │  0.17 │  1.55 │<br/>│      7 │ -0.19 │  0.28 │  1.64 │ -0.33 │ -0.46 │<br/>│      8 │  1.45 │  1.35 │ -0.45 │ -0.67 │ -0.33 │<br/>│      9 │  0.06 │ -0.67 │ -1.54 │ -0.37 │   1.1 │<br/>└────────┴───────┴───────┴───────┴───────┴───────┘<br/>Now we see that all features are similar to each other, compared to our original dataset. </span></pre><h1 class="lq lr jl bm ls lt lu lv lw lx ly lz ma kr mb ks mc ku md kv me kx mf ky mg mh ga" id="9061">Partitioning the dataset</h1><p class="pw-post-body-paragraph mi mj jl mk b ml mm km mn mo mp kp mq mr ms mt mu mv mw mx my mz na nb nc nd je ga" id="a579">Since we followed the steps above, we now have a good set of feature vectors. But wait! There is something we need to do before we can train our model with it. We should not use the entire dataset to train our model, because we need to split the dataset into two, or ideally three, subsets. These subsets are called<strong class="mk jm"> training data</strong>, <strong class="mk jm">validation data,</strong> and<strong class="mk jm"> test data</strong>.</p><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="d853">The <strong class="mk jm">training data</strong> is used to train the model.</p><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="0bd2">The<strong class="mk jm"> test data</strong> is used to evaluate the accuracy and performance of the trained model. It is important that the model <em class="ok">never</em> sees this data during training because then, we will be testing the model on the data it has already seen.</p><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="a7a8">The <strong class="mk jm">validation data </strong>is not needed for every model. However, it can help when the models are deep learning models. This dataset is used similarly to how the test data is used during the training process. It tells us how well the training process is going and provides information about when to stop training and whether the model is suitable for the data.</p><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="1dd9">In summary, <strong class="mk jm">training</strong> and <strong class="mk jm">validation</strong> <strong class="mk jm">data</strong> are used to build the model. The <strong class="mk jm">test data </strong>is held back to evaluate the model.</p><h2 class="nj lr jl bm ls nk nl nm lw nn no np ma mr nq nr mc mv ns nt me mz nu nv mg nw ga" id="7981">How much data should be put in each set?</h2><p class="pw-post-body-paragraph mi mj jl mk b ml mm km mn mo mp kp mq mr ms mt mu mv mw mx my mz na nb nc nd je ga" id="9c6a">It is considered standard practice to split 90% of the set into training and 5% each for validation and test data according to some literature, while others suggest 70% for training and 15% each for validation and test for smaller datasets or 80% for training and 10% for validation and test.</p><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="ce07">It is important to preserve the prior probabilities in the dataset. If a class exists in real-world cases with a chance of 20%, this should be reflected in our dataset as well. This should also extend to the subsets that we create. How can we do this? There are two ways:</p><ul class=""><li class="pf pg jl mk b ml ne mo nf mr ph mv pi mz pj nd pk pl pm pn ga" id="60de">Partitioning by class</li><li class="pf pg jl mk b ml po mo pp mr pq mv pr mz ps nd pk pl pm pn ga" id="018d">Random sampling</li></ul><h2 class="nj lr jl bm ls nk nl nm lw nn no np ma mr nq nr mc mv ns nt me mz nu nv mg nw ga" id="68a4">Partitioning by class</h2><p class="pw-post-body-paragraph mi mj jl mk b ml mm km mn mo mp kp mq mr ms mt mu mv mw mx my mz na nb nc nd je ga" id="e00e">This method can be used when you are working with small datasets. First, we determine how many samples are present for each class. Next, for each class, we set aside the chosen percentages and merge everything together.</p><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="5b2f">For example, assume we have 500 samples, with 400 belonging to class 0 and 100 from class 1. We want to do a 90/5/5 split (90% training data, 5% test, and 5% validation). This means we randomly select 360 samples from class 0 and 90 samples from class 1 to create the training set. From the remaining unused 40 samples of class 0, 20 randomly selected samples will go to validation data and 20 to test data. From the remaining unused 10 samples of class 1, 5 each will go into validation and test datasets.</p><h2 class="nj lr jl bm ls nk nl nm lw nn no np ma mr nq nr mc mv ns nt me mz nu nv mg nw ga" id="a207">Random sampling</h2><p class="pw-post-body-paragraph mi mj jl mk b ml mm km mn mo mp kp mq mr ms mt mu mv mw mx my mz na nb nc nd je ga" id="1209">If we have sufficient data, we do not need to be very precise and follow the above method. Instead, we can randomize the entire dataset and split our data according to the necessary percentages.</p><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="5345">But what if we have a small dataset? We can use methods such as <strong class="mk jm">k-fold cross-validation</strong> to ensure that issues with this method, such as imbalances in train and test data can be mitigated. Have a look at my post below if you would like to learn more about this method.</p><div class="oo op gt gv oq or"><a href="https://medium.com/@praveennellihela/what-is-k-fold-cross-validation-5a7bb241d82f" rel="noopener follow" target="_blank"><div class="os o fr"><div class="ot o da dx en ou"><h2 class="bm jm dm bo fs ov fu fv ow fx fz jk ga">What is K-fold Cross Validation?</h2><div class="ox l"><h3 class="bm b dm bo fs ov fu fv ow fx fz cn">Let’s say that you have trained a machine learning model. Now, you need to find out how well this model performs. Is it…</h3></div><div class="oy l"><p class="bm b hi bo fs ov fu fv ow fx fz cn">medium.com</p></div></div><div class="oz l"><div class="pu l pb pc pd oz pe lk or"></div></div></div></a></div><h1 class="lq lr jl bm ls lt lu lv lw lx ly lz ma kr mb ks mc ku md kv me kx mf ky mg mh ga" id="05ba">Having a final look at our data</h1><p class="pw-post-body-paragraph mi mj jl mk b ml mm km mn mo mp kp mq mr ms mt mu mv mw mx my mz na nb nc nd je ga" id="5fed">Now, we have performed a lot of operations to ensure that our data set is sufficiently good to train a model. We made sure that it has good features, that there are no missing features, and that the features are normalized. We also partitioned the dataset into subsets in a proper manner.</p><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="31ae">The final important step is to have one last look at the data to make sure everything makes sense. This allows us to identify any <strong class="mk jm">mislabeled data </strong>and <strong class="mk jm">missing or outlier data</strong>. These errors can be identified by loading the data into a spreadsheet or for larger sets, using Python scripts to summarize the data. We should look at values such as the mean, median, and standard deviation as well as maximum and minimum values to see if there are any unusual data. We can also generate <a class="au lp" href="https://en.wikipedia.org/wiki/Box_plot" rel="noopener ugc nofollow" target="_blank">boxplots</a> to identify outliers.</p><figure class="lb lc ld le gx lf gl gm paragraph-image"><div class="lg lh do li ce lj" role="button" tabindex="0"><div class="gl gm pv"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/0*rCAQoPDr4nmKaFDQ.png 640w, https://miro.medium.com/max/720/0*rCAQoPDr4nmKaFDQ.png 720w, https://miro.medium.com/max/750/0*rCAQoPDr4nmKaFDQ.png 750w, https://miro.medium.com/max/786/0*rCAQoPDr4nmKaFDQ.png 786w, https://miro.medium.com/max/828/0*rCAQoPDr4nmKaFDQ.png 828w, https://miro.medium.com/max/1100/0*rCAQoPDr4nmKaFDQ.png 1100w, https://miro.medium.com/max/1400/0*rCAQoPDr4nmKaFDQ.png 1400w"/><img alt="" class="ce lk ll c" height="700" loading="lazy" role="presentation" width="700"/></picture></div></div><figcaption class="lm bl gn gl gm ln lo bm b bn bo cn">An example boxplot for <a class="au lp" href="https://en.wikipedia.org/wiki/Michelson%E2%80%93Morley_experiment#Michelson_experiment_(1881)" rel="noopener ugc nofollow" target="_blank">Michelson experiment</a> By User:Schutz — Own work, <a class="au lp" href="https://commons.wikimedia.org/w/index.php?curid=1501411" rel="noopener ugc nofollow" target="_blank">Public Domain</a>,</figcaption></figure><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="28b3">After performing all the above steps, we can be confident that our dataset is a proper dataset that can help a model train well and in turn, produce accurate and reliable predictions on real-world data. It might seem like a lot of effort, but always remember, if you feed your machine learning models garbage, they will only output garbage. You should always ensure that your dataset is sufficiently good enough.</p><h1 class="lq lr jl bm ls lt lu lv lw lx ly lz ma kr mb ks mc ku md kv me kx mf ky mg mh ga" id="ed86">Summary</h1><p class="pw-post-body-paragraph mi mj jl mk b ml mm km mn mo mp kp mq mr ms mt mu mv mw mx my mz na nb nc nd je ga" id="cf4c">In this post, we looked at why a good dataset is important. We then looked at what makes a dataset by looking at the components such as classes and features. We went over data preparation and how to handle missing features and scale features. Next, we discussed why and how we should partition the dataset<strong class="mk jm">. </strong>Finally, we looked at why we should have a<strong class="mk jm"> </strong>final look at our data and how to find problems that could still exist, such as outliers and mislabeled data.</p></div><div class="o dx pw px in py" role="separator"><span class="pz fl ci qa qb qc"></span><span class="pz fl ci qa qb qc"></span><span class="pz fl ci qa qb"></span></div><div class="je jf jg jh ji"><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="7630">If you found this post useful, consider <a class="au lp" href="https://medium.com/@praveennellihela/subscribe" rel="noopener">subscribing</a> to me and <a class="au lp" href="https://medium.com/@praveennellihela/membership" rel="noopener">joining medium</a>. Your membership supports me and other writers you read directly.</p><p class="pw-post-body-paragraph mi mj jl mk b ml ne km mn mo nf kp mq mr ng mt mu mv nh mx my mz ni nb nc nd je ga" id="484a">Thank you for reading! See you in a future post.</p></div></div></section></div></div></article>