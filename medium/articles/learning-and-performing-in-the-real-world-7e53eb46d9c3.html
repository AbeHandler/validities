<article><div class="l"><div class="gg gh gi gj gk gl gm ce gn ch l"></div><div class="l"><header class="pw-post-byline-header go gp gq gr gs gt gu gv gw gx l"><div class="o gy u"><div class="o"><div class="fj l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://culurciello.medium.com/?source=post_page-----7e53eb46d9c3--------------------------------" rel="noopener follow"><div class="l do"><img alt="Eugenio Culurciello" class="l ch fl gz ha fp" height="48" loading="lazy" src="https://miro.medium.com/fit/c/96/96/1*q31u_6hQx5zpz_eIeAtnHg.png" width="48"/><div class="fk fl l gz ha fo aq"></div></div></a></div><div class="l"><div class="pw-author bm b dm dn ga"><div class="hb o hc"><div><div aria-hidden="false" class="ci"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://culurciello.medium.com/?source=post_page-----7e53eb46d9c3--------------------------------" rel="noopener follow">Eugenio Culurciello</a></div></div><div class="hd he hf hg hh d"><span><a class="bm b hi bo hj hk hl hm hn ho hp bd bz hq hr hs cd cf cg ch ci cj" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe53b1a2a902f&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-and-performing-in-the-real-world-7e53eb46d9c3&amp;user=Eugenio+Culurciello&amp;userId=e53b1a2a902f&amp;source=post_page-e53b1a2a902f----7e53eb46d9c3---------------------follow_byline-----------" rel="noopener follow">Follow</a></span></div></div></div><div class="o ao ht"><p class="pw-published-date bm b bn bo cn"><span>Dec 8, 2017</span></p><div aria-hidden="true" class="hu ci"><span aria-hidden="true" class="l"><span class="bm b bn bo cn">Â·</span></span></div><div class="pw-reading-time bm b bn bo cn">6 min read</div></div></div></div><div class="o ao"><div class="h k hv hw hx"><div class="hy l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on twitter" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci hz dw ia"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M20 5.34c-.67.41-1.4.7-2.18.87a3.45 3.45 0 0 0-5.02-.1 3.49 3.49 0 0 0-1.02 2.47c0 .28.03.54.07.8a9.91 9.91 0 0 1-7.17-3.66 3.9 3.9 0 0 0-.5 1.74 3.6 3.6 0 0 0 1.56 2.92 3.36 3.36 0 0 1-1.55-.44V10c0 1.67 1.2 3.08 2.8 3.42-.3.06-.6.1-.94.12l-.62-.06a3.5 3.5 0 0 0 3.24 2.43 7.34 7.34 0 0 1-4.36 1.49l-.81-.05a9.96 9.96 0 0 0 5.36 1.56c6.4 0 9.91-5.32 9.9-9.9v-.5c.69-.49 1.28-1.1 1.74-1.81-.63.3-1.3.48-2 .56A3.33 3.33 0 0 0 20 5.33" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="hy l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on facebook" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci hz dw ia"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M19.75 12.04c0-4.3-3.47-7.79-7.75-7.79a7.77 7.77 0 0 0-5.9 12.84 7.77 7.77 0 0 0 4.69 2.63v-5.49h-1.9v-2.2h1.9v-1.62c0-1.88 1.14-2.9 2.8-2.9.8 0 1.49.06 1.69.08v1.97h-1.15c-.91 0-1.1.43-1.1 1.07v1.4h2.17l-.28 2.2h-1.88v5.52a7.77 7.77 0 0 0 6.7-7.71" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="hy l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on linkedin" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci hz dw ia"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M19.75 5.39v13.22a1.14 1.14 0 0 1-1.14 1.14H5.39a1.14 1.14 0 0 1-1.14-1.14V5.39a1.14 1.14 0 0 1 1.14-1.14h13.22a1.14 1.14 0 0 1 1.14 1.14zM8.81 10.18H6.53v7.3H8.8v-7.3zM9 7.67a1.31 1.31 0 0 0-1.3-1.32h-.04a1.32 1.32 0 0 0 0 2.64A1.31 1.31 0 0 0 9 7.71v-.04zm8.46 5.37c0-2.2-1.4-3.05-2.78-3.05a2.6 2.6 0 0 0-2.3 1.18h-.07v-1h-2.14v7.3h2.28V13.6a1.51 1.51 0 0 1 1.36-1.63h.09c.72 0 1.26.45 1.26 1.6v3.91h2.28l.02-4.43z" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="l fr"><div><div aria-hidden="false" class="ci"><button class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci hz dw ia"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path clip-rule="evenodd" d="M3.57 14.67c0-.57.13-1.11.38-1.6l.02-.02v-.02l.02-.02c0-.02 0-.02.02-.02.12-.26.3-.52.57-.8L7.78 9v-.02l.01-.02c.44-.41.91-.7 1.44-.85a4.87 4.87 0 0 0-1.19 2.36A5.04 5.04 0 0 0 8 11.6L6.04 13.6c-.19.19-.32.4-.38.65a2 2 0 0 0 0 .9c.08.2.2.4.38.57l1.29 1.31c.27.28.62.42 1.03.42.42 0 .78-.14 1.06-.42l1.23-1.25.79-.78 1.15-1.16c.08-.09.19-.22.28-.4.1-.2.15-.42.15-.67 0-.16-.02-.3-.06-.45l-.02-.02v-.02l-.07-.14s0-.03-.04-.06l-.06-.13-.02-.02c0-.02 0-.03-.02-.05a.6.6 0 0 0-.14-.16l-.48-.5c0-.04.02-.1.04-.15l.06-.12 1.17-1.14.09-.09.56.57c.02.04.08.1.16.18l.05.04.03.06.04.05.03.04.04.06.1.14.02.02c0 .02.01.03.03.04l.1.2v.02c.1.16.2.38.3.68a1 1 0 0 1 .04.25 3.2 3.2 0 0 1 .02 1.33 3.49 3.49 0 0 1-.95 1.87l-.66.67-.97.97-1.56 1.57a3.4 3.4 0 0 1-2.47 1.02c-.97 0-1.8-.34-2.49-1.03l-1.3-1.3a3.55 3.55 0 0 1-1-2.51v-.01h-.02v.02zm5.39-3.43c0-.19.02-.4.07-.63.13-.74.44-1.37.95-1.87l.66-.67.97-.98 1.56-1.56c.68-.69 1.5-1.03 2.47-1.03.97 0 1.8.34 2.48 1.02l1.3 1.32a3.48 3.48 0 0 1 1 2.48c0 .58-.11 1.11-.37 1.6l-.02.02v.02l-.02.04c-.14.27-.35.54-.6.8L16.23 15l-.01.02-.01.02c-.44.42-.92.7-1.43.83a4.55 4.55 0 0 0 1.23-3.52L18 10.38c.18-.21.3-.42.35-.65a2.03 2.03 0 0 0-.01-.9 1.96 1.96 0 0 0-.36-.58l-1.3-1.3a1.49 1.49 0 0 0-1.06-.42c-.42 0-.77.14-1.06.4l-1.2 1.27-.8.8-1.16 1.15c-.08.08-.18.21-.29.4a1.66 1.66 0 0 0-.08 1.12l.02.03v.02l.06.14s.01.03.05.06l.06.13.02.02.01.02.01.02c.05.08.1.13.14.16l.47.5c0 .04-.02.09-.04.15l-.06.12-1.15 1.15-.1.08-.56-.56a2.3 2.3 0 0 0-.18-.19c-.02-.01-.02-.03-.02-.04l-.02-.02a.37.37 0 0 1-.1-.12c-.03-.03-.05-.04-.05-.06l-.1-.15-.02-.02-.02-.04-.08-.17v-.02a5.1 5.1 0 0 1-.28-.69 1.03 1.03 0 0 1-.04-.26c-.06-.23-.1-.46-.1-.7v.01z" fill="#A8A8A8" fill-rule="evenodd"></path></svg></span></button></div></div></div><div class="ib o ao"><span><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7e53eb46d9c3&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-and-performing-in-the-real-world-7e53eb46d9c3&amp;source=--------------------------bookmark_header-----------" rel="noopener follow"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="au de aw ax ay az ba hz bc id ie if ig"><svg aria-label="Add to list bookmark button" class="ic" fill="none" height="25" viewbox="0 0 25 25" width="25"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg></button></a></span></div></div><div class="ck ih"><div><div aria-hidden="false" class="ci"></div></div></div></div></div><div class="ii ij ik j i d"><div class="fj l"><span><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7e53eb46d9c3&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Flearning-and-performing-in-the-real-world-7e53eb46d9c3&amp;source=--------------------------bookmark_header-----------" rel="noopener follow"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="au de aw il ay az ba im bc id cd o ao in io ig"><svg aria-label="Add to list bookmark button" class="ic" fill="none" height="25" viewbox="0 0 25 25" width="25"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg><p class="bm b bn bo cn">Save</p></button></a></span></div><div class="ip l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on twitter" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci hz dw ia"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M20 5.34c-.67.41-1.4.7-2.18.87a3.45 3.45 0 0 0-5.02-.1 3.49 3.49 0 0 0-1.02 2.47c0 .28.03.54.07.8a9.91 9.91 0 0 1-7.17-3.66 3.9 3.9 0 0 0-.5 1.74 3.6 3.6 0 0 0 1.56 2.92 3.36 3.36 0 0 1-1.55-.44V10c0 1.67 1.2 3.08 2.8 3.42-.3.06-.6.1-.94.12l-.62-.06a3.5 3.5 0 0 0 3.24 2.43 7.34 7.34 0 0 1-4.36 1.49l-.81-.05a9.96 9.96 0 0 0 5.36 1.56c6.4 0 9.91-5.32 9.9-9.9v-.5c.69-.49 1.28-1.1 1.74-1.81-.63.3-1.3.48-2 .56A3.33 3.33 0 0 0 20 5.33" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="ip l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on facebook" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci hz dw ia"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M19.75 12.04c0-4.3-3.47-7.79-7.75-7.79a7.77 7.77 0 0 0-5.9 12.84 7.77 7.77 0 0 0 4.69 2.63v-5.49h-1.9v-2.2h1.9v-1.62c0-1.88 1.14-2.9 2.8-2.9.8 0 1.49.06 1.69.08v1.97h-1.15c-.91 0-1.1.43-1.1 1.07v1.4h2.17l-.28 2.2h-1.88v5.52a7.77 7.77 0 0 0 6.7-7.71" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="ip l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on linkedin" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci hz dw ia"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M19.75 5.39v13.22a1.14 1.14 0 0 1-1.14 1.14H5.39a1.14 1.14 0 0 1-1.14-1.14V5.39a1.14 1.14 0 0 1 1.14-1.14h13.22a1.14 1.14 0 0 1 1.14 1.14zM8.81 10.18H6.53v7.3H8.8v-7.3zM9 7.67a1.31 1.31 0 0 0-1.3-1.32h-.04a1.32 1.32 0 0 0 0 2.64A1.31 1.31 0 0 0 9 7.71v-.04zm8.46 5.37c0-2.2-1.4-3.05-2.78-3.05a2.6 2.6 0 0 0-2.3 1.18h-.07v-1h-2.14v7.3h2.28V13.6a1.51 1.51 0 0 1 1.36-1.63h.09c.72 0 1.26.45 1.26 1.6v3.91h2.28l.02-4.43z" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="l fr"><div><div aria-hidden="false" class="ci"><button class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci hz dw ia"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path clip-rule="evenodd" d="M3.57 14.67c0-.57.13-1.11.38-1.6l.02-.02v-.02l.02-.02c0-.02 0-.02.02-.02.12-.26.3-.52.57-.8L7.78 9v-.02l.01-.02c.44-.41.91-.7 1.44-.85a4.87 4.87 0 0 0-1.19 2.36A5.04 5.04 0 0 0 8 11.6L6.04 13.6c-.19.19-.32.4-.38.65a2 2 0 0 0 0 .9c.08.2.2.4.38.57l1.29 1.31c.27.28.62.42 1.03.42.42 0 .78-.14 1.06-.42l1.23-1.25.79-.78 1.15-1.16c.08-.09.19-.22.28-.4.1-.2.15-.42.15-.67 0-.16-.02-.3-.06-.45l-.02-.02v-.02l-.07-.14s0-.03-.04-.06l-.06-.13-.02-.02c0-.02 0-.03-.02-.05a.6.6 0 0 0-.14-.16l-.48-.5c0-.04.02-.1.04-.15l.06-.12 1.17-1.14.09-.09.56.57c.02.04.08.1.16.18l.05.04.03.06.04.05.03.04.04.06.1.14.02.02c0 .02.01.03.03.04l.1.2v.02c.1.16.2.38.3.68a1 1 0 0 1 .04.25 3.2 3.2 0 0 1 .02 1.33 3.49 3.49 0 0 1-.95 1.87l-.66.67-.97.97-1.56 1.57a3.4 3.4 0 0 1-2.47 1.02c-.97 0-1.8-.34-2.49-1.03l-1.3-1.3a3.55 3.55 0 0 1-1-2.51v-.01h-.02v.02zm5.39-3.43c0-.19.02-.4.07-.63.13-.74.44-1.37.95-1.87l.66-.67.97-.98 1.56-1.56c.68-.69 1.5-1.03 2.47-1.03.97 0 1.8.34 2.48 1.02l1.3 1.32a3.48 3.48 0 0 1 1 2.48c0 .58-.11 1.11-.37 1.6l-.02.02v.02l-.02.04c-.14.27-.35.54-.6.8L16.23 15l-.01.02-.01.02c-.44.42-.92.7-1.43.83a4.55 4.55 0 0 0 1.23-3.52L18 10.38c.18-.21.3-.42.35-.65a2.03 2.03 0 0 0-.01-.9 1.96 1.96 0 0 0-.36-.58l-1.3-1.3a1.49 1.49 0 0 0-1.06-.42c-.42 0-.77.14-1.06.4l-1.2 1.27-.8.8-1.16 1.15c-.08.08-.18.21-.29.4a1.66 1.66 0 0 0-.08 1.12l.02.03v.02l.06.14s.01.03.05.06l.06.13.02.02.01.02.01.02c.05.08.1.13.14.16l.47.5c0 .04-.02.09-.04.15l-.06.12-1.15 1.15-.1.08-.56-.56a2.3 2.3 0 0 0-.18-.19c-.02-.01-.02-.03-.02-.04l-.02-.02a.37.37 0 0 1-.1-.12c-.03-.03-.05-.04-.05-.06l-.1-.15-.02-.02-.02-.04-.08-.17v-.02a5.1 5.1 0 0 1-.28-.69 1.03 1.03 0 0 1-.04-.26c-.06-.23-.1-.46-.1-.7v.01z" fill="#A8A8A8" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></header><span class="l"></span><section><div><div class="fo as iv iw ix iy"></div><div class="iz ja jb jc jd"><div class=""><h1 class="pw-post-title je jf jg bm jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc ga" id="a5f3">Learning and performing in the real world</h1></div><div class=""><h2 class="pw-subtitle-paragraph kd jf jg bm b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku cn" id="452e">AKA Reinforcement Learning</h2></div><figure class="kw kx ky kz gx la gl gm paragraph-image"><div class="lb lc do ld ce le" role="button" tabindex="0"><div class="gl gm kv"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/1*cYszIAiWlfzf9ppkh2YegQ.jpeg 640w, https://miro.medium.com/max/720/1*cYszIAiWlfzf9ppkh2YegQ.jpeg 720w, https://miro.medium.com/max/750/1*cYszIAiWlfzf9ppkh2YegQ.jpeg 750w, https://miro.medium.com/max/786/1*cYszIAiWlfzf9ppkh2YegQ.jpeg 786w, https://miro.medium.com/max/828/1*cYszIAiWlfzf9ppkh2YegQ.jpeg 828w, https://miro.medium.com/max/1100/1*cYszIAiWlfzf9ppkh2YegQ.jpeg 1100w, https://miro.medium.com/max/1400/1*cYszIAiWlfzf9ppkh2YegQ.jpeg 1400w"/><img alt="" class="ce lf lg c" height="468" loading="eager" role="presentation" width="700"/></picture></div></div></figure><p class="pw-post-body-paragraph lh li jg lj b lk ll kh lm ln lo kk lp lq lr ls lt lu lv lw lx ly lz ma mb mc iz ga" id="0816">In order to interact with a complex environment, living entities need to produce a âcorrectâ sequence of <em class="md">actions</em> to achieve delayed future <em class="md">rewards</em>. These living entities, or <em class="md">actors</em>, can sense the environment and produce actions in response of a sequence of states of both the environment and agent previous history. See figure:</p><figure class="kw kx ky kz gx la gl gm paragraph-image"><div class="lb lc do ld ce le" role="button" tabindex="0"><div class="gl gm me"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/1*aa9Akd8Vf9oA-ALpURejUA.png 640w, https://miro.medium.com/max/720/1*aa9Akd8Vf9oA-ALpURejUA.png 720w, https://miro.medium.com/max/750/1*aa9Akd8Vf9oA-ALpURejUA.png 750w, https://miro.medium.com/max/786/1*aa9Akd8Vf9oA-ALpURejUA.png 786w, https://miro.medium.com/max/828/1*aa9Akd8Vf9oA-ALpURejUA.png 828w, https://miro.medium.com/max/1100/1*aa9Akd8Vf9oA-ALpURejUA.png 1100w, https://miro.medium.com/max/1400/1*aa9Akd8Vf9oA-ALpURejUA.png 1400w"/><img alt="" class="ce lf lg c" height="413" loading="lazy" role="presentation" width="700"/></picture></div></div><figcaption class="mf bl gn gl gm mg mh bm b bn bo cn">The reinforcement learning loop: an agent performs actions in an environment, and gets rewards. Learning has to occur through sparse rewards</figcaption></figure><p class="pw-post-body-paragraph lh li jg lj b lk ll kh lm ln lo kk lp lq lr ls lt lu lv lw lx ly lz ma mb mc iz ga" id="1fa9">The scenario where we want to learn to perform some tasks, is:</p><ul class=""><li class="mi mj jg lj b lk ll ln lo lq mk lu ml ly mm mc mn mo mp mq ga" id="c6f3">actor in an environment</li><li class="mi mj jg lj b lk mr ln ms lq mt lu mu ly mv mc mn mo mp mq ga" id="2762">actors has goals [s]he/it want to achieve</li><li class="mi mj jg lj b lk mr ln ms lq mt lu mu ly mv mc mn mo mp mq ga" id="70c3">loop: actor senses itself and environment</li><li class="mi mj jg lj b lk mr ln ms lq mt lu mu ly mv mc mn mo mp mq ga" id="22d7">loop: actor actions affect itself and environment</li><li class="mi mj jg lj b lk mr ln ms lq mt lu mu ly mv mc mn mo mp mq ga" id="0a22">delayed rewards or punishment â did we achieve goals?</li><li class="mi mj jg lj b lk mr ln ms lq mt lu mu ly mv mc mn mo mp mq ga" id="ae0d">learn: how to maximize reward and minimize punishments</li></ul><p class="pw-post-body-paragraph lh li jg lj b lk ll kh lm ln lo kk lp lq lr ls lt lu lv lw lx ly lz ma mb mc iz ga" id="5809">The actions propel the actor closer to multiple goals, where achieving even one goal may take an undefined number of steps. The agent need to learn the correct sequence of model <em class="md">states</em> and actions that lead to a delayed reward by interacting with the world and performing online learning.</p><p class="pw-post-body-paragraph lh li jg lj b lk ll kh lm ln lo kk lp lq lr ls lt lu lv lw lx ly lz ma mb mc iz ga" id="1524">Recent work in artificial intelligence (AI) uses reinforcement learning algorithms to model the behavior of agents using sparse and sporadic rewards. Recent advances in this field allowed to train agents to perform in complex artificial environment and even surpass human abilities in the same contexts [Mnih2013, -2016]. Some exciting recent results in the field are learning language while performing RL tasks â see <a class="au mw" href="https://arxiv.org/abs/1706.07230" rel="noopener ugc nofollow" target="_blank">here</a>.</p><h1 class="mx my jg bm mz na nb nc nd ne nf ng nh km ni kn nj kp nk kq nl ks nm kt nn no ga" id="53a7">Formalism</h1><p class="pw-post-body-paragraph lh li jg lj b lk np kh lm ln nq kk lp lq nr ls lt lu ns lw lx ly nt ma mb mc iz ga" id="2e8e">To learn to perform in the real world, given a sequence of states <em class="md">s</em> we want to obtain a sequence of actions a that maximize a reward, or probability of winning.</p><p class="pw-post-body-paragraph lh li jg lj b lk ll kh lm ln lo kk lp lq lr ls lt lu lv lw lx ly lz ma mb mc iz ga" id="ae24">Fundamental needs (suppose we do not know rules of games, just a list of possible actions we can perform):</p><ul class=""><li class="mi mj jg lj b lk ll ln lo lq mk lu ml ly mm mc mn mo mp mq ga" id="7733">need to be able to <strong class="lj jh">evaluate</strong> a state â are we winning, losing, how close to a reward?</li><li class="mi mj jg lj b lk mr ln ms lq mt lu mu ly mv mc mn mo mp mq ga" id="72c6">need to be able to <strong class="lj jh">predict</strong> outcome from a state, action <em class="md">{s,a}</em> pair â given we decided to take action <em class="md">a</em>, what reward or outcome do we expect? Play mental game</li><li class="mi mj jg lj b lk mr ln ms lq mt lu mu ly mv mc mn mo mp mq ga" id="c184">need to remember a list of <em class="md">{s, a}</em> pairs that can extend over very long periods of time</li></ul><p class="pw-post-body-paragraph lh li jg lj b lk ll kh lm ln lo kk lp lq lr ls lt lu lv lw lx ly lz ma mb mc iz ga" id="2336">There are major differences in board games like chess, go etc, where the entire game is fully observable, and real-world scenarios, like self-driving robot-cars, where only part of the environment is visible.</p><h1 class="mx my jg bm mz na nb nc nd ne nf ng nh km ni kn nj kp nk kq nl ks nm kt nn no ga" id="cfdd">Fully-observable</h1><p class="pw-post-body-paragraph lh li jg lj b lk np kh lm ln nq kk lp lq nr ls lt lu ns lw lx ly nt ma mb mc iz ga" id="8f37">The <a class="au mw" href="https://arxiv.org/abs/1712.01815" rel="noopener ugc nofollow" target="_blank">AlphaZero paper</a> show an elegant approach to fully-observable games.</p><p class="pw-post-body-paragraph lh li jg lj b lk ll kh lm ln lo kk lp lq lr ls lt lu lv lw lx ly lz ma mb mc iz ga" id="04a7"><strong class="lj jh">Ingredients of </strong><a class="au mw" href="https://arxiv.org/abs/1712.01815" rel="noopener ugc nofollow" target="_blank"><strong class="lj jh">AlphaZero</strong></a><strong class="lj jh">:</strong></p><p class="pw-post-body-paragraph lh li jg lj b lk ll kh lm ln lo kk lp lq lr ls lt lu lv lw lx ly lz ma mb mc iz ga" id="9fc7"><strong class="lj jh">One (AZ1):</strong> a function <em class="md">f</em> (neural network) based on parameters <em class="md">theta</em> outputs probability of action <em class="md">a</em> and value of action <em class="md">v </em>(a prediction of the value of making a move).</p><blockquote class="nu"><p class="nv nw jg bm nx ny nz oa ob oc od mc cn" id="7f7e"><em class="oe">a, v = f(theta, s)</em></p></blockquote><p class="pw-post-body-paragraph lh li jg lj b lk of kh lm ln og kk lp lq oh ls lt lu oi lw lx ly oj ma mb mc iz ga" id="4d5c"><strong class="lj jh">Two (AZ2):</strong> a predictive model that can evaluate the outcome of different moves and self-play (play scenarios in your head). A Monte Carlo Tree Search in <em class="md">AlphaZero</em>. These games outcomes, move probabilities <em class="md">a</em> and value <em class="md">v</em> are then used to update the function <em class="md">f</em>.</p><p class="pw-post-body-paragraph lh li jg lj b lk ll kh lm ln lo kk lp lq lr ls lt lu lv lw lx ly lz ma mb mc iz ga" id="8451">These two ingredients make for a very simple and elegant way to learn to play these table games, and also learn without human support in a relatively short time.</p><h1 class="mx my jg bm mz na nb nc nd ne nf ng nh km ni kn nj kp nk kq nl ks nm kt nn no ga" id="2c24">Partially observable</h1><p class="pw-post-body-paragraph lh li jg lj b lk np kh lm ln nq kk lp lq nr ls lt lu ns lw lx ly nt ma mb mc iz ga" id="eae5">But what about non-fully observable games? Like a 1st person shooter (Doom)? Or an autonomous car learning to drive?</p><p class="pw-post-body-paragraph lh li jg lj b lk ll kh lm ln lo kk lp lq lr ls lt lu lv lw lx ly lz ma mb mc iz ga" id="2cd1">A- Only a portion of the environment is visible, and thus predicting the outcome of actions and next states is a much harder problem.</p><p class="pw-post-body-paragraph lh li jg lj b lk ll kh lm ln lo kk lp lq lr ls lt lu lv lw lx ly lz ma mb mc iz ga" id="e27e">B- There are too many possible options to search. The search tree is too vast. We can use AZ2, but we need to be smart and fast about which options we evaluate, as there are too many and we only have limited time to decide our next action!</p><p class="pw-post-body-paragraph lh li jg lj b lk ll kh lm ln lo kk lp lq lr ls lt lu lv lw lx ly lz ma mb mc iz ga" id="83b1">C- There may not be another player to compete with. The agent here has to compete with himself, or with his own predictions, getting intermediate rewards from its ability to foresee events or not.</p><blockquote class="nu"><p class="nv nw jg bm nx ny nz oa ob oc od mc cn" id="d958">How do we do this?</p></blockquote><h1 class="mx my jg bm mz na nb nc nd ne nf ng nh km ok kn nj kp ol kq nl ks om kt nn no ga" id="d26a">Proposal</h1><p class="pw-post-body-paragraph lh li jg lj b lk np kh lm ln nq kk lp lq nr ls lt lu ns lw lx ly nt ma mb mc iz ga" id="9f40">Unfortunately, and despite efforts in the recent years, RL algorithms only work in small artificial scenarios, and do not extend to more complex or real-life environment. The reason is that currently the bulk of the parameters of these systems are trained with time-consuming reinforcement- learning based on excessively sparse reward. In real-world scenarios, the model becomes very large (with many parameters) and almost impossible to train in short time-frames.</p><p class="pw-post-body-paragraph lh li jg lj b lk ll kh lm ln lo kk lp lq lr ls lt lu lv lw lx ly lz ma mb mc iz ga" id="aa85">What we need is to be able to train a <em class="md">model brain</em> to be familiar with the environment, and be able to predict actions and events combinations. With this pre-trained <em class="md">model brain</em>, reinforcement learning classifier for achieving specific goals, and with a small set of trainable parameters, are much easier to train using sparse rewards. We need a pre-trained neural network that can process at least visual inputs. It needs to be trained on video sequences, and needs to be able to provide prediction of future representation of the inputs.</p><p class="pw-post-body-paragraph lh li jg lj b lk ll kh lm ln lo kk lp lq lr ls lt lu lv lw lx ly lz ma mb mc iz ga" id="d0e2">In order to surpass these limitations, we are studying a synthetic model brain with the following characteristics:</p><ul class=""><li class="mi mj jg lj b lk ll ln lo lq mk lu ml ly mm mc mn mo mp mq ga" id="5214">active in an environment, can sense and act</li><li class="mi mj jg lj b lk mr ln ms lq mt lu mu ly mv mc mn mo mp mq ga" id="4d1b">memory of sensory sequences, actions and rewards</li><li class="mi mj jg lj b lk mr ln ms lq mt lu mu ly mv mc mn mo mp mq ga" id="9366">attention: only focus on data that matters (<a class="au mw" href="https://medium.com/towards-data-science/memory-attention-sequences-37456d271992" rel="noopener">see this</a>)</li><li class="mi mj jg lj b lk mr ln ms lq mt lu mu ly mv mc mn mo mp mq ga" id="020b">predictions: predict actor and environment future states, so we can perform online learning â by predicting we can understand what we know or do not know and have a âsurpriseâ signal for when we need to learn (<a class="au mw" href="https://medium.com/towards-data-science/a-new-kind-of-deep-neural-networks-749bcde19108" rel="noopener">see this</a> and <a class="au mw" href="https://medium.com/towards-data-science/memory-attention-sequences-37456d271992" rel="noopener">this</a>). Predictions also do not require supervisory signals, as they can be tested for errors agains actual future events and their outcomes.</li></ul><blockquote class="nu"><p class="nv nw jg bm nx ny on oo op oq or mc cn" id="13e2">The key is to be able to predict outcomes â function f (AZ1) needs to have predictive capabilities, in other words it needs to have the ability to foresee the future.</p></blockquote><figure class="ot ou ov ow ox la gl gm paragraph-image"><div class="gl gm os"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 398px" srcset="https://miro.medium.com/max/640/1*RLAGZK62TODCUkkt-psbeQ.png 640w, https://miro.medium.com/max/720/1*RLAGZK62TODCUkkt-psbeQ.png 720w, https://miro.medium.com/max/750/1*RLAGZK62TODCUkkt-psbeQ.png 750w, https://miro.medium.com/max/786/1*RLAGZK62TODCUkkt-psbeQ.png 786w, https://miro.medium.com/max/828/1*RLAGZK62TODCUkkt-psbeQ.png 828w, https://miro.medium.com/max/1100/1*RLAGZK62TODCUkkt-psbeQ.png 1100w, https://miro.medium.com/max/796/1*RLAGZK62TODCUkkt-psbeQ.png 796w"/><img alt="" class="ce lf lg c" height="777" loading="lazy" role="presentation" width="398"/></picture></div><figcaption class="mf bl gn gl gm mg mh bm b bn bo cn">A proposal for a neural network that can understand the world and act in it. This network uses video and can predict future representation based on a combination of the state s and its own associative memory. A multi-head attention can recall memories and combine them with state s to predict the best action to take.</figcaption></figure><p class="pw-post-body-paragraph lh li jg lj b lk ll kh lm ln lo kk lp lq lr ls lt lu lv lw lx ly lz ma mb mc iz ga" id="1644">On the left, you can see a proposal for a neural network that can understand the world and act in it. This network uses video and can predict future representation based on a combination of the state s and its own associative memory. A multi-head attention can recall memories and combine them with state s to predict the best action to take.</p><p class="pw-post-body-paragraph lh li jg lj b lk ll kh lm ln lo kk lp lq lr ls lt lu lv lw lx ly lz ma mb mc iz ga" id="03c4">This model is able to perform well in multiple conditions:</p><ul class=""><li class="mi mj jg lj b lk ll ln lo lq mk lu ml ly mm mc mn mo mp mq ga" id="31c9">continuous learning: we need this model to be able to learn multiple task at once, and learning without forgetting, so older task are not forgotten</li><li class="mi mj jg lj b lk mr ln ms lq mt lu mu ly mv mc mn mo mp mq ga" id="c4a1">one-shot learning and transfer learning: we need this model to be able to learn from examples, both real and synthetic</li><li class="mi mj jg lj b lk mr ln ms lq mt lu mu ly mv mc mn mo mp mq ga" id="c5bd">virtual replay: the model is predictive, and can predict different outcomes even after the events have been witnessed. It can play and replay possible action in its mind and chose the best one. The associative memory is acting as a search tree.</li></ul><p class="pw-post-body-paragraph lh li jg lj b lk ll kh lm ln lo kk lp lq lr ls lt lu lv lw lx ly lz ma mb mc iz ga" id="5d73">How do we train this synthetic brain?</p><ul class=""><li class="mi mj jg lj b lk ll ln lo lq mk lu ml ly mm mc mn mo mp mq ga" id="9348">mostly unsupervised, or self-supervised</li><li class="mi mj jg lj b lk mr ln ms lq mt lu mu ly mv mc mn mo mp mq ga" id="22e8">little supervision here and there, but no guarantee of when</li></ul><blockquote class="nu"><p class="nv nw jg bm nx ny on oo op oq or mc cn" id="9498">But designing and training a predictive neural network are current challenges in artificial intelligence.</p></blockquote><p class="pw-post-body-paragraph lh li jg lj b lk of kh lm ln og kk lp lq oh ls lt lu oi lw lx ly oj ma mb mc iz ga" id="b5e9">We argued in the past for a <a class="au mw" href="/a-new-kind-of-deep-neural-networks-749bcde19108" rel="noopener" target="_blank">new kind of neural networks</a>, which have showed to be <a class="au mw" href="https://medium.com/@culurciello/predictive-neural-networks-for-reinforcement-learning-490738725839" rel="noopener">more effective in learning RL tasks</a>.</p><p class="pw-post-body-paragraph lh li jg lj b lk ll kh lm ln lo kk lp lq lr ls lt lu lv lw lx ly lz ma mb mc iz ga" id="717f">We also like the predictive capabilities of <a class="au mw" href="https://arxiv.org/abs/1710.09829" rel="noopener ugc nofollow" target="_blank">Capsules</a>, which does not require a ground-truth representation, but is able to predict next layer outputs based on the previous layer.</p><p class="pw-post-body-paragraph lh li jg lj b lk ll kh lm ln lo kk lp lq lr ls lt lu lv lw lx ly lz ma mb mc iz ga" id="2d59"><strong class="lj jh">Note 1:</strong> this is a <a class="au mw" href="http://A great summary of DL work and success or lack thereof https://www.alexirpan.com/2018/02/14/rl-hard.html" rel="noopener ugc nofollow" target="_blank">great post</a> on why RL does or does not work well and the issues associated with it. I agree a lot of RL these days is highly inefficient and there is no transfer-learning success story. This is why we should push approaches to pre-train networks, curriculum learning and breaking the task into many simpler tasks each with simple rewards. One solution to all problems is hard if your search space is so large!</p><h1 class="mx my jg bm mz na nb nc nd ne nf ng nh km ni kn nj kp nk kq nl ks nm kt nn no ga" id="820e">About the author</h1><p class="pw-post-body-paragraph lh li jg lj b lk np kh lm ln nq kk lp lq nr ls lt lu ns lw lx ly nt ma mb mc iz ga" id="7e75">I have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: <a class="au mw" href="https://medium.com/@culurciello/" rel="noopener">Medium</a>, <a class="au mw" href="https://e-lab.github.io/html/contact-eugenio-culurciello.html" rel="noopener ugc nofollow" target="_blank">webpage</a>, <a class="au mw" href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ" rel="noopener ugc nofollow" target="_blank">Scholar</a>, <a class="au mw" href="https://www.linkedin.com/in/eugenioculurciello/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>, and moreâ¦</p></div></div></section></div></div></article>