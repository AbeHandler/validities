<article class="meteredContent"><div class="l"><div class="gg gh gi gj gk gl gm ce gn ch l"></div><div class="l"><header class="pw-post-byline-header go gp gq gr gs gt gu gv gw gx l"><div class="o gy u"><div class="o"><div class="fj l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@matthew_stewart?source=post_page-----f2a5a262a766--------------------------------" rel="noopener follow"><div class="l do"><img alt="Matthew Stewart" class="l ch fl gz ha fp" height="48" loading="lazy" src="https://miro.medium.com/fit/c/96/96/2*AiKFMI6FEyDWnNMtiPAr1A.jpeg" width="48"/><div class="fk fl l gz ha fo aq"></div></div></a></div><div class="l"><div class="pw-author bm b dm dn ga"><div class="hb o hc"><div><div aria-hidden="false" class="ci"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@matthew_stewart?source=post_page-----f2a5a262a766--------------------------------" rel="noopener follow">Matthew Stewart</a></div></div><div class="hd he hf hg hh d"><span><a class="bm b hi bo hj hk hl hm hn ho hp bd bz hq hr hs cd cf cg ch ci cj" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb89dbc0712c4&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-dataset-shift-f2a5a262a766&amp;user=Matthew+Stewart&amp;userId=b89dbc0712c4&amp;source=post_page-b89dbc0712c4----f2a5a262a766---------------------follow_byline-----------" rel="noopener follow">Follow</a></span></div></div></div><div class="o ao ht"><p class="pw-published-date bm b bn bo cn"><span>Dec 11, 2019</span></p><div aria-hidden="true" class="hu ci"><span aria-hidden="true" class="l"><span class="bm b bn bo cn">·</span></span></div><div class="pw-reading-time bm b bn bo cn">14 min read</div><div aria-hidden="true" class="hu ci"><span aria-hidden="true" class="l"><span class="bm b bn bo cn">·</span></span></div><div class="hx l"><div aria-hidden="false" class="l"><button class="l dz hv bb"><div class="j i d"><div><div aria-hidden="false" class="ci"><svg fill="none" height="20" viewbox="0 0 20 20" width="20"><path d="M12.4 12.77l-1.81 4.99a.63.63 0 0 1-1.18 0l-1.8-4.99a.63.63 0 0 0-.38-.37l-4.99-1.81a.62.62 0 0 1 0-1.18l4.99-1.8a.63.63 0 0 0 .37-.38l1.81-4.99a.63.63 0 0 1 1.18 0l1.8 4.99a.63.63 0 0 0 .38.37l4.99 1.81a.63.63 0 0 1 0 1.18l-4.99 1.8a.63.63 0 0 0-.37.38z" fill="#FFC017"></path></svg></div></div></div><div class="h k hy hz ia"><svg class="hw" fill="none" height="20" viewbox="0 0 20 20" width="20"><path d="M12.4 12.77l-1.81 4.99a.63.63 0 0 1-1.18 0l-1.8-4.99a.63.63 0 0 0-.38-.37l-4.99-1.81a.62.62 0 0 1 0-1.18l4.99-1.8a.63.63 0 0 0 .37-.38l1.81-4.99a.63.63 0 0 1 1.18 0l1.8 4.99a.63.63 0 0 0 .38.37l4.99 1.81a.63.63 0 0 1 0 1.18l-4.99 1.8a.63.63 0 0 0-.37.38z" fill="#FFC017"></path></svg><p class="bm b bn bo cn">Member-only</p></div></button></div></div></div></div></div><div class="o ao"><div class="h k ib ic id"><div class="ie l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on twitter" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci if dw ig"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M20 5.34c-.67.41-1.4.7-2.18.87a3.45 3.45 0 0 0-5.02-.1 3.49 3.49 0 0 0-1.02 2.47c0 .28.03.54.07.8a9.91 9.91 0 0 1-7.17-3.66 3.9 3.9 0 0 0-.5 1.74 3.6 3.6 0 0 0 1.56 2.92 3.36 3.36 0 0 1-1.55-.44V10c0 1.67 1.2 3.08 2.8 3.42-.3.06-.6.1-.94.12l-.62-.06a3.5 3.5 0 0 0 3.24 2.43 7.34 7.34 0 0 1-4.36 1.49l-.81-.05a9.96 9.96 0 0 0 5.36 1.56c6.4 0 9.91-5.32 9.9-9.9v-.5c.69-.49 1.28-1.1 1.74-1.81-.63.3-1.3.48-2 .56A3.33 3.33 0 0 0 20 5.33" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="ie l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on facebook" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci if dw ig"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M19.75 12.04c0-4.3-3.47-7.79-7.75-7.79a7.77 7.77 0 0 0-5.9 12.84 7.77 7.77 0 0 0 4.69 2.63v-5.49h-1.9v-2.2h1.9v-1.62c0-1.88 1.14-2.9 2.8-2.9.8 0 1.49.06 1.69.08v1.97h-1.15c-.91 0-1.1.43-1.1 1.07v1.4h2.17l-.28 2.2h-1.88v5.52a7.77 7.77 0 0 0 6.7-7.71" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="ie l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on linkedin" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci if dw ig"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M19.75 5.39v13.22a1.14 1.14 0 0 1-1.14 1.14H5.39a1.14 1.14 0 0 1-1.14-1.14V5.39a1.14 1.14 0 0 1 1.14-1.14h13.22a1.14 1.14 0 0 1 1.14 1.14zM8.81 10.18H6.53v7.3H8.8v-7.3zM9 7.67a1.31 1.31 0 0 0-1.3-1.32h-.04a1.32 1.32 0 0 0 0 2.64A1.31 1.31 0 0 0 9 7.71v-.04zm8.46 5.37c0-2.2-1.4-3.05-2.78-3.05a2.6 2.6 0 0 0-2.3 1.18h-.07v-1h-2.14v7.3h2.28V13.6a1.51 1.51 0 0 1 1.36-1.63h.09c.72 0 1.26.45 1.26 1.6v3.91h2.28l.02-4.43z" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="l fr"><div><div aria-hidden="false" class="ci"><button class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci if dw ig"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path clip-rule="evenodd" d="M3.57 14.67c0-.57.13-1.11.38-1.6l.02-.02v-.02l.02-.02c0-.02 0-.02.02-.02.12-.26.3-.52.57-.8L7.78 9v-.02l.01-.02c.44-.41.91-.7 1.44-.85a4.87 4.87 0 0 0-1.19 2.36A5.04 5.04 0 0 0 8 11.6L6.04 13.6c-.19.19-.32.4-.38.65a2 2 0 0 0 0 .9c.08.2.2.4.38.57l1.29 1.31c.27.28.62.42 1.03.42.42 0 .78-.14 1.06-.42l1.23-1.25.79-.78 1.15-1.16c.08-.09.19-.22.28-.4.1-.2.15-.42.15-.67 0-.16-.02-.3-.06-.45l-.02-.02v-.02l-.07-.14s0-.03-.04-.06l-.06-.13-.02-.02c0-.02 0-.03-.02-.05a.6.6 0 0 0-.14-.16l-.48-.5c0-.04.02-.1.04-.15l.06-.12 1.17-1.14.09-.09.56.57c.02.04.08.1.16.18l.05.04.03.06.04.05.03.04.04.06.1.14.02.02c0 .02.01.03.03.04l.1.2v.02c.1.16.2.38.3.68a1 1 0 0 1 .04.25 3.2 3.2 0 0 1 .02 1.33 3.49 3.49 0 0 1-.95 1.87l-.66.67-.97.97-1.56 1.57a3.4 3.4 0 0 1-2.47 1.02c-.97 0-1.8-.34-2.49-1.03l-1.3-1.3a3.55 3.55 0 0 1-1-2.51v-.01h-.02v.02zm5.39-3.43c0-.19.02-.4.07-.63.13-.74.44-1.37.95-1.87l.66-.67.97-.98 1.56-1.56c.68-.69 1.5-1.03 2.47-1.03.97 0 1.8.34 2.48 1.02l1.3 1.32a3.48 3.48 0 0 1 1 2.48c0 .58-.11 1.11-.37 1.6l-.02.02v.02l-.02.04c-.14.27-.35.54-.6.8L16.23 15l-.01.02-.01.02c-.44.42-.92.7-1.43.83a4.55 4.55 0 0 0 1.23-3.52L18 10.38c.18-.21.3-.42.35-.65a2.03 2.03 0 0 0-.01-.9 1.96 1.96 0 0 0-.36-.58l-1.3-1.3a1.49 1.49 0 0 0-1.06-.42c-.42 0-.77.14-1.06.4l-1.2 1.27-.8.8-1.16 1.15c-.08.08-.18.21-.29.4a1.66 1.66 0 0 0-.08 1.12l.02.03v.02l.06.14s.01.03.05.06l.06.13.02.02.01.02.01.02c.05.08.1.13.14.16l.47.5c0 .04-.02.09-.04.15l-.06.12-1.15 1.15-.1.08-.56-.56a2.3 2.3 0 0 0-.18-.19c-.02-.01-.02-.03-.02-.04l-.02-.02a.37.37 0 0 1-.1-.12c-.03-.03-.05-.04-.05-.06l-.1-.15-.02-.02-.02-.04-.08-.17v-.02a5.1 5.1 0 0 1-.28-.69 1.03 1.03 0 0 1-.04-.26c-.06-.23-.1-.46-.1-.7v.01z" fill="#A8A8A8" fill-rule="evenodd"></path></svg></span></button></div></div></div><div class="ih o ao"><span><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff2a5a262a766&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-dataset-shift-f2a5a262a766&amp;source=--------------------------bookmark_header-----------" rel="noopener follow"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="au de aw ax ay az ba if bc hv ij ik il"><svg aria-label="Add to list bookmark button" class="ii" fill="none" height="25" viewbox="0 0 25 25" width="25"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg></button></a></span></div></div><div class="ck im"><div><div aria-hidden="false" class="ci"></div></div></div></div></div><div class="in io ip j i d"><div class="fj l"><span><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff2a5a262a766&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-dataset-shift-f2a5a262a766&amp;source=--------------------------bookmark_header-----------" rel="noopener follow"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="au de aw iq ay az ba ir bc hv cd o ao is it il"><svg aria-label="Add to list bookmark button" class="ii" fill="none" height="25" viewbox="0 0 25 25" width="25"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg><p class="bm b bn bo cn">Save</p></button></a></span></div><div class="iu l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on twitter" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci if dw ig"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M20 5.34c-.67.41-1.4.7-2.18.87a3.45 3.45 0 0 0-5.02-.1 3.49 3.49 0 0 0-1.02 2.47c0 .28.03.54.07.8a9.91 9.91 0 0 1-7.17-3.66 3.9 3.9 0 0 0-.5 1.74 3.6 3.6 0 0 0 1.56 2.92 3.36 3.36 0 0 1-1.55-.44V10c0 1.67 1.2 3.08 2.8 3.42-.3.06-.6.1-.94.12l-.62-.06a3.5 3.5 0 0 0 3.24 2.43 7.34 7.34 0 0 1-4.36 1.49l-.81-.05a9.96 9.96 0 0 0 5.36 1.56c6.4 0 9.91-5.32 9.9-9.9v-.5c.69-.49 1.28-1.1 1.74-1.81-.63.3-1.3.48-2 .56A3.33 3.33 0 0 0 20 5.33" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="iu l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on facebook" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci if dw ig"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M19.75 12.04c0-4.3-3.47-7.79-7.75-7.79a7.77 7.77 0 0 0-5.9 12.84 7.77 7.77 0 0 0 4.69 2.63v-5.49h-1.9v-2.2h1.9v-1.62c0-1.88 1.14-2.9 2.8-2.9.8 0 1.49.06 1.69.08v1.97h-1.15c-.91 0-1.1.43-1.1 1.07v1.4h2.17l-.28 2.2h-1.88v5.52a7.77 7.77 0 0 0 6.7-7.71" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="iu l fr"><div><div aria-hidden="false" class="ci"><button aria-label="Share on linkedin" class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci if dw ig"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path d="M19.75 5.39v13.22a1.14 1.14 0 0 1-1.14 1.14H5.39a1.14 1.14 0 0 1-1.14-1.14V5.39a1.14 1.14 0 0 1 1.14-1.14h13.22a1.14 1.14 0 0 1 1.14 1.14zM8.81 10.18H6.53v7.3H8.8v-7.3zM9 7.67a1.31 1.31 0 0 0-1.3-1.32h-.04a1.32 1.32 0 0 0 0 2.64A1.31 1.31 0 0 0 9 7.71v-.04zm8.46 5.37c0-2.2-1.4-3.05-2.78-3.05a2.6 2.6 0 0 0-2.3 1.18h-.07v-1h-2.14v7.3h2.28V13.6a1.51 1.51 0 0 1 1.36-1.63h.09c.72 0 1.26.45 1.26 1.6v3.91h2.28l.02-4.43z" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="l fr"><div><div aria-hidden="false" class="ci"><button class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="ci if dw ig"><svg fill="none" height="24" viewbox="0 0 24 24" width="24"><path clip-rule="evenodd" d="M3.57 14.67c0-.57.13-1.11.38-1.6l.02-.02v-.02l.02-.02c0-.02 0-.02.02-.02.12-.26.3-.52.57-.8L7.78 9v-.02l.01-.02c.44-.41.91-.7 1.44-.85a4.87 4.87 0 0 0-1.19 2.36A5.04 5.04 0 0 0 8 11.6L6.04 13.6c-.19.19-.32.4-.38.65a2 2 0 0 0 0 .9c.08.2.2.4.38.57l1.29 1.31c.27.28.62.42 1.03.42.42 0 .78-.14 1.06-.42l1.23-1.25.79-.78 1.15-1.16c.08-.09.19-.22.28-.4.1-.2.15-.42.15-.67 0-.16-.02-.3-.06-.45l-.02-.02v-.02l-.07-.14s0-.03-.04-.06l-.06-.13-.02-.02c0-.02 0-.03-.02-.05a.6.6 0 0 0-.14-.16l-.48-.5c0-.04.02-.1.04-.15l.06-.12 1.17-1.14.09-.09.56.57c.02.04.08.1.16.18l.05.04.03.06.04.05.03.04.04.06.1.14.02.02c0 .02.01.03.03.04l.1.2v.02c.1.16.2.38.3.68a1 1 0 0 1 .04.25 3.2 3.2 0 0 1 .02 1.33 3.49 3.49 0 0 1-.95 1.87l-.66.67-.97.97-1.56 1.57a3.4 3.4 0 0 1-2.47 1.02c-.97 0-1.8-.34-2.49-1.03l-1.3-1.3a3.55 3.55 0 0 1-1-2.51v-.01h-.02v.02zm5.39-3.43c0-.19.02-.4.07-.63.13-.74.44-1.37.95-1.87l.66-.67.97-.98 1.56-1.56c.68-.69 1.5-1.03 2.47-1.03.97 0 1.8.34 2.48 1.02l1.3 1.32a3.48 3.48 0 0 1 1 2.48c0 .58-.11 1.11-.37 1.6l-.02.02v.02l-.02.04c-.14.27-.35.54-.6.8L16.23 15l-.01.02-.01.02c-.44.42-.92.7-1.43.83a4.55 4.55 0 0 0 1.23-3.52L18 10.38c.18-.21.3-.42.35-.65a2.03 2.03 0 0 0-.01-.9 1.96 1.96 0 0 0-.36-.58l-1.3-1.3a1.49 1.49 0 0 0-1.06-.42c-.42 0-.77.14-1.06.4l-1.2 1.27-.8.8-1.16 1.15c-.08.08-.18.21-.29.4a1.66 1.66 0 0 0-.08 1.12l.02.03v.02l.06.14s.01.03.05.06l.06.13.02.02.01.02.01.02c.05.08.1.13.14.16l.47.5c0 .04-.02.09-.04.15l-.06.12-1.15 1.15-.1.08-.56-.56a2.3 2.3 0 0 0-.18-.19c-.02-.01-.02-.03-.02-.04l-.02-.02a.37.37 0 0 1-.1-.12c-.03-.03-.05-.04-.05-.06l-.1-.15-.02-.02-.02-.04-.08-.17v-.02a5.1 5.1 0 0 1-.28-.69 1.03 1.03 0 0 1-.04-.26c-.06-.23-.1-.46-.1-.7v.01z" fill="#A8A8A8" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></header><span class="l"></span><section><div><div class="fo as ja jb jc jd"></div><div class="je jf jg jh ji"><div class=""><h1 class="pw-post-title jj jk jl bm jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ga" id="9d4c">Understanding Dataset Shift</h1></div><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="8ff3">How to not be fooled by the tricks data plays on you.</p><blockquote class="lg lh li"><p class="ki kj lj kk b kl km kn ko kp kq kr ks lk ku kv kw ll ky kz la lm lc ld le lf je ga" id="a705">Dataset shift is a challenging situation where the joint distribution of inputs and outputs differs between the training and test stages. <strong class="kk jm">— </strong><a class="au ln" href="https://cs.nyu.edu/~roweis/papers/invar-chapter.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="kk jm"><em class="jl">Dataset Shift, The MIT Press.</em></strong></a></p></blockquote><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="33bf"><a class="au ln" href="https://cs.nyu.edu/~roweis/papers/invar-chapter.pdf" rel="noopener ugc nofollow" target="_blank">Dataset shifting</a> is one of those topics which is simple, perhaps so simple that it is considered trivially obvious. In my own data science classes the idea was discussed briefly, however, I think a deeper discussion of the causes and manifestations of dataset shift are of benefit to the data science community.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="b521">The key theme of this article can be summarized in a single sentence:</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="94c4"><strong class="kk jm">Dataset shift is when the training and test distributions are different.</strong></p><figure class="lp lq lr ls gx lt gl gm paragraph-image"><div class="lu lv do lw ce lx" role="button" tabindex="0"><div class="gl gm lo"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/1*0giRszXm8eg951HKxzvWZA.png 640w, https://miro.medium.com/max/720/1*0giRszXm8eg951HKxzvWZA.png 720w, https://miro.medium.com/max/750/1*0giRszXm8eg951HKxzvWZA.png 750w, https://miro.medium.com/max/786/1*0giRszXm8eg951HKxzvWZA.png 786w, https://miro.medium.com/max/828/1*0giRszXm8eg951HKxzvWZA.png 828w, https://miro.medium.com/max/1100/1*0giRszXm8eg951HKxzvWZA.png 1100w, https://miro.medium.com/max/1400/1*0giRszXm8eg951HKxzvWZA.png 1400w"/><img alt="" class="ce ly lz c" height="317" loading="lazy" role="presentation" width="700"/></picture></div></div><figcaption class="ma bl gn gl gm mb mc bm b bn bo cn">An example of differing training and test distributions.</figcaption></figure><figure class="lp lq lr ls gx lt gl gm paragraph-image"><div class="lu lv do lw ce lx" role="button" tabindex="0"><div class="gl gm md"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/1*FaWL_bpexYoPmg9mQJCThA.png 640w, https://miro.medium.com/max/720/1*FaWL_bpexYoPmg9mQJCThA.png 720w, https://miro.medium.com/max/750/1*FaWL_bpexYoPmg9mQJCThA.png 750w, https://miro.medium.com/max/786/1*FaWL_bpexYoPmg9mQJCThA.png 786w, https://miro.medium.com/max/828/1*FaWL_bpexYoPmg9mQJCThA.png 828w, https://miro.medium.com/max/1100/1*FaWL_bpexYoPmg9mQJCThA.png 1100w, https://miro.medium.com/max/1400/1*FaWL_bpexYoPmg9mQJCThA.png 1400w"/><img alt="" class="ce ly lz c" height="76" loading="lazy" role="presentation" width="700"/></picture></div></div></figure><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="08d8">Whilst you may scoff at the triviality of such a statement, this is possibly the most common problem I see when viewing solutions to Kaggle challenges. In some ways, a deep understanding of dataset shifting is key to winning Kaggle competitions.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="85e2">Dataset shift is not a standard term and is sometimes referred to as <strong class="kk jm">concept shift</strong> or <strong class="kk jm">concept drift</strong>, <strong class="kk jm">changes of classification</strong>, <strong class="kk jm">changing environments</strong>, <strong class="kk jm">contrast mining in classification learning</strong>, <strong class="kk jm">fracture points</strong> and <strong class="kk jm">fractures between data.</strong></p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="616e">Dataset shifting occurs predominantly within the machine learning paradigm of supervised and the hybrid paradigm of semi-supervised learning.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="8762">The problem of dataset shift can stem from the way input features are utilized, the way training and test sets are selected, data sparsity, shifts in the data distribution due to non-stationary environments, and also from changes in the activation patterns within layers of deep neural networks.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="e7a5">Why is dataset shift important?</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="bb76">It is application-dependent and thus relies largely on the skill of the data scientist to examine and resolve. For example, how does one determine when the dataset has shifted sufficiently to pose a problem to our algorithms? If only certain features begin to diverge, how do we determine the trade-off between the loss of accuracy by removing features and the loss of accuracy by a misrepresented data distribution?</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="03b9">In this article, I will discuss the different types of dataset shift, problems that can arise from their presence, and current best practices that one can use to avoid them. This article contains no code examples and is purely conceptual. Classification examples will be used for ease of demonstration.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="89c0">There are multiple manifestations of dataset shift that we will examine:</p><ul class=""><li class="me mf jl kk b kl km kp kq kt mg kx mh lb mi lf mj mk ml mm ga" id="9f83">Covariate shift</li><li class="me mf jl kk b kl mn kp mo kt mp kx mq lb mr lf mj mk ml mm ga" id="f617">Prior probability shift</li><li class="me mf jl kk b kl mn kp mo kt mp kx mq lb mr lf mj mk ml mm ga" id="6ec7">Concept shift</li><li class="me mf jl kk b kl mn kp mo kt mp kx mq lb mr lf mj mk ml mm ga" id="d655">Internal covariate shift (an important subtype of covariate shift)</li></ul><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="3156">This is a huge and important topic in machine learning so do not expect a comprehensive overview of this area. If the reader is interested in this subject then are a plethora of research articles on the topic — the vast majority of which focus on covariate shift.</p></div><div class="o dx ms mt in mu" role="separator"><span class="mv fl ci mw mx my"></span><span class="mv fl ci mw mx my"></span><span class="mv fl ci mw mx"></span></div><div class="je jf jg jh ji"><h1 class="mz na jl bm nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw ga" id="de77">Covariate shift</h1><p class="pw-post-body-paragraph ki kj jl kk b kl nx kn ko kp ny kr ks kt nz kv kw kx oa kz la lb ob ld le lf je ga" id="a428">Of all the manifestations of dataset shift, the simplest to understand is covariate shift.</p><blockquote class="lg lh li"><p class="ki kj lj kk b kl km kn ko kp kq kr ks lk ku kv kw ll ky kz la lm lc ld le lf je ga" id="e46a">Covariate shift is the change in the distribution of the <em class="jl">covariates </em>specifically, that is, the independent variables. This is normally due to changes in state of latent variables, which could be temporal (even changes to the stationarity of a temporal process), or spatial, or less obvious. — <a class="au ln" href="https://www.quora.com/What-is-Covariate-shift" rel="noopener ugc nofollow" target="_blank"><strong class="kk jm">Quora</strong></a></p></blockquote><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="74a7">Covariate shift is the scholarly term for when the distribution of the data (i.e. our input features) changes.</p><figure class="lp lq lr ls gx lt gl gm paragraph-image"><div class="lu lv do lw ce lx" role="button" tabindex="0"><div class="gl gm oc"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/1*vWURgfF6EJ0QpWaVm5grhw.png 640w, https://miro.medium.com/max/720/1*vWURgfF6EJ0QpWaVm5grhw.png 720w, https://miro.medium.com/max/750/1*vWURgfF6EJ0QpWaVm5grhw.png 750w, https://miro.medium.com/max/786/1*vWURgfF6EJ0QpWaVm5grhw.png 786w, https://miro.medium.com/max/828/1*vWURgfF6EJ0QpWaVm5grhw.png 828w, https://miro.medium.com/max/1100/1*vWURgfF6EJ0QpWaVm5grhw.png 1100w, https://miro.medium.com/max/1400/1*vWURgfF6EJ0QpWaVm5grhw.png 1400w"/><img alt="" class="ce ly lz c" height="106" loading="lazy" role="presentation" width="700"/></picture></div></div></figure><figure class="lp lq lr ls gx lt gl gm paragraph-image"><div class="lu lv do lw ce lx" role="button" tabindex="0"><div class="gl gm od"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/1*3MLwY2rMHziKHPfZEkIuzA.png 640w, https://miro.medium.com/max/720/1*3MLwY2rMHziKHPfZEkIuzA.png 720w, https://miro.medium.com/max/750/1*3MLwY2rMHziKHPfZEkIuzA.png 750w, https://miro.medium.com/max/786/1*3MLwY2rMHziKHPfZEkIuzA.png 786w, https://miro.medium.com/max/828/1*3MLwY2rMHziKHPfZEkIuzA.png 828w, https://miro.medium.com/max/1100/1*3MLwY2rMHziKHPfZEkIuzA.png 1100w, https://miro.medium.com/max/1400/1*3MLwY2rMHziKHPfZEkIuzA.png 1400w"/><img alt="" class="ce ly lz c" height="563" loading="lazy" role="presentation" width="700"/></picture></div></div></figure><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="8af9">Here are some examples where covariate shift is likely to cause problems:</p><ul class=""><li class="me mf jl kk b kl km kp kq kt mg kx mh lb mi lf mj mk ml mm ga" id="6325">Face recognition algorithms that are trained predominantly on younger faces, yet the dataset has a much larger proportion of older faces in it.</li><li class="me mf jl kk b kl mn kp mo kt mp kx mq lb mr lf mj mk ml mm ga" id="f318">Predicting life expectancy but having very few samples in the training set of individuals that smoke, and many more samples of this in the training set.</li><li class="me mf jl kk b kl mn kp mo kt mp kx mq lb mr lf mj mk ml mm ga" id="787a">Classifying images as either cats or dogs and omitting certain species from the training set that are seen in the test set.</li></ul><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="2006">In this case, there is no change in the underlying relationship between the input and output (the regression line is still the same), yet part of that relationship is data-sparse, omitted, or misrepresented such that the test set and training set do not reflect the same distribution.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="f4fa">Covariance shift can cause a lot of problems when performing cross-validation. Cross-validation is almost unbiased without covariate shift but it is heavily biased under covariate shift!</p></div><div class="o dx ms mt in mu" role="separator"><span class="mv fl ci mw mx my"></span><span class="mv fl ci mw mx my"></span><span class="mv fl ci mw mx"></span></div><div class="je jf jg jh ji"><h1 class="mz na jl bm nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw ga" id="1e81">Prior Probability Shift</h1><p class="pw-post-body-paragraph ki kj jl kk b kl nx kn ko kp ny kr ks kt nz kv kw kx oa kz la lb ob ld le lf je ga" id="fde4">Whilst covariate shift focuses on changes in the feature (<strong class="kk jm"><em class="lj">x</em></strong>) distribution, prior probability shift focuses on changes in the distribution of the class variable <strong class="kk jm"><em class="lj">y</em></strong>.</p><figure class="lp lq lr ls gx lt gl gm paragraph-image"><div class="lu lv do lw ce lx" role="button" tabindex="0"><div class="gl gm oe"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/1*cB8oiGdLbvhMZhHqgCiXtw.png 640w, https://miro.medium.com/max/720/1*cB8oiGdLbvhMZhHqgCiXtw.png 720w, https://miro.medium.com/max/750/1*cB8oiGdLbvhMZhHqgCiXtw.png 750w, https://miro.medium.com/max/786/1*cB8oiGdLbvhMZhHqgCiXtw.png 786w, https://miro.medium.com/max/828/1*cB8oiGdLbvhMZhHqgCiXtw.png 828w, https://miro.medium.com/max/1100/1*cB8oiGdLbvhMZhHqgCiXtw.png 1100w, https://miro.medium.com/max/1400/1*cB8oiGdLbvhMZhHqgCiXtw.png 1400w"/><img alt="" class="ce ly lz c" height="363" loading="eager" role="presentation" width="700"/></picture></div></div></figure><figure class="lp lq lr ls gx lt gl gm paragraph-image"><div class="lu lv do lw ce lx" role="button" tabindex="0"><div class="gl gm of"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/1*a8N2VA7V0WeB3oGRFE6dIw.png 640w, https://miro.medium.com/max/720/1*a8N2VA7V0WeB3oGRFE6dIw.png 720w, https://miro.medium.com/max/750/1*a8N2VA7V0WeB3oGRFE6dIw.png 750w, https://miro.medium.com/max/786/1*a8N2VA7V0WeB3oGRFE6dIw.png 786w, https://miro.medium.com/max/828/1*a8N2VA7V0WeB3oGRFE6dIw.png 828w, https://miro.medium.com/max/1100/1*a8N2VA7V0WeB3oGRFE6dIw.png 1100w, https://miro.medium.com/max/1400/1*a8N2VA7V0WeB3oGRFE6dIw.png 1400w"/><img alt="" class="ce ly lz c" height="112" loading="eager" role="presentation" width="700"/></picture></div></div></figure><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="2e1c">This type of shifting may seem slightly more confusing but is it essentially the reverse of covariate shift. An intuitive way to think about it might be to consider an unbalanced dataset.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="0c76">If the training set has equal prior probabilities on the number of spam emails that you receive (i.e. the probability of an email being spam is 0.5), then we would expect 50% of the training set to contain spam emails and 50% to contain non-spam.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="d3cf">If, in reality, only 90% of our emails are spam (perhaps not unlikely), then our prior probability of the class variables has changed. This idea has relations to data sparsity and biased feature selection that are factors in causing covariance shift, but instead of influencing our input distribution, they instead influence our output distribution.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="6636">This problem only occurs in Y → X problems and is commonly associated with naive Bayes (hence the spam example, since naive Bayes is commonly used to filter spam emails).</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="697e">The below figure on prior probability shift is taken from the <a class="au ln" href="http://www.acad.bg/ebook/ml/The.MIT.Press.Dataset.Shift.in.Machine.Learning.Feb.2009.eBook-DDU.pdf" rel="noopener ugc nofollow" target="_blank">Dataset Shift in Machine Learning</a> book and illustrates this case nicely.</p><figure class="lp lq lr ls gx lt gl gm paragraph-image"><div class="lu lv do lw ce lx" role="button" tabindex="0"><div class="gl gm og"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/1*WbFYon664l1jjqbYPsu4iw.png 640w, https://miro.medium.com/max/720/1*WbFYon664l1jjqbYPsu4iw.png 720w, https://miro.medium.com/max/750/1*WbFYon664l1jjqbYPsu4iw.png 750w, https://miro.medium.com/max/786/1*WbFYon664l1jjqbYPsu4iw.png 786w, https://miro.medium.com/max/828/1*WbFYon664l1jjqbYPsu4iw.png 828w, https://miro.medium.com/max/1100/1*WbFYon664l1jjqbYPsu4iw.png 1100w, https://miro.medium.com/max/1400/1*WbFYon664l1jjqbYPsu4iw.png 1400w"/><img alt="" class="ce ly lz c" height="290" loading="lazy" role="presentation" width="700"/></picture></div></div></figure></div><div class="o dx ms mt in mu" role="separator"><span class="mv fl ci mw mx my"></span><span class="mv fl ci mw mx my"></span><span class="mv fl ci mw mx"></span></div><div class="je jf jg jh ji"><h1 class="mz na jl bm nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw ga" id="0e29"><strong class="ba">Concept Drift</strong></h1><p class="pw-post-body-paragraph ki kj jl kk b kl nx kn ko kp ny kr ks kt nz kv kw kx oa kz la lb ob ld le lf je ga" id="3d16">Concept drift is different from covariate and prior probability shift in that it is not related to the data distribution or the class distribution but instead is related to the relationship between the two variables.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="8a14">An intuitive way to think about this idea is by looking at time series analysis.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="e342">In time series analysis, it is common to examine whether the time series is stationary before performing any analysis, as stationary time series are much easier to analyze than non-stationary time series.</p><figure class="lp lq lr ls gx lt gl gm paragraph-image"><div class="lu lv do lw ce lx" role="button" tabindex="0"><div class="gl gm oh"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/0*5ods4McATY6DHK84.png 640w, https://miro.medium.com/max/720/0*5ods4McATY6DHK84.png 720w, https://miro.medium.com/max/750/0*5ods4McATY6DHK84.png 750w, https://miro.medium.com/max/786/0*5ods4McATY6DHK84.png 786w, https://miro.medium.com/max/828/0*5ods4McATY6DHK84.png 828w, https://miro.medium.com/max/1100/0*5ods4McATY6DHK84.png 1100w, https://miro.medium.com/max/1400/0*5ods4McATY6DHK84.png 1400w"/><img alt="" class="ce ly lz c" height="377" loading="eager" role="presentation" width="700"/></picture></div></div></figure><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="475a">Why is this the case?</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="fea7">This is easier because the relationship between the input and output is not consistently changing! There are ways of detrending a time series to make it stationary, but this does not always work (such as in the case of stock indices that generally contain little autocorrelation or secular variation).</p><figure class="lp lq lr ls gx lt gl gm paragraph-image"><div class="lu lv do lw ce lx" role="button" tabindex="0"><div class="gl gm oi"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/1*Q_zYnVXvHpD86BCcTENkNw.png 640w, https://miro.medium.com/max/720/1*Q_zYnVXvHpD86BCcTENkNw.png 720w, https://miro.medium.com/max/750/1*Q_zYnVXvHpD86BCcTENkNw.png 750w, https://miro.medium.com/max/786/1*Q_zYnVXvHpD86BCcTENkNw.png 786w, https://miro.medium.com/max/828/1*Q_zYnVXvHpD86BCcTENkNw.png 828w, https://miro.medium.com/max/1100/1*Q_zYnVXvHpD86BCcTENkNw.png 1100w, https://miro.medium.com/max/1400/1*Q_zYnVXvHpD86BCcTENkNw.png 1400w"/><img alt="" class="ce ly lz c" height="119" loading="lazy" role="presentation" width="700"/></picture></div></div></figure><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="239b">To give a more concrete example, let’s say we examined the profits of companies before the 2008 financial crisis and made an algorithm to predict the profit based on factors such as the industry, number of employees, information about products, and so on. If our algorithm is trained on data from 2000–2007, but are not using it to predict the same information after the financial crisis, it is likely to perform poorly.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="ba35">So what changed? Clearly, the overall relationship between the inputs and outputs changed due to the new socio-economic environment, and, if these are not reflected in our variables (such as having a dummy variable for the date that the financial crisis occurred and training data before and after this date) then our model is going to suffer the consequences of concept shift.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="33e8">In our specific case, we would expect to see profits change markedly in the years after the financial crisis (this is an example of an <a class="au ln" href="https://en.wikipedia.org/wiki/Interrupted_time_series" rel="noopener ugc nofollow" target="_blank">interrupted time series</a>).</p></div><div class="o dx ms mt in mu" role="separator"><span class="mv fl ci mw mx my"></span><span class="mv fl ci mw mx my"></span><span class="mv fl ci mw mx"></span></div><div class="je jf jg jh ji"><h1 class="mz na jl bm nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw ga" id="9a04">Internal Covariate Shift</h1><p class="pw-post-body-paragraph ki kj jl kk b kl nx kn ko kp ny kr ks kt nz kv kw kx oa kz la lb ob ld le lf je ga" id="6290">One reason this topic has gained interest recently is due to the suspected influence of covariance shift in the hidden layers of deep neural networks (hence the word ‘internal’).</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="f630">Researchers found that due to the variation in the distribution of activations from the output of a given hidden layer, which are used as the input to a subsequent layer, the network layers can suffer from covariate shift which can impede the training of deep neural networks.</p><figure class="lp lq lr ls gx lt gl gm paragraph-image"><div class="o gy dx cm"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/proxy/1*X76xBHMpT4rzws_-L2s1TA.png 640w, https://miro.medium.com/proxy/1*X76xBHMpT4rzws_-L2s1TA.png 720w, https://miro.medium.com/proxy/1*X76xBHMpT4rzws_-L2s1TA.png 750w, https://miro.medium.com/proxy/1*X76xBHMpT4rzws_-L2s1TA.png 786w, https://miro.medium.com/proxy/1*X76xBHMpT4rzws_-L2s1TA.png 828w, https://miro.medium.com/proxy/1*X76xBHMpT4rzws_-L2s1TA.png 1100w, https://miro.medium.com/proxy/1*X76xBHMpT4rzws_-L2s1TA.png 1400w"/><img alt="Image result for internal covariate shift" class="ly ce lz c" loading="eager" width="700"/></picture></div><figcaption class="ma bl gn gl gm mb mc bm b bn bo cn">The situation without batch normalization, network activations are exposed to varying data input distributions that propagate through the network and distort the learned distributions.</figcaption></figure><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="40ad">This idea is the stimulus of <a class="au ln" href="https://en.wikipedia.org/wiki/Batch_normalization" rel="noopener ugc nofollow" target="_blank">batch normalization</a>, proposed by Christian Szegedy and Sergey Ioffe in their 2015 paper <a class="au ln" href="https://arxiv.org/pdf/1502.03167.pdf" rel="noopener ugc nofollow" target="_blank"><em class="lj">“Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift”</em></a><em class="lj">.</em></p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="3467">The authors propose that internal covariate shift in the hidden layers slows down training and requires lower learning rates and careful parameter initialization. They resolve this by normalizing the inputs to hidden layers by adding a batch normalization layer.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="e0b2">This batch norm layer takes the mean and standard deviation of a batch of samples and uses them to standardize the input. This also adds some noise to the inputs (because of the noise inherent in the mean and standard deviation between different batches) which helps to regularize the network.</p><figure class="lp lq lr ls gx lt gl gm paragraph-image"><div class="lu lv do lw ce lx" role="button" tabindex="0"><div class="gl gm oj"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/1*y-hdspSxBxe9EL8vz76Drw.png 640w, https://miro.medium.com/max/720/1*y-hdspSxBxe9EL8vz76Drw.png 720w, https://miro.medium.com/max/750/1*y-hdspSxBxe9EL8vz76Drw.png 750w, https://miro.medium.com/max/786/1*y-hdspSxBxe9EL8vz76Drw.png 786w, https://miro.medium.com/max/828/1*y-hdspSxBxe9EL8vz76Drw.png 828w, https://miro.medium.com/max/1100/1*y-hdspSxBxe9EL8vz76Drw.png 1100w, https://miro.medium.com/max/1400/1*y-hdspSxBxe9EL8vz76Drw.png 1400w"/><img alt="" class="ce ly lz c" height="357" loading="lazy" role="presentation" width="700"/></picture></div></div><figcaption class="ma bl gn gl gm mb mc bm b bn bo cn">How batch normalization fits within the network architecture of deep neural networks.</figcaption></figure><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="8d04">This problem acts to translate the varying distribution to more stable internal data distributions (less drift/oscillations) that helps to stabilize learning.</p><figure class="lp lq lr ls gx lt gl gm paragraph-image"><div class="gl gm ok"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 638px" srcset="https://miro.medium.com/max/640/0*q68JPcHskgSUBAgr 640w, https://miro.medium.com/max/720/0*q68JPcHskgSUBAgr 720w, https://miro.medium.com/max/750/0*q68JPcHskgSUBAgr 750w, https://miro.medium.com/max/786/0*q68JPcHskgSUBAgr 786w, https://miro.medium.com/max/828/0*q68JPcHskgSUBAgr 828w, https://miro.medium.com/max/1100/0*q68JPcHskgSUBAgr 1100w, https://miro.medium.com/max/1276/0*q68JPcHskgSUBAgr 1276w"/><img alt="" class="ce ly lz c" height="466" loading="lazy" role="presentation" width="638"/></picture></div><figcaption class="ma bl gn gl gm mb mc bm b bn bo cn">Varying data distributions across batches are normalized via a batch normalization layer in order to stabilize the data distribution used as input to subsequent layers in a deep neural network.</figcaption></figure><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="a3c5">Batch normalization is now well adopted in the deep learning community, although a <a class="au ln" href="https://arxiv.org/pdf/1805.11604.pdf" rel="noopener ugc nofollow" target="_blank">recent paper</a> alluded that the improved results obtained from this technique may not be purely due to the suppression of internal covariate shift, and may instead be a result of smoothing the loss landscape of the network.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="0662">For those unfamiliar with batch normalization, its purpose, and its implementation, I recommend looking at the relevant Youtube videos of Andrew Ng, one of which is linked below.</p><figure class="lp lq lr ls gx lt"><div class="m fs l do"><div class="ol om l"></div></div></figure></div><div class="o dx ms mt in mu" role="separator"><span class="mv fl ci mw mx my"></span><span class="mv fl ci mw mx my"></span><span class="mv fl ci mw mx"></span></div><div class="je jf jg jh ji"><h1 class="mz na jl bm nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw ga" id="3e67">Major Causes of Dataset Shift</h1><p class="pw-post-body-paragraph ki kj jl kk b kl nx kn ko kp ny kr ks kt nz kv kw kx oa kz la lb ob ld le lf je ga" id="7a83">The two most common causes of dataset shift are (1) <strong class="kk jm">sample selection bias</strong> and (2) <strong class="kk jm">non-stationary environments</strong>.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="5de8">It is important to note that these are not types of dataset shift, and do not always result in dataset shift. They are merely potential reasons that dataset shift can occur in our data.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="2444"><strong class="kk jm">Sample selection bias: </strong>the discrepancy in distribution is due to training data having been obtained through a biased method, and thus do not represent reliably the operating environment where the classifier is to be deployed (which, in machine learning terms, would constitute the test set).</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="1276"><strong class="kk jm">Non -stationary environments: </strong>when the training environment is different from the test one, whether it is due to a temporal or a spatial change.</p><h2 class="on na jl bm nb oo op oq nf or os ot nj kt ou ov nn kx ow ox nr lb oy oz nv pa ga" id="a3fe">Sample Selection Bias</h2><p class="pw-post-body-paragraph ki kj jl kk b kl nx kn ko kp ny kr ks kt nz kv kw kx oa kz la lb ob ld le lf je ga" id="f0c8">Sample selection bias is not a flaw with any algorithm or handling of the data. It is purely a systematic flaw in the process of data collection or labeling which causes nonuniform selection of training examples from a population, which causes biases to form during training.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="a290">Sample selection bias is a form of covariance shift since we are influencing our data distribution.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="c1f8">This can be thought of as a misrepresentation of the operating environment such that our model optimizes its training environment to a factitious or cherry-picked operating environment.</p><figure class="lp lq lr ls gx lt gl gm paragraph-image"><div class="lu lv do lw ce lx" role="button" tabindex="0"><div class="gl gm oe"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/1*u4noYjd10_Yjz6hCJHFMJA.png 640w, https://miro.medium.com/max/720/1*u4noYjd10_Yjz6hCJHFMJA.png 720w, https://miro.medium.com/max/750/1*u4noYjd10_Yjz6hCJHFMJA.png 750w, https://miro.medium.com/max/786/1*u4noYjd10_Yjz6hCJHFMJA.png 786w, https://miro.medium.com/max/828/1*u4noYjd10_Yjz6hCJHFMJA.png 828w, https://miro.medium.com/max/1100/1*u4noYjd10_Yjz6hCJHFMJA.png 1100w, https://miro.medium.com/max/1400/1*u4noYjd10_Yjz6hCJHFMJA.png 1400w"/><img alt="" class="ce ly lz c" height="213" loading="lazy" role="presentation" width="700"/></picture></div></div></figure><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="707d">Dataset shift resulting from sample selection bias is especially relevant when dealing with imbalanced classification, because, in highly imbalanced domains, the minority class is particularly sensitive to singular classification errors, due to the typically low number of samples it presents.</p><figure class="lp lq lr ls gx lt gl gm paragraph-image"><div class="lu lv do lw ce lx" role="button" tabindex="0"><div class="gl gm pb"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/1*TO_p8KEzl8325wp69q0rIg.png 640w, https://miro.medium.com/max/720/1*TO_p8KEzl8325wp69q0rIg.png 720w, https://miro.medium.com/max/750/1*TO_p8KEzl8325wp69q0rIg.png 750w, https://miro.medium.com/max/786/1*TO_p8KEzl8325wp69q0rIg.png 786w, https://miro.medium.com/max/828/1*TO_p8KEzl8325wp69q0rIg.png 828w, https://miro.medium.com/max/1100/1*TO_p8KEzl8325wp69q0rIg.png 1100w, https://miro.medium.com/max/1400/1*TO_p8KEzl8325wp69q0rIg.png 1400w"/><img alt="" class="ce ly lz c" height="326" loading="lazy" role="presentation" width="700"/></picture></div></div><figcaption class="ma bl gn gl gm mb mc bm b bn bo cn">Example of the impact of dataset shift in imbalanced domains.</figcaption></figure><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="4bcf">In the most extreme cases, a single misclassified example of the minority class can create a significant drop in performance.</p><h2 class="on na jl bm nb oo op oq nf or os ot nj kt ou ov nn kx ow ox nr lb oy oz nv pa ga" id="fd1b">Non -stationary environments</h2><p class="pw-post-body-paragraph ki kj jl kk b kl nx kn ko kp ny kr ks kt nz kv kw kx oa kz la lb ob ld le lf je ga" id="5063">In real-world applications it world applications, it is often the case that the data is not (time- or space-) stationary.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="634c">One of the most relevant non-stationary scenarios involves adversarial classification problems, such as spam filtering and network intrusion detection.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="0516">This type of problem is receiving an increasing increasing amount of attention in the machine learning field and usually copes with non-stationary environments due to the existence of an adversary that tries to work around the existing classifier’s learned concepts. In terms of the machine learning task, this adversary warps the test set so that it becomes different from the training set, thus introducing any possible kind of dataset shift.</p></div><div class="o dx ms mt in mu" role="separator"><span class="mv fl ci mw mx my"></span><span class="mv fl ci mw mx my"></span><span class="mv fl ci mw mx"></span></div><div class="je jf jg jh ji"><h1 class="mz na jl bm nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw ga" id="ea17">Identifying Dataset Shift</h1><p class="pw-post-body-paragraph ki kj jl kk b kl nx kn ko kp ny kr ks kt nz kv kw kx oa kz la lb ob ld le lf je ga" id="bd78">There are several methods that can be used to determine whether shifting is present in a dataset and its severity.</p><figure class="lp lq lr ls gx lt gl gm paragraph-image"><div class="lu lv do lw ce lx" role="button" tabindex="0"><div class="gl gm pc"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/1*uwp8xm8T9Hney1WZtHWwWQ.png 640w, https://miro.medium.com/max/720/1*uwp8xm8T9Hney1WZtHWwWQ.png 720w, https://miro.medium.com/max/750/1*uwp8xm8T9Hney1WZtHWwWQ.png 750w, https://miro.medium.com/max/786/1*uwp8xm8T9Hney1WZtHWwWQ.png 786w, https://miro.medium.com/max/828/1*uwp8xm8T9Hney1WZtHWwWQ.png 828w, https://miro.medium.com/max/1100/1*uwp8xm8T9Hney1WZtHWwWQ.png 1100w, https://miro.medium.com/max/1400/1*uwp8xm8T9Hney1WZtHWwWQ.png 1400w"/><img alt="" class="ce ly lz c" height="224" loading="eager" role="presentation" width="700"/></picture></div></div><figcaption class="ma bl gn gl gm mb mc bm b bn bo cn">Tree diagram showing the methods of identifying dataset shift.</figcaption></figure><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="9d8e">Unsupervised methods are perhaps the most useful ways of identifying dataset shift, as they do not require post-hoc analysis to be done, the latency of which cannot be afforded in some production systems. Supervised methods exist which essentially look at growing errors as the model runs and the performance on an external holdout (validation set).</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="8249"><strong class="kk jm">Statistical Distance</strong><br/>The <a class="au ln" href="https://en.wikipedia.org/wiki/Statistical_distance" rel="noopener ugc nofollow" target="_blank">s<em class="lj">tatistical distance</em></a> method is useful for detecting if your model predictions change over time. This is done by creating and using histograms. By making histograms, you are not only able to detect whether your model predictions change over time, but also check if your most important features change over time. Simply put, you form histograms of your training data, keep track of them over time, and compare them to see any changes. This method is used commonly by financial institutions on credit-scoring models.</p><figure class="lp lq lr ls gx lt gl gm paragraph-image"><div class="lu lv do lw ce lx" role="button" tabindex="0"><div class="gl gm pd"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/0*8pehAal_Mwbgaw-S.png 640w, https://miro.medium.com/max/720/0*8pehAal_Mwbgaw-S.png 720w, https://miro.medium.com/max/750/0*8pehAal_Mwbgaw-S.png 750w, https://miro.medium.com/max/786/0*8pehAal_Mwbgaw-S.png 786w, https://miro.medium.com/max/828/0*8pehAal_Mwbgaw-S.png 828w, https://miro.medium.com/max/1100/0*8pehAal_Mwbgaw-S.png 1100w, https://miro.medium.com/max/1400/0*8pehAal_Mwbgaw-S.png 1400w"/><img alt="" class="ce ly lz c" height="350" loading="lazy" role="presentation" width="700"/></picture></div></div><figcaption class="ma bl gn gl gm mb mc bm b bn bo cn">Two distributions are their KL-divergence (effectively the ‘distance’ between the two distributions). If the two distributions overlap, they are effectively the same distribution and the KL-divergence is zero.</figcaption></figure><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="9857">There are several metrics which can be used to monitor the change in model predictions over time. These include the <a class="au ln" href="https://www.quora.com/What-is-population-stability-index" rel="noopener ugc nofollow" target="_blank"><strong class="kk jm">Population Stability Index</strong></a><strong class="kk jm"> (PSI)</strong>, <a class="au ln" href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test" rel="noopener ugc nofollow" target="_blank"><strong class="kk jm">Kolmogorov-Smirnov statistic</strong></a>, <a class="au ln" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank"><strong class="kk jm">Kullback-Lebler divergence</strong></a> (or other <a class="au ln" href="https://en.wikipedia.org/wiki/F-divergence" rel="noopener ugc nofollow" target="_blank"><em class="lj">f-</em>divergences</a>), and <a class="au ln" href="http://blog.datadive.net/histogram-intersection-for-change-detection/" rel="noopener ugc nofollow" target="_blank"><strong class="kk jm">histogram intersection</strong></a>.</p><figure class="lp lq lr ls gx lt gl gm paragraph-image"><div class="gl gm pe"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 640px" srcset="https://miro.medium.com/max/640/0*5xvLZOGQZilh5U_S.png 640w, https://miro.medium.com/max/720/0*5xvLZOGQZilh5U_S.png 720w, https://miro.medium.com/max/750/0*5xvLZOGQZilh5U_S.png 750w, https://miro.medium.com/max/786/0*5xvLZOGQZilh5U_S.png 786w, https://miro.medium.com/max/828/0*5xvLZOGQZilh5U_S.png 828w, https://miro.medium.com/max/1100/0*5xvLZOGQZilh5U_S.png 1100w, https://miro.medium.com/max/1280/0*5xvLZOGQZilh5U_S.png 1280w"/><img alt="" class="ce ly lz c" height="478" loading="lazy" role="presentation" width="640"/></picture></div><figcaption class="ma bl gn gl gm mb mc bm b bn bo cn">Data plotted along one feature axis for a training and test set. There is ~72% intersection of the distributions which indicates a reasonable level of covariate shift between the distributions.</figcaption></figure><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="0520">The major disadvantage of this method is that is not great for high-dimensional or sparse features. However, it can be very useful and in my opinion should be the first thing to try when dealing with this issue.</p><figure class="lp lq lr ls gx lt gl gm paragraph-image"><div class="lu lv do lw ce lx" role="button" tabindex="0"><div class="gl gm pf"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/1*ys9UIdNFJmmbMsphe-U2wg.png 640w, https://miro.medium.com/max/720/1*ys9UIdNFJmmbMsphe-U2wg.png 720w, https://miro.medium.com/max/750/1*ys9UIdNFJmmbMsphe-U2wg.png 750w, https://miro.medium.com/max/786/1*ys9UIdNFJmmbMsphe-U2wg.png 786w, https://miro.medium.com/max/828/1*ys9UIdNFJmmbMsphe-U2wg.png 828w, https://miro.medium.com/max/1100/1*ys9UIdNFJmmbMsphe-U2wg.png 1100w, https://miro.medium.com/max/1400/1*ys9UIdNFJmmbMsphe-U2wg.png 1400w"/><img alt="" class="ce ly lz c" height="307" loading="lazy" role="presentation" width="700"/></picture></div></div><figcaption class="ma bl gn gl gm mb mc bm b bn bo cn">A comparison between KL-divergence, KS statistic, PSI, and histogram intersection for two examples. The left example shows little to no covariate shift, whilst the right example shows a substantial covariate shift. Notice how it affects the expected values of the statistical distances.</figcaption></figure><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="8c66"><strong class="kk jm">2) Novelty Detection</strong><br/>A method that is more amenable to fairly complex domains such as computer vision, is <a class="au ln" href="https://en.wikipedia.org/wiki/Novelty_detection" rel="noopener ugc nofollow" target="_blank"><em class="lj">novelty detection</em></a>. The idea is to create a model for modeling source distribution. Given a new data point, you try to test what is the likelihood that this data point is drawn from the source distribution. For this method, you can use various techniques such as a one-class support vector machine, available in most common libraries.</p><figure class="lp lq lr ls gx lt gl gm paragraph-image"><div class="gl gm pe"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 640px" srcset="https://miro.medium.com/max/640/0*u_NPdjKgwRSg1qAJ.png 640w, https://miro.medium.com/max/720/0*u_NPdjKgwRSg1qAJ.png 720w, https://miro.medium.com/max/750/0*u_NPdjKgwRSg1qAJ.png 750w, https://miro.medium.com/max/786/0*u_NPdjKgwRSg1qAJ.png 786w, https://miro.medium.com/max/828/0*u_NPdjKgwRSg1qAJ.png 828w, https://miro.medium.com/max/1100/0*u_NPdjKgwRSg1qAJ.png 1100w, https://miro.medium.com/max/1280/0*u_NPdjKgwRSg1qAJ.png 1280w"/><img alt="" class="ce ly lz c" height="480" loading="lazy" role="presentation" width="640"/></picture></div></figure><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="2c98">If you are in a regime of homogenous but very complex interactions (e.g. visual, audio, or remote sensing), then this is a method you should look into, because in that case, the statistical distance (histogram method) won’t be as effective a method.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="96ff">The major disadvantage of this method is that it cannot tell you explicitly what has changed, only that there has been a change.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="3798"><strong class="kk jm">3) Discriminative Distance</strong><br/>The <a class="au ln" href="https://www.sciencedirect.com/science/article/abs/pii/S0031320313000307" rel="noopener ugc nofollow" target="_blank"><em class="lj">discriminative distance</em></a> method is less common, nonetheless, it can be effective. The intuition is that you want to train a classifier to detect whether or not an example is from the source or target domain. You can use the training error as proxy of the distance between those two distributions. The higher the error, the closer they are (i.e. the classifier cannot discriminate between the source and target domain).</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="f642">Discriminative distance is widely applicable and high dimensional. Though it takes time and can be very complicated, this method is a useful technique if you are doing domain adaptation (and for some deep learning methods, this may be the only feasible technique that exists).</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="9754">This method is good for high-dimensional and sparse data, and is widely applicable. However, it can only be done offline and is more complicated to implement than the previous methods.</p></div><div class="o dx ms mt in mu" role="separator"><span class="mv fl ci mw mx my"></span><span class="mv fl ci mw mx my"></span><span class="mv fl ci mw mx"></span></div><div class="je jf jg jh ji"><h1 class="mz na jl bm nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw ga" id="fb7a">Handling Dataset Shift</h1><p class="pw-post-body-paragraph ki kj jl kk b kl nx kn ko kp ny kr ks kt nz kv kw kx oa kz la lb ob ld le lf je ga" id="3995">How do you correct dataset shift? If possible, you should always retrain. Of course, in some situations, it may not be possible, for example, if there are latency problems with retraining. In such cases, there are several techniques for correcting dataset shift.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="b200"><strong class="kk jm">1) Feature Removal</strong></p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="400b">By utilizing the statistical distance methods discussed above which are used to identify covariate shift, we can use these as measures of the extent of the shifting. We can set a boundary on what is deemed an acceptable level of shift, and analyzing individual features or through an ablation study, we can determine which features are most responsible for the shifting and remove these from the dataset.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="726c">As you may expect, there is a trade-off between removing features that contribute to the covariate shift and having additional features and tolerating some covariate shift. This trade-off is something that the data scientist would need to assess on a case-by-case basis.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="eb3c">A feature that differs a lot during training and test, but does not give you a lot of predictive power, should always be dropped.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="6db3">As an example, PSI is used in risk management and an arbitrary value of 0.25 is used as the limit, above which this is deemed as a major shift.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="5ec9"><strong class="kk jm">2) Importance Reweighting</strong><br/>The main idea with importance reweighting is that you want to upweight training instances that are very similar to your test instances. Essentially, you try to change your training data set such that it looks like it was drawn from the test data set. The only thing required for this method is unlabeled examples for the test domain. This may result in data leakage from the test set.</p><figure class="lp lq lr ls gx lt gl gm paragraph-image"><div class="lu lv do lw ce lx" role="button" tabindex="0"><div class="gl gm pg"><picture><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/max/640/1*e8jBqBE9E6Dq9I1WaeK_TQ.png 640w, https://miro.medium.com/max/720/1*e8jBqBE9E6Dq9I1WaeK_TQ.png 720w, https://miro.medium.com/max/750/1*e8jBqBE9E6Dq9I1WaeK_TQ.png 750w, https://miro.medium.com/max/786/1*e8jBqBE9E6Dq9I1WaeK_TQ.png 786w, https://miro.medium.com/max/828/1*e8jBqBE9E6Dq9I1WaeK_TQ.png 828w, https://miro.medium.com/max/1100/1*e8jBqBE9E6Dq9I1WaeK_TQ.png 1100w, https://miro.medium.com/max/1400/1*e8jBqBE9E6Dq9I1WaeK_TQ.png 1400w"/><img alt="" class="ce ly lz c" height="192" loading="eager" role="presentation" width="700"/></picture></div></div><figcaption class="ma bl gn gl gm mb mc bm b bn bo cn">On the left, we have our typical training set and in the center our test set. We estimate the data probability of the training and test sets and use this to rescale our training set to produce the training set on the right (notice the size of the points has got larger, this represents the ‘weight’ of the training example).</figcaption></figure><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="8727">To make it clear how this works, we basically reweight each of the training examples by the relative probability of the training and test set. We can do this by density estimation, through kernel methods such as kernel mean matching, or through discriminative reweighting.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="a8e6"><strong class="kk jm">3) Adversarial Search</strong></p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="655c">The <em class="lj">adversarial search</em> method uses an adversarial model where the learning algorithm attempts to construct a predictor that is robust to the deletion of features at test time.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="70fd">The problem is formulated as finding the optimal minimax strategy with respect to an adversary which deletes features and shows that the optimal strategy may be found by either solving a quadratic program or using efficient bundle methods for optimization.</p></div><div class="o dx ms mt in mu" role="separator"><span class="mv fl ci mw mx my"></span><span class="mv fl ci mw mx my"></span><span class="mv fl ci mw mx"></span></div><div class="je jf jg jh ji"><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="cd9e">Covariate shift has been extensively studied in the literature, and a number of proposals to work under it have been published. Some of the most important ones include:</p><ul class=""><li class="me mf jl kk b kl km kp kq kt mg kx mh lb mi lf mj mk ml mm ga" id="510a">Weighting the log-likelihood function (Shimodaira, 2000)</li><li class="me mf jl kk b kl mn kp mo kt mp kx mq lb mr lf mj mk ml mm ga" id="8aec">Importance weighted cross-validation (Sugiyama et al, 2007 JMLR)</li><li class="me mf jl kk b kl mn kp mo kt mp kx mq lb mr lf mj mk ml mm ga" id="f00e">Integrated optimization problem. Discriminative learning. (Bickel et al, 2009 JMRL)</li><li class="me mf jl kk b kl mn kp mo kt mp kx mq lb mr lf mj mk ml mm ga" id="466c">Kernel mean matching (<a class="au ln" href="http://www.gatsby.ucl.ac.uk/~gretton/papers/covariateShiftChapter.pdf" rel="noopener ugc nofollow" target="_blank">Gretton et al., 2009</a>)</li><li class="me mf jl kk b kl mn kp mo kt mp kx mq lb mr lf mj mk ml mm ga" id="bad0">Adversarial search (<a class="au ln" href="http://www.acad.bg/ebook/ml/The.MIT.Press.Dataset.Shift.in.Machine.Learning.Feb.2009.eBook-DDU.pdf" rel="noopener ugc nofollow" target="_blank">Globerson et al, 2009</a>)</li><li class="me mf jl kk b kl mn kp mo kt mp kx mq lb mr lf mj mk ml mm ga" id="0d1d">Frank-Wolfe algorithm (<a class="au ln" href="https://webdocs.cs.ualberta.ca/~dale/papers/ijcai15.pdf" rel="noopener ugc nofollow" target="_blank">Wen et al., 2015</a>)</li></ul></div><div class="o dx ms mt in mu" role="separator"><span class="mv fl ci mw mx my"></span><span class="mv fl ci mw mx my"></span><span class="mv fl ci mw mx"></span></div><div class="je jf jg jh ji"><h1 class="mz na jl bm nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw ga" id="40b4">Final Comments</h1><p class="pw-post-body-paragraph ki kj jl kk b kl nx kn ko kp ny kr ks kt nz kv kw kx oa kz la lb ob ld le lf je ga" id="6532">Dataset shift is a topic that is, in my estimation, extremely important and yet undervalued by people in the field of data science and machine learning.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="057f">Given the impact it can have on the performance of our algorithms, I suggest spending some time working out how to handle data properly in order to give you more confidence in your models, and, hopefully, superior performance.</p><h2 class="on na jl bm nb oo op oq nf or os ot nj kt ou ov nn kx ow ox nr lb oy oz nv pa ga" id="9582">Newsletter</h2><p class="pw-post-body-paragraph ki kj jl kk b kl nx kn ko kp ny kr ks kt nz kv kw kx oa kz la lb ob ld le lf je ga" id="0a14">For updates on new blog posts and extra content, sign up for my newsletter.</p><div class="ph pi gt gv pj pk"><a href="https://mailchi.mp/6304809e49e7/matthew-stewart" rel="noopener ugc nofollow" target="_blank"><div class="pl o fr"><div class="pm o da dx en pn"><h2 class="bm jm dm bo fs po fu fv pp fx fz jk ga">Newsletter Subscription</h2><div class="pq l"><h3 class="bm b dm bo fs po fu fv pp fx fz cn">Enrich your academic journey by joining a community of scientists, researchers, and industry professionals to obtain…</h3></div><div class="pr l"><p class="bm b hi bo fs po fu fv pp fx fz cn">mailchi.mp</p></div></div><div class="ps l"><div class="pt l pu pv pw ps px ly pk"></div></div></div></a></div><h1 class="mz na jl bm nb nc py ne nf ng pz ni nj nk qa nm nn no qb nq nr ns qc nu nv nw ga" id="439c">References</h1><p class="pw-post-body-paragraph ki kj jl kk b kl nx kn ko kp ny kr ks kt nz kv kw kx oa kz la lb ob ld le lf je ga" id="13ab">[1] <a class="au ln" href="http://iwann.ugr.es/2011/pdf/InvitedTalk-FHerrera-IWANN11.pdf" rel="noopener ugc nofollow" target="_blank">http://iwann.ugr.es/2011/pdf/InvitedTalk-FHerrera-IWANN11.pdf</a></p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="a564">[2] J.G. Moreno-Torres, T. Raeder, R. Alaiz-Rodríguez, N.V. Chawla, F. Herrera. A Unifiying view of Data Shift in Classification. Pattern Recognition, 2011, In press.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="4d99">[3] J. Quiñonero Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence. Dataset Shift in Machine Learning Shift in Machine Learning. The MIT Press 2009 The MIT Press, 2009.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="6b11">[4] Raeder, Hoens &amp; Chawla. Consequences of Variability in Classifier of Variability in Classifier Performance Estimates., ICDM ’10 Proceedings of the 2010 IEEE International Conference on Data Mining.</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="fc12">[5] Moreno-Torres, J. G., &amp; Herrera, F. (2010). A preliminary study on overlapping and data fracture in imbalanced domains by means of genetic programming-based feature extraction. In Proceedings of the 10th International Conference on Intelligent Systems Design and Applications (ISDA 2010) (pp. 501–506).</p><p class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf je ga" id="d080">[6] <a class="au ln" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5070592/pdf/f1000research-5-10228.pdf" rel="noopener ugc nofollow" target="_blank">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5070592/pdf/f1000research-5-10228.pdf</a></p></div></div></section></div></div></article>